{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Classifica√ß√£o tabular"
      ],
      "metadata": {
        "id": "3osSJsagjpfD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üåæ **Classifica√ß√£o de Sementes de Trigo - Explica√ß√£o Inicial**  \n",
        "\n",
        "Este conjunto de dados cont√©m **medidas f√≠sicas de sementes de trigo** e tem como objetivo a **classifica√ß√£o da esp√©cie** com base nessas caracter√≠sticas. Cada linha representa uma semente e inclui as seguintes informa√ß√µes:  \n",
        "\n",
        "üîπ **Caracter√≠sticas das sementes:**  \n",
        "- **√Årea**: Tamanho da superf√≠cie da semente.  \n",
        "- **Per√≠metro**: Medida do contorno da semente.  \n",
        "- **Compacidade**: Grau de proximidade entre as partes da semente.  \n",
        "- **Comprimento**: Tamanho longitudinal da semente.  \n",
        "- **Largura**: Largura m√°xima da semente.  \n",
        "- **Assimetria**: Diferen√ßa na forma da semente em rela√ß√£o a um eixo.  \n",
        "- **Comprimento do sulco**: Medida do sulco central da semente.  \n",
        "\n",
        "üîπ **Classe de sa√≠da:**  \n",
        "- **Esp√©cie**: Representa o tipo de semente de trigo (0, 1 ou 2), indicando a qual variedade ela pertence.  \n",
        "\n",
        "O objetivo do problema √© utilizar essas medidas para treinar um modelo de **classifica√ß√£o supervisionada**, permitindo identificar corretamente a esp√©cie de uma nova semente com base nas suas caracter√≠sticas. üöÄüåæ"
      ],
      "metadata": {
        "id": "0iU3CCGZQT_B"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "_lMWVaKAhwHq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gnTt1B4Ehhaf"
      },
      "outputs": [],
      "source": [
        "dados = pd.read_csv('https://raw.githubusercontent.com/alura-cursos/Primeiros_passos_pytorch/main/sementes.csv')\n",
        "dados.head()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dados['Esp√©cie'].unique()"
      ],
      "metadata": {
        "id": "sDxDIuVWBTOU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = dados.drop(['Esp√©cie'],axis=1).values\n",
        "y = dados['Esp√©cie'].values"
      ],
      "metadata": {
        "id": "VIy8Gdo3hzpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_treino,X_teste,y_treino,y_teste = train_test_split(X,y,test_size=0.2,stratify=y,random_state=42)"
      ],
      "metadata": {
        "id": "7X6HhR1GA-3X"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader, TensorDataset\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler"
      ],
      "metadata": {
        "id": "OMQ0UTMyCZsM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Normaliza√ß√£o dos dados"
      ],
      "metadata": {
        "id": "35GJKyDzCbPd"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/tensors.html"
      ],
      "metadata": {
        "id": "YZy7iOJoEYZ7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html"
      ],
      "metadata": {
        "id": "cOA7mR_0D56y"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "TrAKvJ0uC5M_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IEStyhbXEm_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Criando DataLoaders"
      ],
      "metadata": {
        "id": "VDPRyvlpHb_2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U6_IY2jgHgBh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defini√ß√£o da Rede Neural"
      ],
      "metadata": {
        "id": "ZkQpQNd9H6iB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://pytorch.org/docs/stable/generated/torch.nn.Module.html"
      ],
      "metadata": {
        "id": "ZBNrfFDoIcuW"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "hgKNIXQSHuQi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **O que √© `nn.Linear`?**\n",
        "`nn.Linear(in_features, out_features)` √© uma camada totalmente conectada (ou densa), que realiza uma transforma√ß√£o linear dos dados de entrada:\n",
        "\n",
        "\n",
        "$$ Y = XW^T + b $$\n",
        "\n",
        "\n",
        "onde:\n",
        "- \\( X \\) √© o tensor de entrada\n",
        "- \\( W \\) s√£o os pesos da camada\n",
        "- \\( b \\) √© o bias (termo de deslocamento)\n",
        "- \\( Y \\) √© a sa√≠da transformada\n",
        "\n",
        "Essa camada aprende \\( W \\) e \\( b \\) durante o treinamento para mapear a entrada na sa√≠da desejada.\n",
        "\n",
        "---\n",
        "\n",
        "### üîπ **Explica√ß√£o da Arquitetura**\n",
        "1. **Camada oculta (`self.fc1`)**  \n",
        "   ```python\n",
        "   self.fc1 = nn.Linear(input_size, hidden_size)\n",
        "   ```\n",
        "   - Converte a entrada (`input_size`) para um espa√ßo de representa√ß√£o de dimens√£o `hidden_size`.\n",
        "   - Aprender√° pesos $ W_1 $ e bias $ b_1$ para essa transforma√ß√£o.\n",
        "\n",
        "2. **Ativa√ß√£o ReLU (`self.relu`)**  \n",
        "   ```python\n",
        "   self.relu = nn.ReLU()\n",
        "   ```\n",
        "   - Aplica a fun√ß√£o de ativa√ß√£o ReLU:\n",
        "     \n",
        "     $$ f(x) = \\max(0, x) $$\n",
        "     \n",
        "   - Introduz n√£o-linearidade, permitindo que a rede aprenda padr√µes mais complexos.\n",
        "\n",
        "3. **Camada de sa√≠da (`self.fc2`)**  \n",
        "   ```python\n",
        "   self.fc2 = nn.Linear(hidden_size, output_size)\n",
        "   ```\n",
        "   - Transforma a sa√≠da da camada oculta (`hidden_size`) no n√∫mero final de classes (`output_size`).\n",
        "   - A sa√≠da ainda n√£o est√° normalizada (logits), ent√£o usamos `CrossEntropyLoss` para lidar com isso.\n",
        "\n",
        "---\n",
        "\n",
        "### üî• **Fluxo de Dados (`forward`)**\n",
        "```python\n",
        "def forward(self, x):\n",
        "    x = self.fc1(x)   # Transforma√ß√£o linear\n",
        "    x = self.relu(x)  # Ativa√ß√£o ReLU\n",
        "    x = self.fc2(x)   # Segunda transforma√ß√£o linear\n",
        "    return x          # Sa√≠da (logits)\n",
        "```\n",
        "- A entrada passa pela primeira camada densa (`fc1`).\n",
        "- Depois, aplicamos a ativa√ß√£o `ReLU`, introduzindo n√£o-linearidade.\n",
        "- Finalmente, a sa√≠da passa pela camada `fc2`, que gera 3 valores (um para cada classe).\n",
        "\n",
        "üí° **Observa√ß√£o:**  \n",
        "A sa√≠da do modelo s√£o **logits** (valores n√£o normalizados). Para obter probabilidades, aplicar√≠amos `nn.Softmax(dim=1)`, mas `CrossEntropyLoss` j√° faz isso internamente.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **Resumo**\n",
        "- `nn.Linear` realiza transforma√ß√µes lineares de entrada para sa√≠da.\n",
        "- `ReLU` adiciona n√£o-linearidade para melhorar a capacidade de aprendizado.\n",
        "- O modelo √© uma rede neural simples com **uma camada oculta** e **tr√™s neur√¥nios de sa√≠da** (um para cada classe)."
      ],
      "metadata": {
        "id": "raGDLTOlKGSa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Defini√ß√£o de hiperpar√¢metros"
      ],
      "metadata": {
        "id": "NqLTJf1kOxVX"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "zMwHasAcMREP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üìå **Regras pr√°ticas para definir o n√∫mero de neur√¥nios na camada oculta**\n",
        "Como o n√∫mero de features de entrada √© **7**, podemos usar algumas heur√≠sticas comuns:\n",
        "\n",
        "1Ô∏è‚É£ **Regra da m√©dia geom√©trica:**  \n",
        "$$ \\text{neur√¥nios na camada oculta} = \\sqrt{\\text{neur√¥nios de entrada} \\times \\text{neur√¥nios de sa√≠da}}\n",
        "$$\n",
        "Aplicando ao nosso caso:\n",
        "$$\n",
        "h = \\sqrt{7 \\times 3} = \\sqrt{21} \\approx 4 \\text{ ou } 5\n",
        "$$\n",
        "\n",
        "2Ô∏è‚É£ **Regra do dobro do n√∫mero de entradas:**  \n",
        "$$\n",
        "h = 2 \\times \\text{n√∫mero de features} = 2 \\times 7 = 14\n",
        "$$\n",
        "Se o problema for complexo, pode ser √∫til come√ßar com mais neur√¥nios.\n",
        "\n",
        "3Ô∏è‚É£ **Regra do \"funil\" (entre entrada e sa√≠da):**  \n",
        "- A camada oculta geralmente tem um n√∫mero intermedi√°rio de neur√¥nios entre a entrada e a sa√≠da.\n",
        "- Um valor comum seria algo entre $  (7+3)/2 = 5;  2 \\times 7 = 14  $.\n",
        "\n",
        "---\n",
        "\n",
        "### üöÄ **Escolha pr√°tica**\n",
        "Se for um problema simples, **5 a 10 neur√¥nios** na camada oculta pode ser um bom come√ßo.  \n",
        "Se for um problema mais complexo, com padr√µes dif√≠ceis de aprender, **10 a 14 neur√¥nios** pode ser melhor.  \n",
        "\n",
        "#### **Aplicando no c√≥digo:**\n",
        "```python\n",
        "hidden_size = 5  # Ou 7, 10, 14, dependendo dos testes\n",
        "model = Classifier(input_size=7, hidden_size=hidden_size, output_size=3)\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### üî• **Melhor estrat√©gia: Experimenta√ß√£o!**\n",
        "O melhor n√∫mero de neur√¥nios depende da complexidade do problema e dos dados. O ideal √© testar diferentes configura√ß√µes e observar a **performance no conjunto de valida√ß√£o**, usando t√©cnicas como **cross-validation** ou **grid search**."
      ],
      "metadata": {
        "id": "9IVlghkVNumq"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Inicializa√ß√£o do modelo"
      ],
      "metadata": {
        "id": "S95hqik_O7lY"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_PPeyN5tTb_f"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Treinamento do modelo"
      ],
      "metadata": {
        "id": "f8uiQqFEVbM2"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mWg6bNVOVfRL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **1. Estrutura do loop de treinamento**\n",
        "O treinamento do modelo √© dividido em **√©pocas** (`epochs`) e **lotes (batches)**. Cada √©poca representa uma itera√ß√£o completa sobre o conjunto de dados de treinamento, enquanto o processamento em lotes divide o conjunto de dados em partes menores para melhorar a efici√™ncia computacional e permitir o uso de gradientes estoc√°sticos.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. Passo a passo do c√≥digo**\n",
        "\n",
        "#### **(a) Loop de √©pocas**\n",
        "```python\n",
        "for epoch in range(epochs):\n",
        "```\n",
        "- Este loop externo controla o n√∫mero total de √©pocas de treinamento. Cada √©poca representa uma passagem completa pelo conjunto de dados de treinamento.\n",
        "- `epochs` √© o n√∫mero total de vezes que o modelo ver√° o conjunto de dados.\n",
        "\n",
        "#### **(b) Modo de treinamento**\n",
        "```python\n",
        "model.train()\n",
        "```\n",
        "- Coloca o modelo no **modo de treinamento**. Isso √© importante porque algumas camadas (como `Dropout` ou `BatchNorm`) se comportam de forma diferente durante o treinamento e a infer√™ncia.\n",
        "- No modo de treinamento, essas camadas ajustam seus par√¢metros ou aplicam regulariza√ß√£o.\n",
        "\n",
        "#### **(c) Inicializa√ß√£o do total da perda**\n",
        "```python\n",
        "total_loss = 0\n",
        "```\n",
        "- Aqui, √© inicializada uma vari√°vel para acumular a perda total ao longo de todos os lotes da √©poca. Isso ser√° usado para calcular a perda m√©dia no final da √©poca.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. Loop de lotes (batches)**\n",
        "```python\n",
        "for X_batch, y_batch in train_loader:\n",
        "```\n",
        "- Este loop interno itera sobre os dados de treinamento divididos em lotes (batches), que s√£o fornecidos pelo `DataLoader` (`train_loader`).\n",
        "- Cada `X_batch` cont√©m um subconjunto dos dados de entrada, e `y_batch` cont√©m os r√≥tulos correspondentes.\n",
        "\n",
        "#### **(a) Envio dos dados para o dispositivo**\n",
        "```python\n",
        "X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "```\n",
        "- Move os dados de entrada (`X_batch`) e os r√≥tulos (`y_batch`) para o dispositivo de computa√ß√£o configurado (CPU ou GPU). Isso √© necess√°rio para garantir que os dados e o modelo estejam no mesmo dispositivo durante o treinamento.\n",
        "\n",
        "#### **(b) Zerar os gradientes acumulados**\n",
        "```python\n",
        "optimizer.zero_grad()\n",
        "```\n",
        "- Zera os gradientes acumulados nos par√¢metros do modelo. No PyTorch, os gradientes s√£o acumulados por padr√£o, ent√£o √© necess√°rio limp√°-los antes de calcular os novos gradientes para o lote atual.\n",
        "\n",
        "#### **(c) Forward pass**\n",
        "```python\n",
        "outputs = model(X_batch)\n",
        "```\n",
        "- Passa os dados de entrada (`X_batch`) pelo modelo, gerando as sa√≠das (`outputs`), que geralmente s√£o os logits (valores antes da aplica√ß√£o de softmax).\n",
        "\n",
        "#### **(d) C√°lculo da perda**\n",
        "```python\n",
        "loss = criterion(outputs, y_batch)\n",
        "```\n",
        "- Calcula a perda entre as sa√≠das previstas pelo modelo (`outputs`) e os r√≥tulos verdadeiros (`y_batch`) usando a fun√ß√£o de perda definida (`criterion`, que neste caso √© `CrossEntropyLoss`).\n",
        "\n",
        "#### **(e) Backward pass**\n",
        "```python\n",
        "loss.backward()\n",
        "```\n",
        "- Calcula os gradientes da perda em rela√ß√£o aos pesos do modelo usando o algoritmo de backpropagation. Esses gradientes s√£o armazenados nos par√¢metros do modelo (acess√≠veis via `model.parameters()`).\n",
        "\n",
        "#### **(f) Atualiza√ß√£o dos pesos**\n",
        "```python\n",
        "optimizer.step()\n",
        "```\n",
        "- Atualiza os pesos do modelo usando os gradientes calculados no passo anterior. O otimizador (`optimizer`, que neste caso √© `Adam`) aplica a regra de atualiza√ß√£o apropriada com base nos gradientes e na taxa de aprendizado (`learning_rate`).\n",
        "\n",
        "#### **(g) Acumula√ß√£o da perda**\n",
        "```python\n",
        "total_loss += loss.item()\n",
        "```\n",
        "- Adiciona a perda do lote atual (`loss.item()`) ao total acumulado da √©poca (`total_loss`). O m√©todo `.item()` √© usado para obter o valor escalar da perda como um n√∫mero Python.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. Exibi√ß√£o da perda m√©dia por √©poca**\n",
        "```python\n",
        "print(f'Epoch {epoch+1}/{epochs}, Loss: {total_loss/len(train_loader):.4f}')\n",
        "```\n",
        "- Ap√≥s o t√©rmino de todos os lotes em uma √©poca, calcula-se a perda m√©dia dividindo o total acumulado (`total_loss`) pelo n√∫mero de lotes (`len(train_loader)`).\n",
        "- Exibe a perda m√©dia para a √©poca atual no formato especificado.\n",
        "\n",
        "---\n",
        "\n",
        "### **Resumo do Processo**\n",
        "1. **Forward pass**: Os dados de entrada s√£o passados pelo modelo para gerar previs√µes.\n",
        "2. **C√°lculo da perda**: A diferen√ßa entre as previs√µes e os r√≥tulos verdadeiros √© avaliada usando a fun√ß√£o de perda.\n",
        "3. **Backward pass**: Os gradientes da perda em rela√ß√£o aos pesos do modelo s√£o calculados.\n",
        "4. **Atualiza√ß√£o dos pesos**: Os pesos do modelo s√£o atualizados pelo otimizador com base nos gradientes.\n",
        "5. **Monitoramento**: A perda m√©dia por √©poca √© calculada e exibida para acompanhar o progresso do treinamento.\n",
        "\n",
        "---\n",
        "\n",
        "### **Import√¢ncia do Loop de Treinamento**\n",
        "O loop de treinamento √© o cora√ß√£o do aprendizado em redes neurais. Ele ajusta os pesos do modelo para minimizar a fun√ß√£o de perda e, consequentemente, melhorar o desempenho do modelo no conjunto de dados de treinamento. O objetivo final √© que o modelo generalize bem para novos dados (conjunto de teste ou valida√ß√£o)."
      ],
      "metadata": {
        "id": "KtEZtZd1WVxD"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Avalia√ß√£o no conjunto de teste"
      ],
      "metadata": {
        "id": "PvweFyrjXB1K"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3j2H8-pVXE9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZLS69n8aX8LV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "keoEu_UpX_vg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "xAp6a0QnYEgc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "KOB8_ZwKjw1W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "K9cvBoOCkUCW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DOuV1VMakhb5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "DliujAW2kjgl"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}