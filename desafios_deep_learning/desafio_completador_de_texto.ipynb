{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ToTGJ45vqkqx"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"SELECT * FROM customers WHERE active = 1;\",\n",
        "    \"INSERT INTO products (name, price) VALUES ('Laptop', 1200);\",\n",
        "    \"UPDATE employees SET salary = 5000 WHERE department = 'HR';\",\n",
        "    \"DELETE FROM orders WHERE order_date < '2023-01-01';\",\n",
        "    \"CREATE TABLE users (id INT PRIMARY KEY, username VARCHAR(50));\",\n",
        "    \"ALTER TABLE invoices ADD COLUMN total DECIMAL(10, 2);\",\n",
        "    \"DROP TABLE IF EXISTS temp_data;\",\n",
        "    \"SELECT name, COUNT(*) AS total FROM sales GROUP BY name;\",\n",
        "    \"JOIN addresses ON customers.id = addresses.customer_id;\",\n",
        "    \"ORDER BY created_at DESC LIMIT 10;\",\n",
        "    \"WHERE status = 'pending' AND priority > 5;\",\n",
        "    \"HAVING SUM(amount) > 1000;\",\n",
        "    \"GRANT SELECT, INSERT ON database.* TO 'user'@'localhost';\",\n",
        "    \"REVOKE ALL PRIVILEGES ON employees FROM 'intern'@'%';\",\n",
        "    \"BEGIN TRANSACTION; COMMIT;\",\n",
        "    \"ROLLBACK TO SAVEPOINT before_update;\",\n",
        "    \"EXPLAIN SELECT * FROM logs WHERE type = 'error';\",\n",
        "    \"INDEX idx_name ON employees (last_name);\",\n",
        "    \"PRIMARY KEY (order_id, product_id);\",\n",
        "    \"FOREIGN KEY (customer_id) REFERENCES customers(id);\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento"
      ],
      "metadata": {
        "id": "dB0oLBfrr3zJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# Tokenização e vocabulário\n",
        "tokens = [word.lower() for sentence in corpus for word in sentence.split()]\n",
        "vocab = Counter(tokens)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Mapeamento palavra para índice\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "8Qx3wF5irs7T"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 1: Modelo de linguagem"
      ],
      "metadata": {
        "id": "D0iASFx6r65t"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta etapa, vamos criar o modelo de linguagem que será treinado para prever a próxima palavra com base no contexto das palavras anteriores. O modelo será uma rede neural baseada em embeddings e LSTMs, que são adequados para lidar com sequências de texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **Estrutura do Modelo**\n",
        "\n",
        "O modelo será composto por:\n",
        "\n",
        "1. **Camada de Embedding**  \n",
        "   - Converte os índices das palavras em vetores densos de tamanho fixo (representação vetorial das palavras).  \n",
        "   - Tamanho do embedding (`embedding_dim`) é um hiperparâmetro.\n",
        "\n",
        "2. **Camada LSTM**  \n",
        "   - Processa as sequências de embeddings e mantém informações sobre o contexto anterior.  \n",
        "   - Tamanho do estado oculto (`hidden_dim`) é outro hiperparâmetro.\n",
        "\n",
        "3. **Camada Linear (Fully Connected)**  \n",
        "   - Projeta a saída da LSTM para o espaço do vocabulário, gerando scores para cada palavra no vocabulário.\n",
        "\n",
        "4. **Softmax (aplicado durante inferência)**  \n",
        "   - Converte os scores para probabilidades de cada palavra no vocabulário."
      ],
      "metadata": {
        "id": "xnkliBk8vPDR"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "CWa61z4cr0p8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 2: Dataset e DataLoader"
      ],
      "metadata": {
        "id": "UVO_oKeBsEL7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Nesta tarefa, vamos criar um **Dataset** e um **DataLoader** para alimentar o modelo com dados durante o treinamento. O Dataset será responsável por preparar as sequências de entrada e saída a partir do corpus, enquanto o DataLoader organizará os dados em lotes (`batches`) para treinamento eficiente.\n",
        "\n",
        "---\n",
        "\n",
        "### **Estrutura do Dataset**\n",
        "\n",
        "O Dataset utilizará o corpus pré-processado e fará o seguinte:\n",
        "\n",
        "1. **Divisão em Sequências**  \n",
        "   - Cada sentença será dividida em sequências de comprimento fixo (`seq_length`).  \n",
        "   - Por exemplo, dado o texto: `\"SELECT * FROM customers\"`, com `seq_length=3`:\n",
        "     - Entrada (`x`): `\"SELECT * FROM\"`\n",
        "     - Saída (`y`): `\"* FROM customers\"`\n",
        "\n",
        "2. **Mapeamento para Índices**  \n",
        "   - Cada palavra será convertida para seu índice correspondente no vocabulário (`word_to_idx`).\n",
        "\n",
        "3. **Par Entrada-Saída**  \n",
        "   - Para cada sequência de entrada (`x`), haverá uma sequência de saída (`y`), que corresponde às próximas palavras.\n",
        "\n",
        "---\n"
      ],
      "metadata": {
        "id": "4jtsa-f9wJ3q"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VE6868EqsF81"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 3: Treinamento"
      ],
      "metadata": {
        "id": "6x22eymYsl2V"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que temos o **modelo**, o **Dataset** e o **DataLoader**, o próximo passo é implementar o loop de treinamento. O objetivo do treinamento é ajustar os pesos do modelo para minimizar a perda (loss), que mede a diferença entre as previsões do modelo e as saídas reais.\n",
        "\n",
        "---\n",
        "\n",
        "### **Etapas do Treinamento**\n",
        "\n",
        "1. **Definir Função de Perda e Otimizador**  \n",
        "   - A função de perda será a `CrossEntropyLoss`, que é adequada para problemas de classificação, como prever a próxima palavra.  \n",
        "   - O otimizador será o `Adam`, que é eficiente para redes neurais profundas.\n",
        "\n",
        "2. **Loop de Treinamento**  \n",
        "   - Para cada época:\n",
        "     - Iterar sobre os lotes do DataLoader.\n",
        "     - Passar o lote pelo modelo para obter as previsões.\n",
        "     - Calcular a perda comparando as previsões com as saídas reais.\n",
        "     - Atualizar os pesos do modelo usando o otimizador.\n",
        "\n",
        "3. **Monitorar o Progresso**  \n",
        "   - Exibir a perda a cada época para acompanhar o aprendizado do modelo.\n",
        "\n"
      ],
      "metadata": {
        "id": "rz5aYuyZxb-4"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "oR6Tw1GfsaCC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Função de completar texto"
      ],
      "metadata": {
        "id": "xIZYzfLKsuc3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(seed_text, num_words=5, temperature=0.7):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            inputs = torch.tensor(\n",
        "                [word_to_idx[word] for word in words[-3:]]  # Usa contexto de 3 palavras\n",
        "            ).unsqueeze(0).to(device)\n",
        "\n",
        "            output, _ = model(inputs)\n",
        "            probabilities = torch.softmax(output[0, -1] / temperature, dim=0)\n",
        "            next_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            words.append(idx_to_word[next_idx])\n",
        "\n",
        "    return \" \".join(words).capitalize()"
      ],
      "metadata": {
        "id": "S1c4dxHAssc-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplos de prompts\n",
        "print(complete_text(\"SELECT * FROM\", num_words=5))\n",
        "print(complete_text(\"ADD COLUMN total\", num_words=3))\n",
        "print(complete_text(\"ROLLBACK TO SAVEPOINT\", num_words=4))"
      ],
      "metadata": {
        "id": "yaV5y_3as5Ug"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "jS3ajy7Gs7CL"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}