{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Pré-processamento"
      ],
      "metadata": {
        "id": "Mwny0tXdBEFX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import kagglehub\n",
        "\n",
        "# Download latest version\n",
        "path = kagglehub.dataset_download(\"denkuznetz/food-delivery-time-prediction\")\n",
        "\n",
        "print(\"Path to dataset files:\", path)"
      ],
      "metadata": {
        "id": "5pUg2-Uv_2iQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Listar os arquivos no diretório\n",
        "files = os.listdir(path)\n",
        "print(\"Arquivos no diretório:\", files)"
      ],
      "metadata": {
        "id": "7ewmuztLBLB7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "csv_file_path = os.path.join(path, 'Food_Delivery_Times.csv')\n",
        "\n",
        "df = pd.read_csv(csv_file_path)\n",
        "df.head()"
      ],
      "metadata": {
        "id": "nLX80VKNBMJ2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Remover a coluna Order_ID\n",
        "df.drop(columns=['Order_ID'], inplace=True)\n",
        "\n",
        "# Substituir valores nulos em \"Courier_Experience_yrs\" pela mediana\n",
        "imputer_num = SimpleImputer(strategy=\"median\")\n",
        "df[\"Courier_Experience_yrs\"] = imputer_num.fit_transform(df[[\"Courier_Experience_yrs\"]])\n",
        "\n",
        "# Substituir valores nulos em variáveis categóricas pela moda\n",
        "categorical_cols = [\"Weather\", \"Traffic_Level\", \"Time_of_Day\"]\n",
        "imputer_cat = SimpleImputer(strategy=\"most_frequent\")\n",
        "df[categorical_cols] = imputer_cat.fit_transform(df[categorical_cols])\n",
        "\n",
        "# Aplicar One-Hot Encoding nas variáveis categóricas\n",
        "df = pd.get_dummies(df, columns=[\"Weather\", \"Traffic_Level\", \"Time_of_Day\", \"Vehicle_Type\"], drop_first=True)\n",
        "\n",
        "# Exibir as primeiras linhas do dataframe processado\n",
        "df.head()\n"
      ],
      "metadata": {
        "id": "i73Y_8xlBP_c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for col in df.columns:\n",
        "    if df[col].dtype == 'object':\n",
        "        # If column is of object type, try converting to numeric\n",
        "        try:\n",
        "            df[col] = pd.to_numeric(df[col])\n",
        "        except ValueError:\n",
        "            # If conversion fails, handle the non-numeric values (e.g., replace with NaN or a specific value)\n",
        "            # Here, we replace non-numeric values with NaN and then fill them with the column's median\n",
        "            df[col] = pd.to_numeric(df[col], errors='coerce')  # Convert non-numeric to NaN\n",
        "            df[col] = df[col].fillna(df[col].median())  # Fill NaN with median"
      ],
      "metadata": {
        "id": "2QFkCPjABRiI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "X = df.drop(columns=['Delivery_Time_min']).values\n",
        "y = df['Delivery_Time_min'].values\n",
        "# Dividir os dados em conjuntos de treinamento e teste\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "aConNTf_BS2i"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import MinMaxScaler"
      ],
      "metadata": {
        "id": "ILyvt6orBUMT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Selecionando as três primeiras colunas (dados contínuos)\n",
        "continuous_indices = [0, 1, 2]\n",
        "\n",
        "# Aplicando MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "X_train[:, continuous_indices] = scaler.fit_transform(X_train[:, continuous_indices])\n",
        "X_test[:, continuous_indices] = scaler.transform(X_test[:, continuous_indices])"
      ],
      "metadata": {
        "id": "xyR0AKkXBVVL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Normalizando o alvo\n",
        "y_min = y_train.min()\n",
        "y_max = y_train.max()\n",
        "\n",
        "y_train = (y_train - y_min) / (y_max - y_min)\n",
        "y_test = (y_test - y_min) / (y_max - y_min)\n"
      ],
      "metadata": {
        "id": "nhC5DKd7BWm4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "X_train = X_train.astype(np.float32)\n",
        "X_test = X_test.astype(np.float32)"
      ],
      "metadata": {
        "id": "n_wxTdqnBX7I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Código da semana passado"
      ],
      "metadata": {
        "id": "lUAI1Th6Egi_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train, dtype=torch.float32).view(-1, 1)\n",
        "y_test_tensor = torch.tensor(y_test, dtype=torch.float32).view(-1, 1)\n",
        "\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class RegressionDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = X\n",
        "        self.y = y\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "# Criar datasets para treino e teste\n",
        "train_dataset = RegressionDataset(X_train_tensor, y_train_tensor)\n",
        "test_dataset = RegressionDataset(X_test_tensor, y_test_tensor)\n",
        "\n",
        "batch_size = 32  # Escolha um tamanho de batch\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "import torch.nn as nn\n",
        "\n",
        "class RegressionModel(nn.Module):\n",
        "    def __init__(self, input_size):\n",
        "        super(RegressionModel, self).__init__()\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(input_size, 64),  # Primeira camada\n",
        "            nn.ReLU(),                 # Função de ativação\n",
        "            nn.Linear(64, 32),         # Segunda camada\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(32, 1)           # Camada de saída\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "\n",
        "# Inicializar o modelo\n",
        "input_size = X_train_tensor.shape[1]  # Número de recursos (features)\n",
        "model = RegressionModel(input_size)\n",
        "\n",
        "import torch.optim as optim\n",
        "\n",
        "# Função de custo\n",
        "criterion = nn.MSELoss()\n",
        "\n",
        "# Otimizador\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)  # Taxa de aprendizado\n",
        "\n",
        "# Configurações do treinamento\n",
        "epochs = 100  # Número de épocas\n",
        "model.train()  # Colocar o modelo em modo de treinamento\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    epoch_loss = 0.0\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        # Zerar os gradientes\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        predictions = model(X_batch)\n",
        "\n",
        "        # Calcular a perda\n",
        "        loss = criterion(predictions, y_batch)\n",
        "\n",
        "        # Backward pass\n",
        "        loss.backward()\n",
        "\n",
        "        # Atualizar os pesos\n",
        "        optimizer.step()\n",
        "\n",
        "        # Acumular a perda\n",
        "        epoch_loss += loss.item()\n",
        "\n",
        "    print(f\"Época {epoch+1}/{epochs}, Perda: {epoch_loss/len(train_loader):.4f}\")\n",
        "\n",
        "model.eval()  # Colocar o modelo em modo de avaliação\n",
        "test_loss = 0.0\n",
        "\n",
        "with torch.no_grad():  # Desativar o cálculo do gradiente\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        test_loss += loss.item()\n",
        "\n",
        "print(f\"Perda no conjunto de teste: {test_loss/len(test_loader):.4f}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "XkNSFLRsBZ4C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Otimização"
      ],
      "metadata": {
        "id": "1S9CKmg3Fpvv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://optuna.org/#code_examples\n",
        "\n",
        "https://www.geeksforgeeks.org/hyperparameter-tuning-with-optuna-in-pytorch/"
      ],
      "metadata": {
        "id": "5-XrX93HGOTJ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install optuna"
      ],
      "metadata": {
        "id": "eAZ_E2j2GDzE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import optuna"
      ],
      "metadata": {
        "id": "MnDKVZTkFpKp"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 1: Recriar o modelo"
      ],
      "metadata": {
        "id": "BqeWM2y9NWDp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mB3UVXq7NXpz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa 2: Criar função objetivo para o Optuna"
      ],
      "metadata": {
        "id": "nL1hP89hONbO"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eIA90CtSOH8J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tafera 3: Rodar a otimização com o optuna"
      ],
      "metadata": {
        "id": "SwAjLDUAP0hE"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "MWDDQ7BDP5J6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Tarefa: Hiperparâmetros com LLMs"
      ],
      "metadata": {
        "id": "xki6ZP6vUdD7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. Descrição do Problema: Forneça um prompt inicial para a LLM com uma descrição clara do problema, incluindo:\n",
        "\n",
        "O modelo que você está treinando.\n",
        "A estrutura dos dados (número de features, etc.).\n",
        "Os hiperparâmetros que você deseja otimizar.\n",
        "O orçamento de busca (número de configurações a serem testadas).\n",
        "\n",
        "2. Sugestão de Hiperparâmetros: A LLM gera uma configuração de hiperparâmetros no formato que você especificar (ex.: JSON).\n",
        "\n",
        "3. Treinamento e Avaliação: Use os hiperparâmetros sugeridos pela LLM para treinar o modelo e calcule a métrica de validação (ex.: perda ou acurácia).\n",
        "\n",
        "4. Feedback: Envie o resultado da avaliação de volta para a LLM, junto com o histórico de configurações testadas, para que ela sugira a próxima configuração.\n",
        "\n",
        "5. Iteração: Repita o processo até que o orçamento de busca seja esgotado."
      ],
      "metadata": {
        "id": "zA9rgCHEUrgn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "hyperparameters = {\n",
        "  \"learning_rate\": ,\n",
        "  \"hidden1\": ,\n",
        "  \"hidden2\": ,\n",
        "  \"batch_size\":\n",
        "}"
      ],
      "metadata": {
        "id": "Y2KNqcS3VQ4a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Configuração do modelo com os hiperparâmetros sugeridos\n",
        "model = RegressionModel(input_size, hyperparameters[\"hidden1\"], hyperparameters[\"hidden2\"])\n",
        "optimizer = optim.Adam(model.parameters(), lr=hyperparameters[\"learning_rate\"])\n",
        "batch_size = hyperparameters[\"batch_size\"]\n",
        "\n",
        "# Atualize o DataLoader\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Treinamento do modelo\n",
        "model.train()\n",
        "for epoch in range(10):  # Ajuste o número de épocas conforme necessário\n",
        "    for X_batch, y_batch in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        predictions = model(X_batch)\n",
        "        loss = criterion(predictions, y_batch)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "# Avaliação no conjunto de validação\n",
        "model.eval()\n",
        "val_loss = 0.0\n",
        "with torch.no_grad():\n",
        "    for X_batch, y_batch in test_loader:\n",
        "        predictions = model(X_batch)\n",
        "        val_loss += criterion(predictions, y_batch).item()\n",
        "\n",
        "val_loss /= len(test_loader)\n",
        "print(f\"Validação Loss: {val_loss}\")\n"
      ],
      "metadata": {
        "id": "ydK6h8DzQRtW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}