{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Carregar os dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers\n",
      "  Downloading transformers-4.48.2-py3-none-any.whl.metadata (44 kB)\n",
      "Requirement already satisfied: filelock in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (3.17.0)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (0.28.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (6.0.2)\n",
      "Collecting regex!=2019.12.17 (from transformers)\n",
      "  Downloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (40 kB)\n",
      "Requirement already satisfied: requests in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (2.32.3)\n",
      "Collecting tokenizers<0.22,>=0.21 (from transformers)\n",
      "  Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Collecting safetensors>=0.4.1 (from transformers)\n",
      "  Downloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Requirement already satisfied: tqdm>=4.27 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from transformers) (4.67.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (2024.12.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.24.0->transformers) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /system/conda/miniconda3/envs/cloudspace/lib/python3.10/site-packages (from requests->transformers) (2024.12.14)\n",
      "Downloading transformers-4.48.2-py3-none-any.whl (9.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m228.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading regex-2024.11.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (781 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m781.7/781.7 kB\u001b[0m \u001b[31m54.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading safetensors-0.5.2-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (461 kB)\n",
      "Downloading tokenizers-0.21.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.0 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m146.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: safetensors, regex, tokenizers, transformers\n",
      "Successfully installed regex-2024.11.6 safetensors-0.5.2 tokenizers-0.21.0 transformers-4.48.2\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#%pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text\n",
      "License(s): other\n",
      "Downloading ai-vs-human-text.zip to /teamspace/studios/this_studio\n",
      " 96%|███████████████████████████████████████▌ | 337M/350M [00:02<00:00, 137MB/s]\n",
      "100%|█████████████████████████████████████████| 350M/350M [00:03<00:00, 122MB/s]\n"
     ]
    }
   ],
   "source": [
    "#!kaggle datasets download -d shanegerami/ai-vs-human-text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import zipfile\n",
    "\n",
    "# Substitua o nome do arquivo pelo nome correto\n",
    "#with zipfile.ZipFile(\"ai-vs-human-text.zip\", 'r') as zip_ref:\n",
    "#    zip_ref.extractall(\"dados_textos\")  # Pasta onde os dados serão extraídos\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "caminho = '/teamspace/studios/this_studio/dados_textos/AI_Human.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import BertModel,BertTokenizer\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>generated</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Cars. Cars have been around since they became ...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Transportation is a large necessity in most co...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>\"America's love affair with it's vehicles seem...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>How often do you ride in a car? Do you drive a...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Cars are a wonderful thing. They are perhaps o...</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                text  generated\n",
       "0  Cars. Cars have been around since they became ...        0.0\n",
       "1  Transportation is a large necessity in most co...        0.0\n",
       "2  \"America's love affair with it's vehicles seem...        0.0\n",
       "3  How often do you ride in a car? Do you drive a...        0.0\n",
       "4  Cars are a wonderful thing. They are perhaps o...        0.0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Carregar o dataset\n",
    "df = pd.read_csv(caminho)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir em treino e teste\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['text'].values, \n",
    "    df['generated'].values,\n",
    "    test_size=0.2,\n",
    "    random_state=42\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando uma classe personalizada para o dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassificationDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        text = str(self.texts[idx])\n",
    "        label = self.labels[idx]\n",
    "        \n",
    "        encoding = self.tokenizer.encode_plus(\n",
    "            text,\n",
    "            add_special_tokens=True,\n",
    "            max_length=self.max_length,\n",
    "            return_token_type_ids=False,\n",
    "            padding='max_length',\n",
    "            truncation=True,\n",
    "            return_attention_mask=True,\n",
    "            return_tensors='pt'\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            'input_ids': encoding['input_ids'].flatten(),\n",
    "            'attention_mask': encoding['attention_mask'].flatten(),\n",
    "            'labels': torch.tensor(label, dtype=torch.float)\n",
    "        }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa classe **`TextClassificationDataset`** é uma implementação personalizada do `Dataset` do PyTorch para **classificação de textos** usando um `tokenizer`, como os da biblioteca `transformers` (BERT, DistilBERT, etc.).  \n",
    "\n",
    "Vamos analisar **passo a passo** o que ela faz. 🚀  \n",
    "\n",
    "---\n",
    "\n",
    "### **📌 O que essa classe faz?**\n",
    "Ela **recebe** um conjunto de textos e rótulos, **aplica tokenização** e **retorna tensores formatados** para serem usados no treinamento de um modelo.\n",
    "\n",
    "### **📌 Estrutura e funcionamento**\n",
    "\n",
    "#### 🔹 **1. Construtor `__init__`**\n",
    "```python\n",
    "def __init__(self, texts, labels, tokenizer, max_length=512):\n",
    "    self.texts = texts\n",
    "    self.labels = labels\n",
    "    self.tokenizer = tokenizer\n",
    "    self.max_length = max_length\n",
    "```\n",
    "👉 **O que acontece aqui?**  \n",
    "- **`texts`** → Lista de textos brutos.  \n",
    "- **`labels`** → Lista de rótulos correspondentes.  \n",
    "- **`tokenizer`** → Tokenizador, geralmente um modelo pré-treinado do Hugging Face, como BERT.  \n",
    "- **`max_length=512`** → Define o comprimento máximo da sequência (o BERT suporta até 512 tokens).  \n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 **2. Método `__len__`**\n",
    "```python\n",
    "def __len__(self):\n",
    "    return len(self.texts)\n",
    "```\n",
    "👉 **Retorna o número de amostras** no dataset. Isso é necessário para que o `DataLoader` saiba quantos exemplos existem.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 **3. Método `__getitem__`**\n",
    "```python\n",
    "def __getitem__(self, idx):\n",
    "    text = str(self.texts[idx])  # Converte o texto para string\n",
    "    label = self.labels[idx]  # Obtém o rótulo correspondente\n",
    "```\n",
    "👉 **Pega um texto e seu rótulo correspondente pelo índice `idx`**.\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 **4. Tokenização com `tokenizer.encode_plus`**\n",
    "```python\n",
    "encoding = self.tokenizer.encode_plus(\n",
    "    text,\n",
    "    add_special_tokens=True,\n",
    "    max_length=self.max_length,\n",
    "    return_token_type_ids=False,\n",
    "    padding='max_length',\n",
    "    truncation=True,\n",
    "    return_attention_mask=True,\n",
    "    return_tensors='pt'\n",
    ")\n",
    "```\n",
    "👉 **Aqui acontece a mágica!** Ele **converte o texto em números** para que a rede neural consiga processá-lo. Vamos entender cada parâmetro:\n",
    "\n",
    "| Parâmetro | O que faz? |\n",
    "|-----------|-----------|\n",
    "| `text` | Texto de entrada. |\n",
    "| `add_special_tokens=True` | Adiciona tokens `[CLS]` e `[SEP]` (essenciais para modelos como BERT). |\n",
    "| `max_length=self.max_length` | Define o tamanho máximo da sequência. |\n",
    "| `return_token_type_ids=False` | Não retorna token types (usado para tarefas como QA, não para classificação). |\n",
    "| `padding='max_length'` | Garante que todas as sequências tenham o mesmo tamanho, preenchendo com `[PAD]` se necessário. |\n",
    "| `truncation=True` | Se o texto for maior que `max_length`, ele será cortado. |\n",
    "| `return_attention_mask=True` | Retorna uma máscara indicando quais tokens são reais (1) e quais são padding (0). |\n",
    "| `return_tensors='pt'` | Retorna tensores do PyTorch (`torch.Tensor`). |\n",
    "\n",
    "---\n",
    "\n",
    "#### 🔹 **5. Retorno final**\n",
    "```python\n",
    "return {\n",
    "    'input_ids': encoding['input_ids'].flatten(),\n",
    "    'attention_mask': encoding['attention_mask'].flatten(),\n",
    "    'labels': torch.tensor(label, dtype=torch.float)\n",
    "}\n",
    "```\n",
    "👉 **Esse dicionário é o que o `DataLoader` recebe e retorna durante o treinamento**:\n",
    "\n",
    "- **`input_ids`** → Os tokens do texto convertidos em números.  \n",
    "- **`attention_mask`** → Indica quais tokens são válidos (1) e quais são padding (0).  \n",
    "- **`labels`** → O rótulo da amostra como um tensor do PyTorch.  \n",
    "\n",
    "---\n",
    "\n",
    "### **📌 Exemplo de Uso**\n",
    "Agora vamos ver como essa classe é usada na prática:\n",
    "\n",
    "```python\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Carregar o tokenizador do BERT\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Criar listas de textos e rótulos\n",
    "texts = [\"This is an AI-generated text.\", \"Humans write differently than AI.\"]\n",
    "labels = [1, 0]\n",
    "\n",
    "# Criar o dataset\n",
    "dataset = TextClassificationDataset(texts, labels, tokenizer)\n",
    "\n",
    "# Testar a saída\n",
    "sample = dataset[0]\n",
    "print(sample)\n",
    "```\n",
    "\n",
    "#### **📌 Saída esperada**\n",
    "```python\n",
    "{\n",
    "    'input_ids': tensor([101, 2023, 2003, 2019, ...]),\n",
    "    'attention_mask': tensor([1, 1, 1, ...]),\n",
    "    'labels': tensor(1.)\n",
    "}\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### **📌 Resumo**\n",
    "Essa classe **transforma um conjunto de textos e rótulos em tensores prontos para serem usados no PyTorch**, aplicando:\n",
    "✅ **Tokenização automática** com modelos da Hugging Face.  \n",
    "✅ **Padding/truncamento automático** para manter tamanhos uniformes.  \n",
    "✅ **Conversão para tensores do PyTorch** para facilitar o treinamento.\n",
    "\n",
    "Se precisar de mais detalhes ou quiser treinar um modelo com essa abordagem, me avise! 🚀🔥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar datasets\n",
    "train_dataset = TextClassificationDataset(X_train, y_train, tokenizer)\n",
    "test_dataset = TextClassificationDataset(X_test, y_test, tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Criar dataloaders\n",
    "train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criando o modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TextClassifier(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TextClassifier, self).__init__()\n",
    "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "        self.dropout = nn.Dropout(0.1)\n",
    "        self.fc = nn.Linear(768, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "        \n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        outputs = self.bert(\n",
    "            input_ids=input_ids,\n",
    "            attention_mask=attention_mask\n",
    "        )\n",
    "        pooled_output = outputs[1]\n",
    "        dropout_output = self.dropout(pooled_output)\n",
    "        logits = self.fc(dropout_output)\n",
    "        return self.sigmoid(logits)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Esse código define uma rede neural chamada `TextClassifier` que usa **BERT** para classificar textos. Vou explicar **passo a passo** o que acontece. 🚀  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 O que esse modelo faz?**  \n",
    "- Usa **BERT** como base para extrair representações dos textos.  \n",
    "- Passa a saída do BERT por uma **camada totalmente conectada (`fc`)** para classificação.  \n",
    "- Usa **sigmoide (`sigmoid`)** para produzir uma saída entre `0` e `1`, útil para **classificação binária** (IA vs. humano, por exemplo).  \n",
    "\n",
    "---\n",
    "\n",
    "## **📌 Estrutura e funcionamento**\n",
    "Vamos destrinchar o código.\n",
    "\n",
    "### **1️⃣ Construtor `__init__`**\n",
    "```python\n",
    "def __init__(self):\n",
    "    super(TextClassifier, self).__init__()\n",
    "    self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
    "    self.dropout = nn.Dropout(0.1)\n",
    "    self.fc = nn.Linear(768, 1)\n",
    "    self.sigmoid = nn.Sigmoid()\n",
    "```\n",
    "\n",
    "👉 **O que acontece aqui?**  \n",
    "1. **`self.bert = BertModel.from_pretrained('bert-base-uncased')`**  \n",
    "   - Carrega o modelo **BERT pré-treinado** (versão `bert-base-uncased`, que não diferencia maiúsculas/minúsculas).  \n",
    "   - BERT gera representações dos textos em um espaço vetorial de **768 dimensões** (para `bert-base`).  \n",
    "\n",
    "2. **`self.dropout = nn.Dropout(0.1)`**  \n",
    "   - Aplica **Dropout** com `10%` de taxa para evitar overfitting.  \n",
    "\n",
    "3. **`self.fc = nn.Linear(768, 1)`**  \n",
    "   - Camada **totalmente conectada (`Linear`)** que reduz as 768 dimensões do BERT para **1 único valor**, que será a **probabilidade da classe 1** (texto gerado por IA).  \n",
    "\n",
    "4. **`self.sigmoid = nn.Sigmoid()`**  \n",
    "   - Aplica a **função sigmoide** para converter o valor final em uma **probabilidade entre 0 e 1**.  \n",
    "\n",
    "---\n",
    "\n",
    "### **2️⃣ Método `forward` (Propagação para Frente)**\n",
    "```python\n",
    "def forward(self, input_ids, attention_mask):\n",
    "    outputs = self.bert(\n",
    "        input_ids=input_ids,\n",
    "        attention_mask=attention_mask\n",
    "    )\n",
    "    pooled_output = outputs[1]\n",
    "    dropout_output = self.dropout(pooled_output)\n",
    "    logits = self.fc(dropout_output)\n",
    "    return self.sigmoid(logits)\n",
    "```\n",
    "\n",
    "👉 **Explicação passo a passo**:\n",
    "1. **Executa o BERT**\n",
    "   ```python\n",
    "   outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "   ```\n",
    "   - O `input_ids` representa os tokens dos textos.\n",
    "   - O `attention_mask` diz quais tokens são reais (`1`) e quais são padding (`0`).\n",
    "   - O BERT retorna **duas saídas principais**:\n",
    "     1. `outputs[0]` → Embeddings de cada palavra (não usados aqui).\n",
    "     2. `outputs[1]` → Representação do token `[CLS]`, que é um **resumo do significado do texto**.  \n",
    "\n",
    "2. **Extrai o vetor do `[CLS]`**\n",
    "   ```python\n",
    "   pooled_output = outputs[1]\n",
    "   ```\n",
    "   - O primeiro token de cada texto no BERT é sempre o **token especial `[CLS]`**, que contém uma **representação global** da sentença.  \n",
    "\n",
    "3. **Aplica Dropout**\n",
    "   ```python\n",
    "   dropout_output = self.dropout(pooled_output)\n",
    "   ```\n",
    "   - O Dropout ajuda a evitar overfitting removendo aleatoriamente alguns valores durante o treinamento.  \n",
    "\n",
    "4. **Passa pela camada totalmente conectada**\n",
    "   ```python\n",
    "   logits = self.fc(dropout_output)\n",
    "   ```\n",
    "   - A camada `fc` reduz o vetor de **768 dimensões** para **1 única saída**, que representa a **probabilidade do texto ser IA**.  \n",
    "\n",
    "5. **Aplica a sigmoide**\n",
    "   ```python\n",
    "   return self.sigmoid(logits)\n",
    "   ```\n",
    "   - Converte o valor da `fc` para um número entre **0 e 1**, que será interpretado como **probabilidade da classe 1**.  \n",
    "   - Se for maior que `0.5`, pode ser considerado \"Texto gerado por IA\", senão \"Texto humano\".  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configurar o treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inicializar modelo\n",
    "model = TextClassifier()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "\n",
    "# Definir otimizador e função de perda\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "criterion = nn.BCELoss()\n",
    "\n",
    "# Número de épocas\n",
    "num_epochs = 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loop de treinamento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 1/3, Perda média: 0.0121\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    \n",
    "    for batch in train_loader:\n",
    "        input_ids = batch['input_ids'].to(device)\n",
    "        attention_mask = batch['attention_mask'].to(device)\n",
    "        labels = batch['labels'].to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        loss = criterion(outputs.squeeze(), labels)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "    \n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f'Época {epoch+1}/{num_epochs}, Perda média: {avg_loss:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação dos resultados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()  # Coloca o modelo em modo de avaliação\n",
    "    predictions = []\n",
    "    actual_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Desativa o cálculo de gradientes\n",
    "        for batch in test_loader:\n",
    "            input_ids = batch['input_ids'].to(device)\n",
    "            attention_mask = batch['attention_mask'].to(device)\n",
    "            labels = batch['labels'].to(device)\n",
    "            \n",
    "            # Fazer predições\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            # Converter as probabilidades em classes (0 ou 1)\n",
    "            preds = (outputs.squeeze() >= 0.5).float()\n",
    "            \n",
    "            # Mover para CPU e converter para lista\n",
    "            predictions.extend(preds.cpu().numpy())\n",
    "            actual_labels.extend(labels.cpu().numpy())\n",
    "    \n",
    "    # Converter listas para arrays numpy\n",
    "    predictions = np.array(predictions)\n",
    "    actual_labels = np.array(actual_labels)\n",
    "    \n",
    "    # Calcular métricas\n",
    "    accuracy = accuracy_score(actual_labels, predictions)\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        actual_labels, \n",
    "        predictions, \n",
    "        average='binary'\n",
    "    )\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(\"\\nResultados da Avaliação:\")\n",
    "    print(f\"Acurácia: {accuracy:.4f}\")\n",
    "    print(f\"Precisão: {precision:.4f}\")\n",
    "    print(f\"Recall: {recall:.4f}\")\n",
    "    print(f\"F1-Score: {f1:.4f}\")\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'f1': f1\n",
    "    }\n",
    "\n",
    "# Chamar a função de avaliação\n",
    "metrics = evaluate_model(model, test_loader, device)\n",
    "\n",
    "# Opcional: Salvar o modelo treinado\n",
    "torch.save(model.state_dict(), 'modelo_classificador.pt')\n",
    "\n",
    "# Para carregar o modelo posteriormente:\n",
    "# model.load_state_dict(torch.load('modelo_classificador.pt'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred):\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    plt.figure(figsize=(8, 6))\n",
    "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
    "    plt.title('Matriz de Confusão')\n",
    "    plt.ylabel('Valor Real')\n",
    "    plt.xlabel('Valor Previsto')\n",
    "    plt.show()\n",
    "\n",
    "# Para usar a matriz de confusão, adicione ao final da função evaluate_model:\n",
    "plot_confusion_matrix(actual_labels, predictions)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Avaliação em um único texto"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_single_text(text, model, tokenizer, device):\n",
    "    # Coloca o modelo em modo de avaliação\n",
    "    model.eval()\n",
    "    \n",
    "    # Tokeniza o texto\n",
    "    encoding = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=512,\n",
    "        padding='max_length',\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors='pt'\n",
    "    )\n",
    "    \n",
    "    # Move para o device apropriado (CPU ou GPU)\n",
    "    input_ids = encoding['input_ids'].to(device)\n",
    "    attention_mask = encoding['attention_mask'].to(device)\n",
    "    \n",
    "    # Faz a predição\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        probability = outputs.squeeze().item()\n",
    "        prediction = 1 if probability >= 0.5 else 0\n",
    "    \n",
    "    # Retorna os resultados\n",
    "    return {\n",
    "        'texto': text,\n",
    "        'probabilidade': probability,\n",
    "        'previsao': 'IA' if prediction == 1 else 'Humano',\n",
    "    }\n",
    "\n",
    "# Exemplo de uso:\n",
    "texto = \"Olá, bom dia! Como você está hoje?\"\n",
    "resultado = predict_single_text(texto, model, tokenizer, device)\n",
    "\n",
    "print(\"\\nResultados da Análise:\")\n",
    "print(f\"Texto: {resultado['texto']}\")\n",
    "print(f\"Probabilidade de ser IA: {resultado['probabilidade']:.4f}\")\n",
    "print(f\"Classificação: {resultado['previsao']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testando com diferentes textos\n",
    "textos_teste = [\n",
    "    \"Olá, bom dia! Como você está hoje?\",\n",
    "    \"O processo de fotossíntese é um mecanismo biológico complexo que envolve a conversão de energia luminosa em energia química.\",\n",
    "    \"Cara, nem acredito que perdi o ônibus hoje de manhã! Que azar...\",\n",
    "]\n",
    "\n",
    "for texto in textos_teste:\n",
    "    resultado = predict_single_text(texto, model, tokenizer, device)\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(f\"Texto: {resultado['texto']}\")\n",
    "    print(f\"Probabilidade de ser IA: {resultado['probabilidade']:.4f}\")\n",
    "    print(f\"Classificação: {resultado['previsao']}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lembre-se que você precisa usar o mesmo tokenizer que foi usado no treinamento do modelo. Se você estiver carregando um modelo salvo, certifique-se de também ter o tokenizer correto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Se você precisar carregar o modelo e tokenizer novamente:\n",
    "from transformers import BertTokenizer\n",
    "\n",
    "# Carregar o tokenizer\n",
    "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# Carregar o modelo (assumindo que você já tem a classe do modelo definida)\n",
    "model = TextClassifier()  # Substitua pelo nome da sua classe de modelo\n",
    "model.load_state_dict(torch.load('modelo_classificador.pt'))\n",
    "model.to(device)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
