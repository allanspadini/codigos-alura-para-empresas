{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Redes recorrentes"
      ],
      "metadata": {
        "id": "Uo7J27KwE-u9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Ainda √© relevante aprender redes recorrentes (RNNs, LSTMs, GRUs), mas sua\n",
        "\n",
        "*   Item da lista\n",
        "*   Item da lista\n",
        "\n",
        "aplica√ß√£o diminuiu bastante com a ascens√£o das redes Transformer. Aqui est√£o alguns pontos para considerar:  \n",
        "\n",
        "### üìâ **Por que as RNNs perderam espa√ßo?**  \n",
        "- **Efici√™ncia**: Transformers processam tokens em paralelo, enquanto RNNs s√£o sequenciais, tornando-as mais lentas para treinar.  \n",
        "- **Depend√™ncia de longo prazo**: RNNs t√™m dificuldade em capturar depend√™ncias de longo prazo, problema que LSTMs/GRUs tentam mitigar, mas ainda n√£o resolvem completamente.  \n",
        "- **Atua√ß√£o em NLP**: Modelos baseados em Transformer (BERT, GPT, T5) superaram as RNNs em praticamente todas as tarefas de PLN.  \n",
        "\n",
        "### üìå **Ainda vale a pena aprender?**  \n",
        "Sim, mas com foco mais espec√≠fico:  \n",
        "- **S√©ries temporais**: Modelos baseados em RNNs/LSTMs ainda s√£o usados para previs√µes financeiras, meteorol√≥gicas e outras aplica√ß√µes onde os Transformers nem sempre s√£o a melhor escolha.  \n",
        "- **Modelos h√≠bridos**: Algumas arquiteturas combinam CNNs, RNNs e Transformers para tarefas multimodais.  \n",
        "- **Entendimento conceitual**: Conhecer RNNs ajuda a entender a evolu√ß√£o do Deep Learning e algumas limita√ß√µes que os Transformers resolveram.  \n",
        "\n",
        "Se seu foco for NLP, vis√£o computacional ou IA generativa, priorize Transformers. Mas se trabalhar com s√©ries temporais ou sistemas embarcados, RNNs ainda podem ser √∫teis. üöÄ"
      ],
      "metadata": {
        "id": "Hi79FzlGGTq7"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "https://www.kaggle.com/code/taavikalaluka/ai-vs-human-text-pytorch-lstm-cnn-99-8-acc\n",
        "\n",
        "\n",
        "https://www.kaggle.com/datasets/shanegerami/ai-vs-human-text\n"
      ],
      "metadata": {
        "id": "RmNx2kvnghru"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "6h8Ra2RzE4V1"
      },
      "outputs": [],
      "source": [
        "corpus = [\n",
        "    \"Ahoy matey! Hand over the treasure or walk the plank!\",\n",
        "    \"Shiver me timbers! A giant squid off the starboard bow!\",\n",
        "    \"Yo-ho-ho and a bottle of rum!\",\n",
        "    \"Avast ye landlubber! Where be the golden doubloons?\",\n",
        "    \"Shark-infested waters ahead! Batten down the hatches!\",\n",
        "    \"Arrr! The kraken wakes! Man the cannons!\",\n",
        "    \"Dead men tell no tales, but live ones sing shanties!\",\n",
        "    \"Hoist the Jolly Roger! We sail with the morning tide!\"\n",
        "]"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Pr√©-processamento"
      ],
      "metadata": {
        "id": "bilQPI06gjL3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from collections import Counter\n",
        "\n",
        "# Tokeniza√ß√£o e vocabul√°rio\n",
        "tokens = [word.lower() for sentence in corpus for word in sentence.split()]\n",
        "vocab = Counter(tokens)\n",
        "vocab = sorted(vocab, key=vocab.get, reverse=True)\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# Mapeamento palavra para √≠ndice\n",
        "word_to_idx = {word: i for i, word in enumerate(vocab)}\n",
        "idx_to_word = {i: word for i, word in enumerate(vocab)}"
      ],
      "metadata": {
        "id": "JTlrU64zgfl2"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Modelo de Linguagem"
      ],
      "metadata": {
        "id": "kfG47tythhqU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PirateLM(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim=64, hidden_dim=128):\n",
        "        super().__init__()\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, vocab_size)\n",
        "\n",
        "    def forward(self, x, hidden=None):\n",
        "        x = self.embedding(x)\n",
        "        out, hidden = self.lstm(x, hidden)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden"
      ],
      "metadata": {
        "id": "gvLIISmvhV6N"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset e DataLoader"
      ],
      "metadata": {
        "id": "B8eyldgch-J4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PirateDataset(Dataset):\n",
        "    def __init__(self, corpus, seq_length=3):\n",
        "        self.seq_length = seq_length\n",
        "        self.data = []\n",
        "\n",
        "        for sentence in corpus:\n",
        "            tokens = sentence.lower().split()\n",
        "            indices = [word_to_idx[word] for word in tokens]\n",
        "            for i in range(len(indices) - self.seq_length):\n",
        "                self.data.append((\n",
        "                    torch.tensor(indices[i:i+self.seq_length]),\n",
        "                    torch.tensor(indices[i+1:i+1+self.seq_length])\n",
        "                ))\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.data[idx]\n",
        "\n",
        "dataset = PirateDataset(corpus, seq_length=3)\n",
        "dataloader = DataLoader(dataset, batch_size=2, shuffle=True)"
      ],
      "metadata": {
        "id": "t1kxUivth6ZL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### **Fun√ß√£o do PirateDataset**\n",
        "O `PirateDataset` organiza o texto tokenizado em **sequ√™ncias de treinamento** para o modelo. Ele:\n",
        "1. Divide o texto em **sequ√™ncias de palavras** (tokens) de comprimento fixo (`seq_length`).\n",
        "2. Cria pares de **entrada** (contexto) e **alvo** (pr√≥xima palavra) para treinar o modelo a prever a pr√≥xima palavra com base nas anteriores.\n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo Pr√°tico**\n",
        "Vamos usar a primeira frase do corpus:\n",
        "```python\n",
        "\"Ahoy matey! Hand over the treasure or walk the plank!\"\n",
        "```\n",
        "\n",
        "#### Passo a Passo:\n",
        "1. **Tokeniza√ß√£o** (ap√≥s pr√©-processamento):\n",
        "   ```python\n",
        "   [\"ahoy\", \"matey!\", \"hand\", \"over\", \"the\", \"treasure\", \"or\", \"walk\", \"the\", \"plank!\"]\n",
        "   ```\n",
        "\n",
        "2. **Mapeamento para √çndices** (exemplo fict√≠cio do vocabul√°rio):\n",
        "   ```python\n",
        "   {\n",
        "       \"ahoy\": 0,\n",
        "       \"matey!\": 1,\n",
        "       \"hand\": 2,\n",
        "       \"over\": 3,\n",
        "       \"the\": 4,\n",
        "       \"...\": ...\n",
        "   }\n",
        "   ```\n",
        "\n",
        "3. **Sequ√™ncias Geradas** (com `seq_length = 3`):\n",
        "   | Entrada (Input)      | Alvo (Target)       |\n",
        "   |----------------------|---------------------|\n",
        "   | `[0, 1, 2]`          | `[1, 2, 3]`         |\n",
        "   | `[1, 2, 3]`          | `[2, 3, 4]`         |\n",
        "   | `[2, 3, 4]`          | `[3, 4, 5]`         |\n",
        "   | `[3, 4, 5]`          | `[4, 5, 6]`         |\n",
        "   | ... (e assim por diante) | ... |\n",
        "\n",
        "---\n",
        "\n",
        "### **Explica√ß√£o Detalhada**\n",
        "#### 1. Estrutura Interna do Dataset:\n",
        "Cada entrada √© uma sequ√™ncia de `seq_length` palavras, e o alvo √© a **mesma sequ√™ncia deslocada 1 posi√ß√£o para frente**. Isso for√ßa o modelo a aprender a prever a pr√≥xima palavra em cada passo.\n",
        "\n",
        "#### 2. Por Que Isso Funciona?\n",
        "- O modelo recebe `[ahoy, matey!, hand]` e tenta prever `[matey!, hand, over]`.\n",
        "- Na pr√°tica, para cada posi√ß√£o na sequ√™ncia de entrada, o modelo aprende a prever a pr√≥xima palavra (autoregress√£o).\n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo Completo (C√≥digo + Sa√≠da)**\n",
        "Suponha que temos o seguinte vocabul√°rio mapeado:\n",
        "```python\n",
        "word_to_idx = {\n",
        "    \"ahoy\": 0, \"matey!\": 1, \"hand\": 2, \"over\": 3,\n",
        "    \"the\": 4, \"treasure\": 5, \"or\": 6, \"walk\": 7, \"plank!\": 8\n",
        "}\n",
        "```\n",
        "\n",
        "#### Dados Gerados para a Frase:\n",
        "```python\n",
        "# Sequ√™ncia Original (√≠ndices):\n",
        "[0, 1, 2, 3, 4, 5, 6, 7, 4, 8]\n",
        "\n",
        "# Exemplos de pares (input, target):\n",
        "# Cada linha √© um elemento do dataset:\n",
        "[\n",
        "    (tensor([0, 1, 2]), tensor([1, 2, 3])),\n",
        "    (tensor([1, 2, 3]), tensor([2, 3, 4])),\n",
        "    (tensor([2, 3, 4]), tensor([3, 4, 5])),\n",
        "    (tensor([3, 4, 5]), tensor([4, 5, 6])),\n",
        "    (tensor([4, 5, 6]), tensor([5, 6, 7])),\n",
        "    (tensor([5, 6, 7]), tensor([6, 7, 4])),\n",
        "    (tensor([6, 7, 4]), tensor([7, 4, 8]))\n",
        "]\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "### **Visualiza√ß√£o do Funcionamento**\n",
        "Para a entrada `[0, 1, 2]` (\"ahoy\", \"matey!\", \"hand\"):\n",
        "- O modelo tenta prever `[1, 2, 3]` (\"matey!\", \"hand\", \"over\").\n",
        "\n",
        "Isso cria um **deslizamento de janela** sobre o texto, garantindo que o modelo aprenda padr√µes locais e sequenciais do corpus.\n",
        "\n",
        "---\n",
        "\n",
        "### **Por Que Usar `seq_length`?**\n",
        "- Define o **contexto m√°ximo** que o modelo usar√° para prever a pr√≥xima palavra.\n",
        "- Valores pequenos (ex: 3) focam em padr√µes locais; valores maiores capturam depend√™ncias de longo prazo (mas exigem mais dados)."
      ],
      "metadata": {
        "id": "46fDZSr9l8qU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Treinamento"
      ],
      "metadata": {
        "id": "lOR6hDoaidA1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = PirateLM(vocab_size).to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "epochs = 200\n",
        "for epoch in range(epochs):\n",
        "    for inputs, targets in dataloader:\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        output, _ = model(inputs)\n",
        "        loss = criterion(output.view(-1, vocab_size), targets.view(-1))\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "\n",
        "    print(f'Epoch {epoch+1}, Loss: {loss.item():.4f}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oUYFGlZdiBwN",
        "outputId": "edf0547a-9d80-4881-dff9-53e0a7ae669e"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1, Loss: 3.9766\n",
            "Epoch 2, Loss: 3.7064\n",
            "Epoch 3, Loss: 3.2192\n",
            "Epoch 4, Loss: 2.7276\n",
            "Epoch 5, Loss: 2.0673\n",
            "Epoch 6, Loss: 1.6430\n",
            "Epoch 7, Loss: 1.2928\n",
            "Epoch 8, Loss: 0.5196\n",
            "Epoch 9, Loss: 0.8460\n",
            "Epoch 10, Loss: 0.7868\n",
            "Epoch 11, Loss: 0.3296\n",
            "Epoch 12, Loss: 0.5313\n",
            "Epoch 13, Loss: 0.3089\n",
            "Epoch 14, Loss: 0.1987\n",
            "Epoch 15, Loss: 0.2380\n",
            "Epoch 16, Loss: 0.1463\n",
            "Epoch 17, Loss: 0.2000\n",
            "Epoch 18, Loss: 0.4452\n",
            "Epoch 19, Loss: 0.0715\n",
            "Epoch 20, Loss: 0.0786\n",
            "Epoch 21, Loss: 0.0669\n",
            "Epoch 22, Loss: 0.0481\n",
            "Epoch 23, Loss: 0.2804\n",
            "Epoch 24, Loss: 0.0424\n",
            "Epoch 25, Loss: 0.0341\n",
            "Epoch 26, Loss: 0.0310\n",
            "Epoch 27, Loss: 0.2695\n",
            "Epoch 28, Loss: 0.0225\n",
            "Epoch 29, Loss: 0.0444\n",
            "Epoch 30, Loss: 0.1813\n",
            "Epoch 31, Loss: 0.0231\n",
            "Epoch 32, Loss: 0.0247\n",
            "Epoch 33, Loss: 0.2408\n",
            "Epoch 34, Loss: 0.0160\n",
            "Epoch 35, Loss: 0.0158\n",
            "Epoch 36, Loss: 0.1778\n",
            "Epoch 37, Loss: 0.0144\n",
            "Epoch 38, Loss: 0.0129\n",
            "Epoch 39, Loss: 0.0131\n",
            "Epoch 40, Loss: 0.0095\n",
            "Epoch 41, Loss: 0.2305\n",
            "Epoch 42, Loss: 0.0121\n",
            "Epoch 43, Loss: 0.1531\n",
            "Epoch 44, Loss: 0.0107\n",
            "Epoch 45, Loss: 0.0110\n",
            "Epoch 46, Loss: 0.0096\n",
            "Epoch 47, Loss: 0.0146\n",
            "Epoch 48, Loss: 0.0111\n",
            "Epoch 49, Loss: 0.0063\n",
            "Epoch 50, Loss: 0.2059\n",
            "Epoch 51, Loss: 0.0098\n",
            "Epoch 52, Loss: 0.0062\n",
            "Epoch 53, Loss: 0.0071\n",
            "Epoch 54, Loss: 0.0084\n",
            "Epoch 55, Loss: 0.0142\n",
            "Epoch 56, Loss: 0.0075\n",
            "Epoch 57, Loss: 0.1761\n",
            "Epoch 58, Loss: 0.2292\n",
            "Epoch 59, Loss: 0.0046\n",
            "Epoch 60, Loss: 0.0052\n",
            "Epoch 61, Loss: 0.0056\n",
            "Epoch 62, Loss: 0.0049\n",
            "Epoch 63, Loss: 0.0053\n",
            "Epoch 64, Loss: 0.0046\n",
            "Epoch 65, Loss: 0.2256\n",
            "Epoch 66, Loss: 0.0043\n",
            "Epoch 67, Loss: 0.0048\n",
            "Epoch 68, Loss: 0.1743\n",
            "Epoch 69, Loss: 0.0051\n",
            "Epoch 70, Loss: 0.0050\n",
            "Epoch 71, Loss: 0.0032\n",
            "Epoch 72, Loss: 0.0057\n",
            "Epoch 73, Loss: 0.2475\n",
            "Epoch 74, Loss: 0.0033\n",
            "Epoch 75, Loss: 0.0047\n",
            "Epoch 76, Loss: 0.0046\n",
            "Epoch 77, Loss: 0.0051\n",
            "Epoch 78, Loss: 0.0036\n",
            "Epoch 79, Loss: 0.0028\n",
            "Epoch 80, Loss: 0.0028\n",
            "Epoch 81, Loss: 0.0029\n",
            "Epoch 82, Loss: 0.1699\n",
            "Epoch 83, Loss: 0.1778\n",
            "Epoch 84, Loss: 0.0045\n",
            "Epoch 85, Loss: 0.0026\n",
            "Epoch 86, Loss: 0.3817\n",
            "Epoch 87, Loss: 0.2123\n",
            "Epoch 88, Loss: 0.2142\n",
            "Epoch 89, Loss: 0.0033\n",
            "Epoch 90, Loss: 0.0022\n",
            "Epoch 91, Loss: 0.0034\n",
            "Epoch 92, Loss: 0.0020\n",
            "Epoch 93, Loss: 0.0028\n",
            "Epoch 94, Loss: 0.0038\n",
            "Epoch 95, Loss: 0.3600\n",
            "Epoch 96, Loss: 0.0026\n",
            "Epoch 97, Loss: 0.0029\n",
            "Epoch 98, Loss: 0.0023\n",
            "Epoch 99, Loss: 0.0017\n",
            "Epoch 100, Loss: 0.0019\n",
            "Epoch 101, Loss: 0.1377\n",
            "Epoch 102, Loss: 0.0023\n",
            "Epoch 103, Loss: 0.0017\n",
            "Epoch 104, Loss: 0.0017\n",
            "Epoch 105, Loss: 0.0029\n",
            "Epoch 106, Loss: 0.0024\n",
            "Epoch 107, Loss: 0.0018\n",
            "Epoch 108, Loss: 0.0013\n",
            "Epoch 109, Loss: 0.2384\n",
            "Epoch 110, Loss: 0.2232\n",
            "Epoch 111, Loss: 0.0014\n",
            "Epoch 112, Loss: 0.0015\n",
            "Epoch 113, Loss: 0.2270\n",
            "Epoch 114, Loss: 0.0017\n",
            "Epoch 115, Loss: 0.0014\n",
            "Epoch 116, Loss: 0.1677\n",
            "Epoch 117, Loss: 0.0016\n",
            "Epoch 118, Loss: 0.0016\n",
            "Epoch 119, Loss: 0.0017\n",
            "Epoch 120, Loss: 0.0019\n",
            "Epoch 121, Loss: 0.0015\n",
            "Epoch 122, Loss: 0.0012\n",
            "Epoch 123, Loss: 0.0010\n",
            "Epoch 124, Loss: 0.0010\n",
            "Epoch 125, Loss: 0.0011\n",
            "Epoch 126, Loss: 0.1304\n",
            "Epoch 127, Loss: 0.0013\n",
            "Epoch 128, Loss: 0.0012\n",
            "Epoch 129, Loss: 0.0009\n",
            "Epoch 130, Loss: 0.0007\n",
            "Epoch 131, Loss: 0.0016\n",
            "Epoch 132, Loss: 0.0009\n",
            "Epoch 133, Loss: 0.0009\n",
            "Epoch 134, Loss: 0.0016\n",
            "Epoch 135, Loss: 0.0012\n",
            "Epoch 136, Loss: 0.2231\n",
            "Epoch 137, Loss: 0.0008\n",
            "Epoch 138, Loss: 0.1437\n",
            "Epoch 139, Loss: 0.0015\n",
            "Epoch 140, Loss: 0.0011\n",
            "Epoch 141, Loss: 0.0012\n",
            "Epoch 142, Loss: 0.0011\n",
            "Epoch 143, Loss: 0.0008\n",
            "Epoch 144, Loss: 0.2308\n",
            "Epoch 145, Loss: 0.0005\n",
            "Epoch 146, Loss: 0.0014\n",
            "Epoch 147, Loss: 0.0016\n",
            "Epoch 148, Loss: 0.0008\n",
            "Epoch 149, Loss: 0.0009\n",
            "Epoch 150, Loss: 0.0008\n",
            "Epoch 151, Loss: 0.2017\n",
            "Epoch 152, Loss: 0.3608\n",
            "Epoch 153, Loss: 0.0008\n",
            "Epoch 154, Loss: 0.1342\n",
            "Epoch 155, Loss: 0.0010\n",
            "Epoch 156, Loss: 0.0007\n",
            "Epoch 157, Loss: 0.0006\n",
            "Epoch 158, Loss: 0.0007\n",
            "Epoch 159, Loss: 0.0007\n",
            "Epoch 160, Loss: 0.0008\n",
            "Epoch 161, Loss: 0.0005\n",
            "Epoch 162, Loss: 0.0007\n",
            "Epoch 163, Loss: 0.0013\n",
            "Epoch 164, Loss: 0.2262\n",
            "Epoch 165, Loss: 0.2106\n",
            "Epoch 166, Loss: 0.0007\n",
            "Epoch 167, Loss: 0.0010\n",
            "Epoch 168, Loss: 0.1205\n",
            "Epoch 169, Loss: 0.0008\n",
            "Epoch 170, Loss: 0.0006\n",
            "Epoch 171, Loss: 0.0006\n",
            "Epoch 172, Loss: 0.0005\n",
            "Epoch 173, Loss: 0.0010\n",
            "Epoch 174, Loss: 0.2314\n",
            "Epoch 175, Loss: 0.0013\n",
            "Epoch 176, Loss: 0.0009\n",
            "Epoch 177, Loss: 0.0011\n",
            "Epoch 178, Loss: 0.0006\n",
            "Epoch 179, Loss: 0.0008\n",
            "Epoch 180, Loss: 0.0004\n",
            "Epoch 181, Loss: 0.0004\n",
            "Epoch 182, Loss: 0.1439\n",
            "Epoch 183, Loss: 0.0007\n",
            "Epoch 184, Loss: 0.0005\n",
            "Epoch 185, Loss: 0.0004\n",
            "Epoch 186, Loss: 0.0005\n",
            "Epoch 187, Loss: 0.0004\n",
            "Epoch 188, Loss: 0.0003\n",
            "Epoch 189, Loss: 0.0010\n",
            "Epoch 190, Loss: 0.0004\n",
            "Epoch 191, Loss: 0.2141\n",
            "Epoch 192, Loss: 0.0007\n",
            "Epoch 193, Loss: 0.0003\n",
            "Epoch 194, Loss: 0.0004\n",
            "Epoch 195, Loss: 0.0004\n",
            "Epoch 196, Loss: 0.1983\n",
            "Epoch 197, Loss: 0.0003\n",
            "Epoch 198, Loss: 0.0002\n",
            "Epoch 199, Loss: 0.2379\n",
            "Epoch 200, Loss: 0.0004\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Fun√ß√£o de completar texto"
      ],
      "metadata": {
        "id": "3EDUNEMlilr4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def complete_text(seed_text, num_words=5, temperature=0.7):\n",
        "    model.eval()\n",
        "    words = seed_text.lower().split()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for _ in range(num_words):\n",
        "            inputs = torch.tensor(\n",
        "                [word_to_idx[word] for word in words[-3:]]  # Usa contexto de 3 palavras\n",
        "            ).unsqueeze(0).to(device)\n",
        "\n",
        "            output, _ = model(inputs)\n",
        "            probabilities = torch.softmax(output[0, -1] / temperature, dim=0)\n",
        "            next_idx = torch.multinomial(probabilities, 1).item()\n",
        "\n",
        "            words.append(idx_to_word[next_idx])\n",
        "\n",
        "    return \" \".join(words).capitalize()"
      ],
      "metadata": {
        "id": "n9G3PWMhifHp"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Testando"
      ],
      "metadata": {
        "id": "JVNhqxSijNJc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exemplos de prompts\n",
        "print(complete_text(\"Shiver me\", num_words=5))        # Ex: \"Shiver me timbers! A giant squid off\"\n",
        "print(complete_text(\"Yo-ho-ho\", num_words=3))         # Ex: \"Yo-ho-ho and a bottle\"\n",
        "print(complete_text(\"Avast ye\", num_words=4))         # Ex: \"Avast ye landlubber! Where be\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eKSGen45jKHB",
        "outputId": "98f85a4b-b462-4006-c311-1c78b7c16035"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Shiver me timbers! a giant squid off\n",
            "Yo-ho-ho and a bottle\n",
            "Avast ye landlubber! where be the\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3LTcEDv1xqGo"
      },
      "execution_count": 39,
      "outputs": []
    }
  ]
}