{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "!pip install pdfplumber gradio"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_lEEtHGz1F0m",
        "outputId": "34f35c48-020c-44db-d44d-d556412d0688"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pdfplumber in /usr/local/lib/python3.11/dist-packages (0.11.5)\n",
            "Collecting gradio\n",
            "  Downloading gradio-5.15.0-py3-none-any.whl.metadata (16 kB)\n",
            "Requirement already satisfied: pdfminer.six==20231228 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (20231228)\n",
            "Requirement already satisfied: Pillow>=9.1 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (11.1.0)\n",
            "Requirement already satisfied: pypdfium2>=4.18.0 in /usr/local/lib/python3.11/dist-packages (from pdfplumber) (4.30.1)\n",
            "Requirement already satisfied: charset-normalizer>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (3.4.1)\n",
            "Requirement already satisfied: cryptography>=36.0.0 in /usr/local/lib/python3.11/dist-packages (from pdfminer.six==20231228->pdfplumber) (43.0.3)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio)\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.7.1)\n",
            "Collecting fastapi<1.0,>=0.115.2 (from gradio)\n",
            "  Downloading fastapi-0.115.8-py3-none-any.whl.metadata (27 kB)\n",
            "Collecting ffmpy (from gradio)\n",
            "  Downloading ffmpy-0.5.0-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting gradio-client==1.7.0 (from gradio)\n",
            "  Downloading gradio_client-1.7.0-py3-none-any.whl.metadata (7.1 kB)\n",
            "Requirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\n",
            "Collecting huggingface-hub>=0.28.1 (from gradio)\n",
            "  Downloading huggingface_hub-0.28.1-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.5)\n",
            "Collecting markupsafe~=2.0 (from gradio)\n",
            "  Downloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\n",
            "Requirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.15)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\n",
            "Requirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.2)\n",
            "Requirement already satisfied: pydantic>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.6)\n",
            "Collecting pydub (from gradio)\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl.metadata (1.4 kB)\n",
            "Collecting python-multipart>=0.0.18 (from gradio)\n",
            "  Downloading python_multipart-0.0.20-py3-none-any.whl.metadata (1.8 kB)\n",
            "Requirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\n",
            "Collecting ruff>=0.9.3 (from gradio)\n",
            "  Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (25 kB)\n",
            "Collecting safehttpx<0.2.0,>=0.1.6 (from gradio)\n",
            "  Downloading safehttpx-0.1.6-py3-none-any.whl.metadata (4.2 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio)\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl.metadata (9.7 kB)\n",
            "Collecting starlette<1.0,>=0.40.0 (from gradio)\n",
            "  Downloading starlette-0.45.3-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting tomlkit<0.14.0,>=0.12.0 (from gradio)\n",
            "  Downloading tomlkit-0.13.2-py3-none-any.whl.metadata (2.7 kB)\n",
            "Requirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.15.1)\n",
            "Requirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.12.2)\n",
            "Collecting uvicorn>=0.14.0 (from gradio)\n",
            "  Downloading uvicorn-0.34.0-py3-none-any.whl.metadata (6.5 kB)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (2024.10.0)\n",
            "Requirement already satisfied: websockets<15.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.7.0->gradio) (14.2)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2024.12.14)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.7)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.14.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.17.0)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.3)\n",
            "Requirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic>=2.0->gradio) (2.27.2)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.1.8)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (13.9.4)\n",
            "Requirement already satisfied: cffi>=1.12 in /usr/local/lib/python3.11/dist-packages (from cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (1.17.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.18.0)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.3.0)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.12->cryptography>=36.0.0->pdfminer.six==20231228->pdfplumber) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
            "Downloading gradio-5.15.0-py3-none-any.whl (57.8 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m57.8/57.8 MB\u001b[0m \u001b[31m9.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading gradio_client-1.7.0-py3-none-any.whl (321 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.9/321.9 kB\u001b[0m \u001b[31m20.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading fastapi-0.115.8-py3-none-any.whl (94 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading huggingface_hub-0.28.1-py3-none-any.whl (464 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m464.1/464.1 kB\u001b[0m \u001b[31m28.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading MarkupSafe-2.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (28 kB)\n",
            "Downloading python_multipart-0.0.20-py3-none-any.whl (24 kB)\n",
            "Downloading ruff-0.9.4-py3-none-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m84.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading safehttpx-0.1.6-py3-none-any.whl (8.7 kB)\n",
            "Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Downloading starlette-0.45.3-py3-none-any.whl (71 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m4.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tomlkit-0.13.2-py3-none-any.whl (37 kB)\n",
            "Downloading uvicorn-0.34.0-py3-none-any.whl (62 kB)\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ffmpy-0.5.0-py3-none-any.whl (6.0 kB)\n",
            "Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Installing collected packages: pydub, uvicorn, tomlkit, semantic-version, ruff, python-multipart, markupsafe, ffmpy, aiofiles, starlette, huggingface-hub, safehttpx, gradio-client, fastapi, gradio\n",
            "  Attempting uninstall: markupsafe\n",
            "    Found existing installation: MarkupSafe 3.0.2\n",
            "    Uninstalling MarkupSafe-3.0.2:\n",
            "      Successfully uninstalled MarkupSafe-3.0.2\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.27.1\n",
            "    Uninstalling huggingface-hub-0.27.1:\n",
            "      Successfully uninstalled huggingface-hub-0.27.1\n",
            "Successfully installed aiofiles-23.2.1 fastapi-0.115.8 ffmpy-0.5.0 gradio-5.15.0 gradio-client-1.7.0 huggingface-hub-0.28.1 markupsafe-2.1.5 pydub-0.25.1 python-multipart-0.0.20 ruff-0.9.4 safehttpx-0.1.6 semantic-version-2.10.0 starlette-0.45.3 tomlkit-0.13.2 uvicorn-0.34.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "markupsafe"
                ]
              },
              "id": "4c17859057e145b8874dd409ebc0c7bc"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "qCYPNsyNwChS"
      },
      "outputs": [],
      "source": [
        "import spacy\n",
        "from spacy.language import Language\n",
        "from spacy.tokens import Doc, Span\n",
        "from spacy.util import filter_spans\n",
        "import pdfplumber"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# FunÃ§Ã£o para carregar o texto do PDF\n",
        "def extract_text_from_pdf(pdf_path):\n",
        "    with pdfplumber.open(pdf_path) as pdf:\n",
        "        text = \"\"\n",
        "        for page in pdf.pages:\n",
        "            text += page.extract_text()\n",
        "    #text = text.replace(\"\\n\", \" \")\n",
        "    return text"
      ],
      "metadata": {
        "id": "RXoKs5Pk009H"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pdf_path = 'deepseek.pdf'\n",
        "# Extrair texto do PDF\n",
        "text = extract_text_from_pdf(pdf_path)"
      ],
      "metadata": {
        "id": "d-xoite01nHT"
      },
      "execution_count": 71,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "jic96_uB2W59",
        "outputId": "fd98b56b-b887-4c67-ec17-fe6594efcf99"
      },
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via\\nReinforcement Learning\\nDeepSeek-AI\\nresearch@deepseek.com\\nAbstract\\nWe introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1.\\nDeepSeek-R1-Zero,amodeltrainedvialarge-scalereinforcementlearning(RL)withoutsuper-\\nvisedfine-tuning(SFT)asapreliminarystep,demonstratesremarkablereasoningcapabilities.\\nThrough RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing\\nreasoningbehaviors. However,itencounterschallengessuchaspoorreadability,andlanguage\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1,whichincorporatesmulti-stagetrainingandcold-startdatabeforeRL.DeepSeek-\\nR1achievesperformancecomparabletoOpenAI-o1-1217onreasoningtasks. Tosupportthe\\nresearchcommunity,weopen-sourceDeepSeek-R1-Zero,DeepSeek-R1,andsixdensemodels\\n(1.5B,7B,8B,14B,32B,70B)distilledfromDeepSeek-R1basedonQwenandLlama.\\n100\\n80\\n60\\n40\\n20\\n0\\nAIME 2024 Codeforces GPQA Diamond MATH-500 MMLU SWE-bench Verified\\n(Pass@1) (Percentile) (Pass@1) (Pass@1) (Pass@1) (Resolved)\\n)%(\\nelitnecreP\\n/\\nycaruccA\\nDeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3\\n96.396.6 97.396.4\\n93.4 94.3\\n90.6 90.090.2 90.891.8\\n87.4 88.5\\n85.2\\n79.879.2\\n75.7\\n72.6 71.5\\n63.6\\n62.1 58.7 60.059.1\\n49.248.9\\n41.642.0\\n39.2\\n36.8\\nFigure1 | BenchmarkperformanceofDeepSeek-R1.\\n5202\\nnaJ\\n22\\n]LC.sc[\\n1v84921.1052:viXraContents\\n1 Introduction 3\\n1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n1.2 SummaryofEvaluationResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4\\n2 Approach 5\\n2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2 DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel . . . . . . . . . . 5\\n2.2.1 ReinforcementLearningAlgorithm . . . . . . . . . . . . . . . . . . . . . . 5\\n2.2.2 RewardModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.3 TrainingTemplate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6\\n2.2.4 Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero 6\\n2.3 DeepSeek-R1: ReinforcementLearningwithColdStart . . . . . . . . . . . . . . . 9\\n2.3.1 ColdStart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9\\n2.3.2 Reasoning-orientedReinforcementLearning . . . . . . . . . . . . . . . . . 10\\n2.3.3 RejectionSamplingandSupervisedFine-Tuning . . . . . . . . . . . . . . . 10\\n2.3.4 ReinforcementLearningforallScenarios . . . . . . . . . . . . . . . . . . . 11\\n2.4 Distillation: EmpowerSmallModelswithReasoningCapability . . . . . . . . . . 11\\n3 Experiment 11\\n3.1 DeepSeek-R1Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13\\n3.2 DistilledModelEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4 Discussion 14\\n4.1 Distillationv.s. ReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . 14\\n4.2 UnsuccessfulAttempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15\\n5 Conclusion,Limitations,andFutureWork 16\\nA ContributionsandAcknowledgments 20\\n21. Introduction\\nIn recent years, Large Language Models (LLMs) have been undergoing rapid iteration and\\nevolution(Anthropic,2024;Google,2024;OpenAI,2024a),progressivelydiminishingthegap\\ntowardsArtificialGeneralIntelligence(AGI).\\nRecently,post-traininghasemergedasanimportantcomponentofthefulltrainingpipeline.\\nIthasbeenshowntoenhanceaccuracyonreasoningtasks,alignwithsocialvalues,andadapt\\nto user preferences, all while requiring relatively minimal computational resources against\\npre-training. Inthecontextofreasoningcapabilities,OpenAIâ€™so1(OpenAI,2024b)seriesmodels\\nwere the first to introduce inference-time scaling by increasing the length of the Chain-of-\\nThoughtreasoningprocess. Thisapproachhasachievedsignificantimprovementsinvarious\\nreasoningtasks,suchasmathematics,coding,andscientificreasoning. However,thechallenge\\nofeffectivetest-timescalingremainsanopenquestionfortheresearchcommunity. Severalprior\\nworkshaveexploredvariousapproaches,includingprocess-basedrewardmodels(Lightman\\netal.,2023;Uesatoetal.,2022;Wangetal.,2023),reinforcementlearning(Kumaretal.,2024),\\nandsearchalgorithmssuchasMonteCarloTreeSearchandBeamSearch(Fengetal.,2024;Trinh\\netal.,2024;Xinetal.,2024). However,noneofthesemethodshasachievedgeneralreasoning\\nperformancecomparabletoOpenAIâ€™so1seriesmodels.\\nInthispaper,wetakethefirststeptowardimprovinglanguagemodelreasoningcapabilities\\nusingpurereinforcementlearning(RL).OurgoalistoexplorethepotentialofLLMstodevelop\\nreasoningcapabilitieswithoutanysuperviseddata,focusingontheirself-evolutionthrough\\na pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ\\nGRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning.\\nDuringtraining,DeepSeek-R1-Zeronaturallyemergedwithnumerouspowerfulandinteresting\\nreasoningbehaviors. AfterthousandsofRLsteps,DeepSeek-R1-Zeroexhibitssuperperformance\\nonreasoningbenchmarks. Forinstance,thepass@1scoreonAIME2024increasesfrom15.6%to\\n71.0%,andwithmajorityvoting,thescorefurtherimprovesto86.7%,matchingtheperformance\\nofOpenAI-o1-0912.\\nHowever,DeepSeek-R1-Zeroencounterschallengessuchaspoorreadability,andlanguage\\nmixing. To address these issues and further enhance reasoning performance, we introduce\\nDeepSeek-R1,whichincorporatesasmallamountofcold-startdataandamulti-stagetraining\\npipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the\\nDeepSeek-V3-Basemodel. Followingthis,weperformreasoning-orientedRLlikeDeepSeek-R1-\\nZero. UponnearingconvergenceintheRLprocess,wecreatenewSFTdatathroughrejection\\nsamplingontheRLcheckpoint,combinedwithsuperviseddatafromDeepSeek-V3indomains\\nsuchaswriting,factualQA,andself-cognition,andthenretraintheDeepSeek-V3-Basemodel.\\nAfterfine-tuningwiththenewdata,thecheckpointundergoesanadditionalRLprocess,taking\\nintoaccountpromptsfromallscenarios. Afterthesesteps,weobtainedacheckpointreferredto\\nasDeepSeek-R1,whichachievesperformanceonparwithOpenAI-o1-1217.\\nWefurtherexploredistillationfromDeepSeek-R1tosmallerdensemodels. UsingQwen2.5-\\n32B(Qwen,2024b)asthebasemodel,directdistillationfromDeepSeek-R1outperformsapplying\\nRLonit. Thisdemonstratesthatthereasoningpatternsdiscoveredbylargerbasemodelsarecru-\\ncialforimprovingreasoningcapabilities. Weopen-sourcethedistilledQwenandLlama(Dubey\\netal.,2024)series. Notably,ourdistilled14Bmodeloutperformsstate-of-the-artopen-source\\nQwQ-32B-Preview(Qwen,2024a)byalargemargin,andthedistilled32Band70Bmodelsseta\\nnewrecordonthereasoningbenchmarksamongdensemodels.\\n31.1. Contributions\\nPost-Training: Large-ScaleReinforcementLearningontheBaseModel\\nâ€¢ WedirectlyapplyRLtothebasemodelwithoutrelyingonsupervisedfine-tuning(SFT)as\\napreliminarystep. Thisapproachallowsthemodeltoexplorechain-of-thought(CoT)for\\nsolvingcomplexproblems,resultinginthedevelopmentofDeepSeek-R1-Zero. DeepSeek-\\nR1-Zero demonstrates capabilities such as self-verification, reflection, and generating\\nlongCoTs,markingasignificantmilestonefortheresearchcommunity. Notably,itisthe\\nfirst open research to validate that reasoning capabilities of LLMs can be incentivized\\npurelythroughRL,withouttheneedforSFT.Thisbreakthroughpavesthewayforfuture\\nadvancementsinthisarea.\\nâ€¢ We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL\\nstagesaimedatdiscoveringimprovedreasoningpatternsandaligningwithhumanpref-\\nerences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and\\nnon-reasoningcapabilities. Webelievethepipelinewillbenefittheindustrybycreating\\nbettermodels.\\nDistillation: SmallerModelsCanBePowerfulToo\\nâ€¢ Wedemonstratethatthereasoningpatternsoflargermodelscanbedistilledintosmaller\\nmodels,resultinginbetterperformancecomparedtothereasoningpatternsdiscovered\\nthroughRLonsmallmodels. TheopensourceDeepSeek-R1,aswellasitsAPI,willbenefit\\ntheresearchcommunitytodistillbettersmallermodelsinthefuture.\\nâ€¢ UsingthereasoningdatageneratedbyDeepSeek-R1,wefine-tunedseveraldensemodels\\nthatarewidelyusedintheresearchcommunity. Theevaluationresultsdemonstratethat\\nthedistilledsmallerdensemodelsperformexceptionallywellonbenchmarks. DeepSeek-\\nR1-Distill-Qwen-7Bachieves55.5%onAIME2024,surpassingQwQ-32B-Preview. Addi-\\ntionally,DeepSeek-R1-Distill-Qwen-32Bscores72.6%onAIME2024,94.3%onMATH-500,\\nand 57.2% on LiveCodeBench. These results significantly outperform previous open-\\nsourcemodelsandarecomparabletoo1-mini. Weopen-sourcedistilled1.5B,7B,8B,14B,\\n32B,and70BcheckpointsbasedonQwen2.5andLlama3seriestothecommunity.\\n1.2. SummaryofEvaluationResults\\nâ€¢ Reasoningtasks: (1)DeepSeek-R1achievesascoreof79.8%Pass@1onAIME2024,slightly\\nsurpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%,\\nperformingonparwithOpenAI-o1-1217andsignificantlyoutperformingothermodels. (2)\\nOncoding-relatedtasks,DeepSeek-R1demonstratesexpertlevelincodecompetitiontasks,\\nasitachieves2,029EloratingonCodeforcesoutperforming96.3%humanparticipantsin\\nthecompetition. Forengineering-relatedtasks,DeepSeek-R1performsslightlybetterthan\\nDeepSeek-V3,whichcouldhelpdevelopersinrealworldtasks.\\nâ€¢ Knowledge: OnbenchmarkssuchasMMLU,MMLU-Pro,andGPQADiamond,DeepSeek-\\nR1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores\\nof 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its\\nperformanceisslightlybelowthatofOpenAI-o1-1217onthesebenchmarks,DeepSeek-R1\\nsurpassesotherclosed-sourcemodels,demonstratingitscompetitiveedgeineducational\\ntasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3,\\ndemonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobserved\\nwhereOpenAI-o1surpasses4oonthisbenchmark.\\n4â€¢ Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing,\\ngeneralquestionanswering,editing,summarization,andmore. Itachievesanimpressive\\nlength-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are-\\nnaHard,showcasingitsstrongabilitytointelligentlyhandlenon-exam-orientedqueries.\\nAdditionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring\\nlong-contextunderstanding,substantiallyoutperformingDeepSeek-V3onlong-context\\nbenchmarks.\\n2. Approach\\n2.1. Overview\\nPrevious work has heavily relied on large amounts of supervised data to enhance model\\nperformance. In this study, we demonstrate that reasoning capabilities can be significantly\\nimproved through large-scale reinforcement learning (RL), even without using supervised\\nfine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with\\nthe inclusion of a small amount of cold-start data. In the following sections, we present: (1)\\nDeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and\\n(2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of\\nlongChain-of-Thought(CoT)examples. 3)DistillthereasoningcapabilityfromDeepSeek-R1to\\nsmalldensemodels.\\n2.2. DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel\\nReinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev-\\nidenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works\\nheavilydependedonsuperviseddata,whicharetime-intensivetogather. Inthissection,we\\nexplorethepotentialofLLMstodevelopreasoningcapabilitieswithoutanysuperviseddata,\\nfocusingontheirself-evolutionthroughapurereinforcementlearningprocess. Westartwitha\\nbriefoverviewofourRLalgorithm,followedbythepresentationofsomeexcitingresults,and\\nhopethisprovidesthecommunitywithvaluableinsights.\\n2.2.1. ReinforcementLearningAlgorithm\\nGroupRelativePolicyOptimization InordertosavethetrainingcostsofRL,weadoptGroup\\nRelativePolicyOptimization(GRPO)(Shaoetal.,2024),whichforegoesthecriticmodelthatis\\ntypicallythesamesizeasthepolicymodel,andestimatesthebaselinefromgroupscoresinstead.\\nSpecifically,foreachquestionğ‘,GRPOsamplesagroupofoutputs {ğ‘œ 1,ğ‘œ 2,Â·Â·Â· ,ğ‘œ ğº} fromtheold\\npolicyğœ‹ ğœƒ andthenoptimizesthepolicymodelğœ‹ ğœƒ bymaximizingthefollowingobjective:\\nğ‘œğ‘™ğ‘‘\\nJ ğºğ‘…ğ‘ƒğ‘‚(ğœƒ) = E[ğ‘ âˆ¼ ğ‘ƒ(ğ‘„),{ğ‘œ ğ‘–}ğº\\nğ‘–=1\\nâˆ¼ ğœ‹\\nğœƒ\\nğ‘œğ‘™ğ‘‘(ğ‘‚|ğ‘)]\\n1 âˆ‘ï¸ğº (cid:18) min(cid:18) ğœ‹ ğœƒ(ğ‘œ ğ‘–|ğ‘) ğ´ ğ‘–,clip(cid:18) ğœ‹ ğœƒ(ğ‘œ ğ‘–|ğ‘) ,1âˆ’ğœ€,1+ğœ€(cid:19) ğ´ ğ‘–(cid:19) âˆ’ğ›½D ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ||ğœ‹ ğ‘Ÿğ‘’ğ‘“(cid:1)(cid:19) , (1)\\nğº ğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nğœƒ ğ‘– ğœƒ ğ‘–\\nğ‘–=1 ğ‘œğ‘™ğ‘‘ ğ‘œğ‘™ğ‘‘\\nğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nD ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ||ğœ‹ ğ‘Ÿğ‘’ğ‘“(cid:1) = ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’log ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’1, (2)\\nğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘)\\nğœƒ ğ‘– ğœƒ ğ‘–\\nwhere ğœ€ and ğ›½ are hyper-parameters, and ğ´ ğ‘– is the advantage, computed using a group of\\nrewards{ğ‘Ÿ 1,ğ‘Ÿ 2,...,ğ‘Ÿ ğº}correspondingtotheoutputswithineachgroup:\\nğ´ ğ‘– =\\nğ‘Ÿ ğ‘–âˆ’mğ‘’ğ‘ğ‘›({ğ‘Ÿ 1,ğ‘Ÿ 2,Â·Â·Â· ,ğ‘Ÿ ğº})\\n. (3)\\nsğ‘¡ğ‘‘({ğ‘Ÿ 1,ğ‘Ÿ 2,Â·Â·Â· ,ğ‘Ÿ ğº})\\n5AconversationbetweenUserandAssistant. Theuserasksaquestion,andtheAssistantsolvesit.\\nTheassistantfirstthinksaboutthereasoningprocessinthemindandthenprovidestheuser\\nwiththeanswer. Thereasoningprocessandanswerareenclosedwithin<think></think>and\\n<answer></answer>tags,respectively,i.e.,<think>reasoningprocesshere</think>\\n<answer>answerhere</answer>. User: prompt. Assistant:\\nTable1 | TemplateforDeepSeek-R1-Zero. promptwillbereplacedwiththespecificreasoning\\nquestionduringtraining.\\n2.2.2. RewardModeling\\nTherewardisthesourceofthetrainingsignal,whichdecidestheoptimizationdirectionofRL.\\nTotrainDeepSeek-R1-Zero,weadoptarule-basedrewardsystemthatmainlyconsistsoftwo\\ntypesofrewards:\\nâ€¢ Accuracyrewards: Theaccuracyrewardmodelevaluateswhethertheresponseiscorrect.\\nForexample,inthecaseofmathproblemswithdeterministicresults,themodelisrequired\\nto provide the final answer in a specified format (e.g., within a box), enabling reliable\\nrule-basedverificationofcorrectness. Similarly,forLeetCodeproblems,acompilercanbe\\nusedtogeneratefeedbackbasedonpredefinedtestcases.\\nâ€¢ Formatrewards: Inadditiontotheaccuracyrewardmodel,weemployaformatreward\\nmodelthatenforcesthemodeltoputitsthinkingprocessbetweenâ€˜<think>â€™andâ€˜</think>â€™\\ntags.\\nWedonotapplytheoutcomeorprocessneuralrewardmodelindevelopingDeepSeek-R1-Zero,\\nbecausewefindthattheneuralrewardmodelmaysufferfromrewardhackinginthelarge-scale\\nreinforcement learning process, and retraining the reward model needs additional training\\nresourcesanditcomplicatesthewholetrainingpipeline.\\n2.2.3. TrainingTemplate\\nTo train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides\\nthe base model to adhere to our specified instructions. As depicted in Table 1, this template\\nrequiresDeepSeek-R1-Zerotofirstproduceareasoningprocess,followedbythefinalanswer.\\nWeintentionallylimitourconstraintstothisstructuralformat,avoidinganycontent-specific\\nbiasesâ€”suchasmandatingreflectivereasoningorpromotingparticularproblem-solvingstrate-\\ngiesâ€”toensurethatwecanaccuratelyobservethemodelâ€™snaturalprogressionduringtheRL\\nprocess.\\n2.2.4. Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero\\nPerformanceofDeepSeek-R1-Zero Figure2depictstheperformancetrajectoryofDeepSeek-\\nR1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated,\\nDeepSeek-R1-Zerodemonstratesasteadyandconsistentenhancementinperformanceasthe\\nRL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant\\nincrease, jumpingfromaninitial15.6%toanimpressive71.0%, reachingperformancelevels\\ncomparabletoOpenAI-o1-0912. ThissignificantimprovementhighlightstheefficacyofourRL\\nalgorithminoptimizingthemodelâ€™sperformanceovertime.\\nTable2providesacomparativeanalysisbetweenDeepSeek-R1-ZeroandOpenAIâ€™so1-0912\\nmodelsacrossavarietyofreasoning-relatedbenchmarks. ThefindingsrevealthatRLempowers\\n6GPQA LiveCode\\nAIME2024 MATH-500 CodeForces\\nModel Diamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nOpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843\\nDeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444\\nTable2 | ComparisonofDeepSeek-R1-ZeroandOpenAIo1modelsonreasoning-related\\nbenchmarks.\\nFigure2 | AIMEaccuracyofDeepSeek-R1-Zeroduringtraining. Foreachquestion,wesample\\n16responsesandcalculatetheoverallaverageaccuracytoensureastableevaluation.\\nDeepSeek-R1-Zerotoattainrobustreasoningcapabilitieswithouttheneedforanysupervised\\nfine-tuning data. This is a noteworthy achievement, as it underscores the modelâ€™s ability to\\nlearnandgeneralizeeffectivelythroughRLalone. Additionally,theperformanceofDeepSeek-\\nR1-Zero can be further augmented through the application of majority voting. For example,\\nwhenmajorityvotingisemployedontheAIMEbenchmark,DeepSeek-R1-Zeroâ€™sperformance\\nescalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The\\nabilityofDeepSeek-R1-Zerotoachievesuchcompetitiveperformance,bothwithandwithout\\nmajority voting, highlights its strong foundational capabilities and its potential for further\\nadvancementsinreasoningtasks.\\nSelf-evolutionProcessofDeepSeek-R1-Zero Theself-evolutionprocessofDeepSeek-R1-Zero\\nisafascinatingdemonstrationofhowRLcandriveamodeltoimproveitsreasoningcapabilities\\nautonomously. ByinitiatingRLdirectlyfromthebasemodel,wecancloselymonitorthemodelâ€™s\\nprogressionwithouttheinfluenceofthesupervisedfine-tuningstage. Thisapproachprovides\\naclearviewofhowthemodelevolvesovertime,particularlyintermsofitsabilitytohandle\\ncomplexreasoningtasks.\\nAsdepictedinFigure3,thethinkingtimeofDeepSeek-R1-Zeroshowsconsistentimprove-\\n7Figure3 | TheaverageresponselengthofDeepSeek-R1-ZeroonthetrainingsetduringtheRL\\nprocess. DeepSeek-R1-Zeronaturallylearnstosolvereasoningtaskswithmorethinkingtime.\\nmentthroughoutthetrainingprocess. Thisimprovementisnottheresultofexternaladjustments\\nbutratheranintrinsicdevelopmentwithinthemodel. DeepSeek-R1-Zeronaturallyacquiresthe\\nabilitytosolveincreasinglycomplexreasoningtasksbyleveragingextendedtest-timecompu-\\ntation. Thiscomputationrangesfromgeneratinghundredstothousandsofreasoningtokens,\\nallowingthemodeltoexploreandrefineitsthoughtprocessesingreaterdepth.\\nOneofthemostremarkableaspectsofthisself-evolutionistheemergenceofsophisticated\\nbehaviorsasthetest-timecomputationincreases. Behaviorssuchasreflectionâ€”wherethemodel\\nrevisits and reevaluates its previous stepsâ€”and the exploration of alternative approaches to\\nproblem-solvingarisespontaneously. Thesebehaviorsarenotexplicitlyprogrammedbutinstead\\nemergeasaresultofthemodelâ€™sinteractionwiththereinforcementlearningenvironment. This\\nspontaneousdevelopmentsignificantlyenhancesDeepSeek-R1-Zeroâ€™sreasoningcapabilities,\\nenablingittotacklemorechallengingtaskswithgreaterefficiencyandaccuracy.\\nAhaMomentofDeepSeek-R1-Zero Aparticularlyintriguingphenomenonobservedduring\\nthe training of DeepSeek-R1-Zero is the occurrence of an â€œaha momentâ€. This moment, as\\nillustrated in Table 3, occurs in an intermediate version of the model. During this phase,\\nDeepSeek-R1-Zerolearnstoallocatemorethinkingtimetoaproblembyreevaluatingitsinitial\\napproach. This behavior is not only a testament to the modelâ€™s growing reasoning abilities\\nbut also a captivating example of how reinforcement learning can lead to unexpected and\\nsophisticatedoutcomes.\\nThis moment is not only an â€œaha momentâ€ for the model but also for the researchers\\nobservingitsbehavior. Itunderscoresthepowerandbeautyofreinforcementlearning: rather\\nthanexplicitlyteachingthemodelonhowtosolveaproblem,wesimplyprovideitwiththe\\nright incentives, and it autonomously develops advanced problem-solving strategies. The\\nâ€œaha momentâ€ serves as a powerful reminder of the potential of RL to unlock new levels of\\nintelligenceinartificialsystems,pavingthewayformoreautonomousandadaptivemodelsin\\nthefuture.\\n8âˆšï¸ âˆš\\nQuestion: Ifğ‘ > 1,thenthesumoftherealsolutionsof ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥ isequalto\\nResponse: <think>\\nâˆšï¸ âˆš\\nTosolvetheequation ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥,letâ€™sstartbysquaringbothÂ·Â·Â·\\n(cid:16)âˆšï¸ âˆš (cid:17)2 âˆš\\nğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2.\\nRearrangetoisolatetheinnersquarerootterm:\\n(ğ‘âˆ’ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘2âˆ’2ğ‘ğ‘¥2+(ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘¥4âˆ’2ğ‘ğ‘¥2âˆ’ğ‘¥+(ğ‘2âˆ’ğ‘) = 0\\n...\\nWait,wait. Wait. Thatâ€™sanahamomentIcanflaghere.\\nLetâ€™sreevaluatethisstep-by-steptoidentifyifthecorrectsumcanbeÂ·Â·Â·\\nWestartedwiththeequation:\\nâˆšï¸ âˆš\\nğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥\\nFirst,letâ€™ssquarebothsides:\\nâˆš âˆš\\nğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘+ğ‘¥ = ğ‘âˆ’ğ‘¥2\\nNext,Icouldsquarebothsidesagain,treatingtheequation: Â·Â·Â·\\n...\\nTable3 | Aninterestingâ€œahamomentâ€ofanintermediateversionofDeepSeek-R1-Zero. The\\nmodellearnstorethinkusingananthropomorphictone. Thisisalsoanahamomentforus,\\nallowingustowitnessthepowerandbeautyofreinforcementlearning.\\nDrawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning\\ncapabilitiesandautonomouslydevelopsunexpectedandpowerfulreasoningbehaviors,itfaces\\nseveralissues. Forinstance,DeepSeek-R1-Zerostruggleswithchallengeslikepoorreadability,\\nand language mixing. To make reasoning processes more readable and share them with the\\nopen community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly\\ncold-startdata.\\n2.3. DeepSeek-R1: ReinforcementLearningwithColdStart\\nInspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can\\nreasoningperformancebefurtherimprovedorconvergenceacceleratedbyincorporatingasmall\\namount of high-quality data as a cold start? 2) How can we train a user-friendly model that\\nnotonlyproducesclearandcoherentChainsofThought(CoT)butalsodemonstratesstrong\\ngeneralcapabilities? Toaddressthesequestions,wedesignapipelinetotrainDeepSeek-R1. The\\npipelineconsistsoffourstages,outlinedasfollows.\\n2.3.1. ColdStart\\nUnlikeDeepSeek-R1-Zero,topreventtheearlyunstablecoldstartphaseofRLtrainingfrom\\nthe base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data\\nto fine-tune the model as the initial RL actor. To collect such data, we have explored several\\napproaches: using few-shot prompting with a long CoT as an example, directly prompting\\nmodelstogeneratedetailedanswerswithreflectionandverification,gatheringDeepSeek-R1-\\nZerooutputsinareadableformat,andrefiningtheresultsthroughpost-processingbyhuman\\nannotators.\\nInthiswork,wecollectthousandsofcold-startdatatofine-tunetheDeepSeek-V3-Baseas\\nthe starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data\\n9include:\\nâ€¢ Readability: AkeylimitationofDeepSeek-R1-Zeroisthatitscontentisoftennotsuitable\\nfor reading. Responses may mix multiple languages or lack markdown formatting to\\nhighlightanswersforusers. Incontrast,whencreatingcold-startdataforDeepSeek-R1,\\nwedesignareadablepatternthatincludesasummaryattheendofeachresponseand\\nfilters out responses that are not reader-friendly. Here, we define the output format as\\n|special_token|<reasoning_process>|special_token|<summary>,wherethereasoning\\nprocessistheCoTforthequery,andthesummaryisusedtosummarizethereasoning\\nresults.\\nâ€¢ Potential: Bycarefullydesigningthepatternforcold-startdatawithhumanpriors, we\\nobservebetterperformanceagainstDeepSeek-R1-Zero. Webelievetheiterativetrainingis\\nabetterwayforreasoningmodels.\\n2.3.2. Reasoning-orientedReinforcementLearning\\nAfter fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale\\nreinforcementlearningtrainingprocessasemployedinDeepSeek-R1-Zero. Thisphasefocuses\\nonenhancingthemodelâ€™sreasoningcapabilities,particularlyinreasoning-intensivetaskssuch\\nascoding,mathematics,science,andlogicreasoning,whichinvolvewell-definedproblemswith\\nclearsolutions. Duringthetrainingprocess,weobservethatCoToftenexhibitslanguagemixing,\\nparticularlywhenRLpromptsinvolvemultiplelanguages. Tomitigatetheissueoflanguage\\nmixing,weintroducealanguageconsistencyrewardduringRLtraining,whichiscalculated\\nastheproportionoftargetlanguagewordsintheCoT.Althoughablationexperimentsshow\\nthat such alignment results in a slight degradation in the modelâ€™s performance, this reward\\nalignswithhumanpreferences,makingitmorereadable. Finally,wecombinetheaccuracyof\\nreasoningtasksandtherewardforlanguageconsistencybydirectlysummingthemtoformthe\\nfinalreward. WethenapplyRLtrainingonthefine-tunedmodeluntilitachievesconvergence\\nonreasoningtasks.\\n2.3.3. RejectionSamplingandSupervisedFine-Tuning\\nWhen reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT\\n(SupervisedFine-Tuning)dataforthesubsequentround. Unliketheinitialcold-startdata,which\\nprimarilyfocusesonreasoning,thisstageincorporatesdatafromotherdomainstoenhancethe\\nmodelâ€™scapabilitiesinwriting,role-playing,andothergeneral-purposetasks. Specifically,we\\ngeneratethedataandfine-tunethemodelasdescribedbelow.\\nReasoningdata Wecuratereasoningpromptsandgeneratereasoningtrajectoriesbyperform-\\ningrejectionsamplingfromthecheckpointfromtheaboveRLtraining. Inthepreviousstage,\\nweonlyincludeddatathatcouldbeevaluatedusingrule-basedrewards. However,inthisstage,\\nweexpandthedatasetbyincorporatingadditionaldata,someofwhichuseagenerativereward\\nmodel by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment.\\nAdditionally, because the model output is sometimes chaotic and difficult to read, we have\\nfiltered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For\\neachprompt,wesamplemultipleresponsesandretainonlythecorrectones. Intotal,wecollect\\nabout600kreasoningrelatedtrainingsamples.\\n10Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition,\\nandtranslation,weadopttheDeepSeek-V3pipelineandreuseportionsoftheSFTdatasetof\\nDeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential\\nchain-of-thoughtbeforeansweringthequestionbyprompting. However,forsimplerqueries,\\nsuch as â€œhelloâ€ we do not provide a CoT in response. In the end, we collected a total of\\napproximately200ktrainingsamplesthatareunrelatedtoreasoning.\\nWefine-tuneDeepSeek-V3-Basefortwoepochsusingtheabovecurateddatasetofabout\\n800ksamples.\\n2.3.4. ReinforcementLearningforallScenarios\\nTofurtheralignthemodelwithhumanpreferences,weimplementasecondaryreinforcement\\nlearningstageaimedatimprovingthemodelâ€™shelpfulnessandharmlessnesswhilesimultane-\\nouslyrefiningitsreasoningcapabilities. Specifically,wetrainthemodelusingacombination\\nof reward signals and diverse prompt distributions. For reasoning data, we adhere to the\\nmethodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the\\nlearningprocessinmath,code,andlogicalreasoningdomains. Forgeneraldata,weresortto\\nreward models to capture human preferences in complex and nuanced scenarios. We build\\nupontheDeepSeek-V3pipelineandadoptasimilardistributionofpreferencepairsandtrain-\\ning prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the\\nassessmentemphasizestheutilityandrelevanceoftheresponsetotheuserwhileminimizing\\ninterferencewiththeunderlyingreasoningprocess. Forharmlessness,weevaluatetheentire\\nresponseofthemodel,includingboththereasoningprocessandthesummary,toidentifyand\\nmitigate any potential risks, biases, or harmful content that may arise during the generation\\nprocess. Ultimately,theintegrationofrewardsignalsanddiversedatadistributionsenablesus\\ntotrainamodelthatexcelsinreasoningwhileprioritizinghelpfulnessandharmlessness.\\n2.4. Distillation: EmpowerSmallModelswithReasoningCapability\\nToequipmoreefficientsmallermodelswithreasoningcapabilitieslikeDeepSeek-R1,wedirectly\\nfine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using\\nthe800ksamplescuratedwithDeepSeek-R1,asdetailedinÂ§2.3.3. Ourfindingsindicatethat\\nthisstraightforwarddistillationmethodsignificantlyenhancesthereasoningabilitiesofsmaller\\nmodels. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5-\\n14B,Qwen2.5-32B,Llama-3.1-8B,andLlama-3.3-70B-Instruct. WeselectLlama-3.3becauseits\\nreasoningcapabilityisslightlybetterthanthatofLlama-3.1.\\nFor distilled models, we apply only SFT and do not include an RL stage, even though\\nincorporating RL could substantially boost model performance. Our primary goal here is to\\ndemonstrate the effectiveness of the distillation technique, leaving the exploration of the RL\\nstagetothebroaderresearchcommunity.\\n3. Experiment\\nBenchmarks WeevaluatemodelsonMMLU(Hendrycksetal.,2020),MMLU-Redux(Gema\\netal.,2024),MMLU-Pro(Wangetal.,2024),C-Eval(Huangetal.,2023),andCMMLU(Lietal.,\\n2023),IFEval(Zhouetal.,2023),FRAMES(Krishnaetal.,2024),GPQADiamond (Reinetal.,\\n2023),SimpleQA(OpenAI,2024c),C-SimpleQA(Heetal.,2024),SWE-BenchVerified(OpenAI,\\n112024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2, Chinese\\nNationalHighSchoolMathematicsOlympiad(CNMO2024)3,andAmericanInvitationalMath-\\nematicsExamination2024(AIME2024)(MAA,2024). Inadditiontostandardbenchmarks,we\\nalsoevaluateourmodelsonopen-endedgenerationtasksusingLLMsasjudges. Specifically,we\\nadheretotheoriginalconfigurationsofAlpacaEval2.0(Duboisetal.,2024)andArena-Hard(Li\\netal.,2024),whichleverageGPT-4-Turbo-1106asjudgesforpairwisecomparisons. Here,we\\nonly feed the final summary to evaluation to avoid the length bias. For distilled models, we\\nreport representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and\\nLiveCodeBench.\\nEvaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as\\nMMLU,DROP,GPQADiamond,andSimpleQAareevaluatedusingpromptsfromthesimple-\\nevalsframework. ForMMLU-Redux,weadopttheZero-Evalpromptformat(Lin,2024)ina\\nzero-shotsetting. IntermsofMMLU-Pro,C-EvalandCLUE-WSC,sincetheoriginalprompts\\nare few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot\\nmay hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation\\nprotocolswithdefaultpromptsprovidedbytheircreators. Forcodeandmathbenchmarks,the\\nHumanEval-Muldatasetcoverseightmainstreamprogramminglanguages(Python,Java,C++,\\nC#,JavaScript,TypeScript,PHP,andBash). ModelperformanceonLiveCodeBenchisevaluated\\nusingCoTformat,withdatacollectedbetweenAugust2024andJanuary2025. TheCodeforces\\ndatasetisevaluatedusingproblemsfrom10Div.2contestsalongwithexpert-craftedtestcases,\\nafter which the expected ratings and percentages of competitors are calculated. SWE-Bench\\nverified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related\\nbenchmarksaremeasuredusinga\"diff\"format. DeepSeek-R1outputsarecappedatamaximum\\nof32,768tokensforeachbenchmark.\\nBaselines Weconductcomprehensiveevaluationsagainstseveralstrongbaselines,including\\nDeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, andOpenAI-o1-1217.\\nSinceaccessingtheOpenAI-o1-1217APIischallenginginmainlandChina,wereportitsperfor-\\nmancebasedonofficialreports. Fordistilledmodels,wealsocomparetheopen-sourcemodel\\nQwQ-32B-Preview(Qwen,2024a).\\nEvaluation Setup We set the maximum generation length to 32,768 tokens for the models.\\nWe found that using greedy decoding to evaluate long-output reasoning models results in\\nhigherrepetitionratesandsignificantvariabilityacrossdifferentcheckpoints. Therefore,we\\ndefaulttopass@ğ‘˜evaluation(Chenetal.,2021)andreportpass@1usinganon-zerotemperature.\\nSpecifically, we use a sampling temperature of 0.6 and a top-ğ‘ value of 0.95 to generate ğ‘˜\\nresponses(typicallybetween4and64,dependingonthetestsetsize)foreachquestion. Pass@1\\nisthencalculatedas\\nğ‘˜\\n1 âˆ‘ï¸\\npass@1 = ğ‘ ğ‘–,\\nğ‘˜\\nğ‘–=1\\nwhere ğ‘ ğ‘– denotes the correctness of the ğ‘–-th response. This method provides more reliable\\nperformanceestimates. ForAIME2024,wealsoreportconsensus(majorityvote)results(Wang\\netal.,2022)using64samples,denotedascons@64.\\n1https://aider.chat\\n2https://codeforces.com\\n3https://www.cms.org.cn/Home/comp/comp/cid/12.html\\n123.1. DeepSeek-R1Evaluation\\nClaude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek\\nBenchmark(Metric)\\nSonnet-1022 0513 V3 o1-mini o1-1217 R1\\nArchitecture - - MoE - - MoE\\n#ActivatedParams - - 37B - - 37B\\n#TotalParams - - 671B - - 671B\\nMMLU(Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8\\nMMLU-Redux(EM) 88.9 88.0 89.1 86.7 - 92.9\\nMMLU-Pro(EM) 78.0 72.6 75.9 80.3 - 84.0\\nDROP(3-shotF1) 88.3 83.7 91.6 83.9 90.2 92.2\\nIF-Eval(PromptStrict) 86.5 84.3 86.1 84.8 - 83.3\\nEnglish\\nGPQADiamond(Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5\\nSimpleQA(Correct) 28.4 38.2 24.9 7.0 47.0 30.1\\nFRAMES(Acc.) 72.5 80.5 73.3 76.9 - 82.5\\nAlpacaEval2.0(LC-winrate) 52.0 51.1 70.0 57.8 - 87.6\\nArenaHard(GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3\\nLiveCodeBench(Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9\\nCodeforces(Percentile) 20.3 23.6 58.7 93.4 96.6 96.3\\nCode\\nCodeforces(Rating) 717 759 1134 1820 2061 2029\\nSWEVerified(Resolved) 50.8 38.8 42.0 41.6 48.9 49.2\\nAider-Polyglot(Acc.) 45.3 16.0 49.6 32.9 61.7 53.3\\nAIME2024(Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8\\nMath MATH-500(Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3\\nCNMO2024(Pass@1) 13.1 10.8 43.2 67.6 - 78.8\\nCLUEWSC(EM) 85.4 87.9 90.9 89.9 - 92.8\\nChinese C-Eval(EM) 76.7 76.0 86.5 68.9 - 91.8\\nC-SimpleQA(Correct) 55.4 58.7 68.0 40.3 - 63.7\\nTable4 | ComparisonbetweenDeepSeek-R1andotherrepresentativemodels.\\nFor education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA\\nDiamond,DeepSeek-R1demonstratessuperiorperformancecomparedtoDeepSeek-V3. Thisim-\\nprovementisprimarilyattributedtoenhancedaccuracyinSTEM-relatedquestions,wheresignif-\\nicantgainsareachievedthroughlarge-scalereinforcementlearning. Additionally,DeepSeek-R1\\nexcelsonFRAMES,along-context-dependentQAtask,showcasingitsstrongdocumentanalysis\\ncapabilities. This highlights the potential of reasoning models in AI-driven search and data\\nanalysistasks. OnthefactualbenchmarkSimpleQA,DeepSeek-R1outperformsDeepSeek-V3,\\ndemonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobservedwhere\\nOpenAI-o1surpassesGPT-4oonthisbenchmark. However,DeepSeek-R1performsworsethan\\nDeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse\\nansweringcertainqueriesaftersafetyRL.WithoutsafetyRL,DeepSeek-R1couldachievean\\naccuracyofover70%.\\nDeepSeek-R1alsodeliversimpressiveresultsonIF-Eval,abenchmarkdesignedtoassessa\\nmodelâ€™sabilitytofollowformatinstructions. Theseimprovementscanbelinkedtotheinclusion\\nof instruction-following data during the final stages of supervised fine-tuning (SFT) and RL\\ntraining. Furthermore,remarkableperformanceisobservedonAlpacaEval2.0andArenaHard,\\nindicatingDeepSeek-R1â€™sstrengthsinwritingtasksandopen-domainquestionanswering. Its\\nsignificantoutperformanceofDeepSeek-V3underscoresthegeneralizationbenefitsoflarge-scale\\nRL,whichnotonlyboostsreasoningcapabilitiesbutalsoimprovesperformanceacrossdiverse\\ndomains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an\\naverageof689tokensonArenaHardand2,218charactersonAlpacaEval2.0. Thisindicatesthat\\n13DeepSeek-R1avoidsintroducinglengthbiasduringGPT-basedevaluations,furthersolidifying\\nitsrobustnessacrossmultipletasks.\\nOn math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217,\\nsurpassingothermodelsbyalargemargin. Asimilartrendisobservedoncodingalgorithm\\ntasks,suchasLiveCodeBenchandCodeforces,wherereasoning-focusedmodelsdominatethese\\nbenchmarks. Onengineering-orientedcodingtasks,OpenAI-o1-1217outperformsDeepSeek-R1\\nonAiderbutachievescomparableperformanceonSWEVerified. Webelievetheengineering\\nperformance of DeepSeek-R1 will improve in the next version, as the amount of related RL\\ntrainingdatacurrentlyremainsverylimited.\\n3.2. DistilledModelEvaluation\\nGPQA LiveCode\\nAIME2024 MATH-500 CodeForces\\nModel Diamond Bench\\npass@1 cons@64 pass@1 pass@1 pass@1 rating\\nGPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759\\nClaude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717\\nOpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316\\nDeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954\\nDeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189\\nDeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691\\nDeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205\\nDeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633\\nTable5 | ComparisonofDeepSeek-R1distilledmodelsandothercomparablemodelson\\nreasoning-relatedbenchmarks.\\nAsshowninTable5,simplydistillingDeepSeek-R1â€™soutputsenablestheefficientDeepSeek-\\nR1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non-\\nreasoningmodelslikeGPT-4o-0513acrosstheboard. DeepSeek-R1-14BsurpassesQwQ-32B-\\nPreviewonallevaluationmetrics,whileDeepSeek-R1-32BandDeepSeek-R1-70Bsignificantly\\nexceedo1-minionmostbenchmarks. Theseresultsdemonstratethestrongpotentialofdistilla-\\ntion. Additionally,wefoundthatapplyingRLtothesedistilledmodelsyieldssignificantfurther\\ngains. Webelievethiswarrantsfurtherexplorationandthereforepresentonlytheresultsofthe\\nsimpleSFT-distilledmodelshere.\\n4. Discussion\\n4.1. Distillationv.s. ReinforcementLearning\\nInSection3.2,wecanseethatbydistillingDeepSeek-R1,thesmallmodelcanachieveimpressive\\nresults. However,thereisstillonequestionleft: canthemodelachievecomparableperformance\\nthroughthelarge-scaleRLtrainingdiscussedinthepaperwithoutdistillation?\\nToanswerthisquestion,weconductlarge-scaleRLtrainingonQwen-32B-Baseusingmath,\\ncode,andSTEMdata,trainingforover10Ksteps,resultinginDeepSeek-R1-Zero-Qwen-32B.The\\nexperimentalresults,showninTable6,demonstratethatthe32Bbasemodel,afterlarge-scale\\n14AIME2024 MATH-500 GPQADiamond LiveCodeBench\\nModel\\npass@1 cons@64 pass@1 pass@1 pass@1\\nQwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9\\nDeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2\\nDeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2\\nTable6 | ComparisonofdistilledandRLModelsonReasoning-RelatedBenchmarks.\\nRL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1-\\nDistill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than\\nDeepSeek-R1-Zero-Qwen-32Bacrossallbenchmarks.\\nTherefore,wecandrawtwoconclusions: First,distillingmorepowerfulmodelsintosmaller\\nonesyieldsexcellentresults,whereassmallermodelsrelyingonthelarge-scaleRLmentionedin\\nthispaperrequireenormouscomputationalpowerandmaynotevenachievetheperformance\\nofdistillation. Second,whiledistillationstrategiesarebotheconomicalandeffective,advancing\\nbeyondtheboundariesofintelligencemaystillrequiremorepowerfulbasemodelsandlarger-\\nscalereinforcementlearning.\\n4.2. UnsuccessfulAttempts\\nIntheearlystagesofdevelopingDeepSeek-R1,wealsoencounteredfailuresandsetbacksalong\\ntheway. Weshareourfailureexperiencesheretoprovideinsights,butthisdoesnotimplythat\\ntheseapproachesareincapableofdevelopingeffectivereasoningmodels.\\nProcessRewardModel(PRM) PRMisareasonablemethodtoguidethemodeltowardbetter\\napproachesforsolvingreasoningtasks(Lightmanetal.,2023;Uesatoetal.,2022;Wangetal.,\\n2023). However,inpractice,PRMhasthreemainlimitationsthatmayhinderitsultimatesuc-\\ncess. First,itischallengingtoexplicitlydefineafine-grainstepingeneralreasoning. Second,\\ndeterminingwhetherthecurrentintermediatestepiscorrectisachallengingtask. Automated\\nannotationusingmodelsmaynotyieldsatisfactoryresults,whilemanualannotationisnotcon-\\nducivetoscalingup. Third,onceamodel-basedPRMisintroduced,itinevitablyleadstoreward\\nhacking(Gaoetal.,2022),andretrainingtherewardmodelneedsadditionaltrainingresources\\nanditcomplicatesthewholetrainingpipeline. Inconclusion,whilePRMdemonstratesagood\\nabilitytorerankthetop-Nresponsesgeneratedbythemodelorassistinguidedsearch(Snell\\netal.,2024),itsadvantagesarelimitedcomparedtotheadditionalcomputationaloverheadit\\nintroducesduringthelarge-scalereinforcementlearningprocessinourexperiments.\\nMonteCarloTreeSearch(MCTS) InspiredbyAlphaGo(Silveretal.,2017b)andAlphaZero(Sil-\\nver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time\\ncomputescalability. Thisapproachinvolvesbreakinganswersintosmallerpartstoallowthe\\nmodeltoexplorethesolutionspacesystematically. Tofacilitatethis,wepromptthemodelto\\ngeneratemultipletagsthatcorrespondtospecificreasoningstepsnecessaryforthesearch. For\\ntraining,wefirstusecollectedpromptstofindanswersviaMCTSguidedbyapre-trainedvalue\\nmodel. Subsequently,weusetheresultingquestion-answerpairstotrainboththeactormodel\\nandthevaluemodel,iterativelyrefiningtheprocess.\\nHowever,thisapproachencountersseveralchallengeswhenscalingupthetraining. First,\\nunlike chess, wherethe search spaceis relatively well-defined, tokengeneration presents an\\n15exponentiallylargersearchspace. Toaddressthis,wesetamaximumextensionlimitforeach\\nnode, but this can lead to the model getting stuck in local optima. Second, the value model\\ndirectly influences the quality of generation since it guides each step of the search process.\\nTrainingafine-grainedvaluemodelisinherentlydifficult,whichmakesitchallengingforthe\\nmodeltoiterativelyimprove. WhileAlphaGoâ€™scoresuccessreliedontrainingavaluemodelto\\nprogressivelyenhanceitsperformance,thisprincipleprovesdifficulttoreplicateinoursetup\\nduetothecomplexitiesoftokengeneration.\\nInconclusion,whileMCTScanimproveperformanceduringinferencewhenpairedwitha\\npre-trainedvaluemodel,iterativelyboostingmodelperformancethroughself-searchremainsa\\nsignificantchallenge.\\n5. Conclusion, Limitations, and Future Work\\nInthiswork,weshareourjourneyinenhancingmodelreasoningabilitiesthroughreinforcement\\nlearning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start\\ndata, achieving strong performance across various tasks. DeepSeek-R1 is more powerful,\\nleveragingcold-startdataalongsideiterativeRLfine-tuning. Ultimately,DeepSeek-R1achieves\\nperformancecomparabletoOpenAI-o1-1217onarangeoftasks.\\nWe further explore distillation the reasoning capability to small dense models. We use\\nDeepSeek-R1astheteachermodeltogenerate800Ktrainingsamples,andfine-tuneseveralsmall\\ndensemodels. Theresultsarepromising: DeepSeek-R1-Distill-Qwen-1.5BoutperformsGPT-4o\\nandClaude-3.5-Sonnetonmathbenchmarkswith28.9%onAIMEand83.9%onMATH.Other\\ndense models also achieve impressive results, significantly outperforming other instruction-\\ntunedmodelsbasedonthesameunderlyingcheckpoints.\\nInthefuture,weplantoinvestinresearchacrossthefollowingdirectionsforDeepSeek-R1.\\nâ€¢ GeneralCapability: Currently,thecapabilitiesofDeepSeek-R1fallshortofDeepSeek-V3\\nin tasks such as function calling, multi-turn, complex role-playing, and JSON output.\\nMovingforward,weplantoexplorehowlongCoTcanbeleveragedtoenhancetasksin\\nthesefields.\\nâ€¢ LanguageMixing: DeepSeek-R1iscurrentlyoptimizedforChineseandEnglish,which\\nmay result in language mixing issues when handling queries in other languages. For\\ninstance,DeepSeek-R1mightuseEnglishforreasoningandresponses,evenifthequeryis\\ninalanguageotherthanEnglishorChinese. Weaimtoaddressthislimitationinfuture\\nupdates.\\nâ€¢ PromptingEngineering: WhenevaluatingDeepSeek-R1,weobservethatitissensitive\\ntoprompts. Few-shotpromptingconsistentlydegradesitsperformance. Therefore,we\\nrecommend users directly describe the problem and specify the output format using a\\nzero-shotsettingforoptimalresults.\\nâ€¢ Software Engineering Tasks: Due to the long evaluation times, which impact the effi-\\nciency of the RL process, large-scale RL has not been applied extensively in software\\nengineeringtasks. Asaresult,DeepSeek-R1hasnotdemonstratedahugeimprovement\\nover DeepSeek-V3 on software engineering benchmarks. Future versions will address\\nthisbyimplementingrejectionsamplingonsoftwareengineeringdataorincorporating\\nasynchronousevaluationsduringtheRLprocesstoimproveefficiency.\\n16References\\nAI@Meta. Llama3.1modelcard,2024. URLhttps://github.com/meta-llama/llama-m\\nodels/blob/main/models/llama3_1/MODEL_CARD.md.\\nAnthropic. Claude3.5sonnet,2024. URLhttps://www.anthropic.com/news/claude-3\\n-5-sonnet.\\nM.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda,\\nN.Joseph,G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin,\\nB.Chan,S.Gray,N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,P.Tillet,\\nF.P.Such,D.Cummings,M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss,\\nA.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,S.Balaji,S.Jain,W.Saunders,C.Hesse,\\nA.N.Carr,J.Leike,J.Achiam,V.Misra,E.Morikawa,A.Radford,M.Knight,M.Brundage,\\nM.Murati,K.Mayer,P.Welinder,B.McGrew,D.Amodei,S.McCandlish,I.Sutskever,and\\nW.Zaremba. Evaluatinglargelanguagemodelstrainedoncode. CoRR,abs/2107.03374,2021.\\nURLhttps://arxiv.org/abs/2107.03374.\\nA. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten,\\nA.Yang,A.Fan,etal. Thellama3herdofmodels. arXivpreprintarXiv:2407.21783,2024.\\nY.Dubois,B.Galambosi,P.Liang,andT.B.Hashimoto. Length-controlledalpacaeval: Asimple\\nwaytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,2024.\\nX. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like\\ntree-search can guide large language model decoding and training, 2024. URL https:\\n//arxiv.org/abs/2309.17179.\\nL.Gao,J.Schulman,andJ.Hilton. Scalinglawsforrewardmodeloveroptimization,2022. URL\\nhttps://arxiv.org/abs/2210.10760.\\nA.P.Gema, J.O.J.Leang, G.Hong, A.Devoto, A.C.M.Mancino, R.Saxena, X.He, Y.Zhao,\\nX. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and\\nP.Minervini. Arewedonewithmmlu? CoRR,abs/2406.04127,2024. URLhttps://doi.or\\ng/10.48550/arXiv.2406.04127.\\nGoogle. Ournext-generationmodel: Gemini1.5,2024. URLhttps://blog.google/techno\\nlogy/ai/google-gemini-next-generation-model-february-2024.\\nY. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi-\\nnese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint\\narXiv:2411.07140,2024.\\nD.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring\\nmassivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020.\\nY.Huang,Y.Bai,Z.Zhu,J.Zhang,J.Zhang,T.Su,J.Liu,C.Lv,Y.Zhang,J.Lei,etal. C-Eval: A\\nmulti-levelmulti-disciplinechineseevaluationsuiteforfoundationmodels. arXivpreprint\\narXiv:2305.08322,2023.\\nN.Jain,K.Han,A.Gu,W.Li,F.Yan,T.Zhang,S.Wang,A.Solar-Lezama,K.Sen,andI.Stoica.\\nLivecodebench: Holisticandcontaminationfreeevaluationoflargelanguagemodelsforcode.\\nCoRR,abs/2403.07974,2024. URLhttps://doi.org/10.48550/arXiv.2403.07974.\\n17S.Krishna,K.Krishna,A.Mohananey,S.Schwarcz,A.Stambler,S.Upadhyay,andM.Faruqui.\\nFact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR,\\nabs/2409.12941,2024. doi: 10.48550/ARXIV.2409.12941. URLhttps://doi.org/10.485\\n50/arXiv.2409.12941.\\nA.Kumar,V.Zhuang,R.Agarwal,Y.Su,J.D.Co-Reyes,A.Singh,K.Baumli,S.Iqbal,C.Bishop,\\nR.Roelofs,etal. Traininglanguagemodelstoself-correctviareinforcementlearning. arXiv\\npreprintarXiv:2409.12917,2024.\\nH.Li,Y.Zhang,F.Koto,Y.Yang,H.Zhao,Y.Gong,N.Duan,andT.Baldwin. CMMLU:Measur-\\ningmassivemultitasklanguageunderstandinginChinese. arXivpreprintarXiv:2306.09212,\\n2023.\\nT. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From\\ncrowdsourceddatatohigh-qualitybenchmarks: Arena-hardandbenchbuilderpipeline. arXiv\\npreprintarXiv:2406.11939,2024.\\nH. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman,\\nI.Sutskever,andK.Cobbe. Letâ€™sverifystepbystep. arXivpreprintarXiv:2305.20050,2023.\\nB.Y.Lin. ZeroEval: AUnifiedFrameworkforEvaluatingLanguageModels,July2024. URL\\nhttps://github.com/WildEval/ZeroEval.\\nMAA. American invitational mathematics examination - aime. In American Invitational\\nMathematics Examination - AIME 2024, February 2024. URL https://maa.org/math\\n-competitions/american-invitational-mathematics-examination-aime.\\nOpenAI. HelloGPT-4o,2024a. URLhttps://openai.com/index/hello-gpt-4o/.\\nOpenAI. Learningtoreasonwithllms,2024b. URLhttps://openai.com/index/learnin\\ng-to-reason-with-llms/.\\nOpenAI. IntroducingSimpleQA,2024c. URLhttps://openai.com/index/introducing\\n-simpleqa/.\\nOpenAI. Introducing SWE-bench verified weâ€™re releasing a human-validated subset of swe-\\nbenchthatmore, 2024d. URLhttps://openai.com/index/introducing-swe-bench\\n-verified/.\\nQwen. Qwq: Reflectdeeplyontheboundariesoftheunknown,2024a. URLhttps://qwenlm\\n.github.io/blog/qwq-32b-preview/.\\nQwen. Qwen2.5: Apartyoffoundationmodels,2024b. URLhttps://qwenlm.github.io/b\\nlog/qwen2.5.\\nD.Rein,B.L.Hou,A.C.Stickland,J.Petty,R.Y.Pang,J.Dirani,J.Michael,andS.R.Bowman.\\nGPQA:Agraduate-levelgoogle-proofq&abenchmark. arXivpreprintarXiv:2311.12022,2023.\\nZ.Shao,P.Wang,Q.Zhu,R.Xu,J.Song,M.Zhang,Y.Li,Y.Wu,andD.Guo. Deepseekmath:\\nPushing the limits of mathematical reasoning in open language models. arXiv preprint\\narXiv:2402.03300,2024.\\nD. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre,\\nD.Kumaran,T.Graepel,T.P.Lillicrap,K.Simonyan,andD.Hassabis. Masteringchessand\\nshogibyself-playwithageneralreinforcementlearningalgorithm. CoRR,abs/1712.01815,\\n2017a. URLhttp://arxiv.org/abs/1712.01815.\\n18D.Silver,J.Schrittwieser,K.Simonyan,I.Antonoglou,A.Huang,A.Guez,T.Hubert,L.Baker,\\nM.Lai,A.Bolton,Y.Chen,T.P.Lillicrap,F.Hui,L.Sifre,G.vandenDriessche,T.Graepel,and\\nD.Hassabis. Masteringthegameofgowithouthumanknowledge. Nat.,550(7676):354â€“359,\\n2017b. doi: 10.1038/NATURE24270. URLhttps://doi.org/10.1038/nature24270.\\nC. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more\\neffectivethanscalingmodelparameters,2024. URLhttps://arxiv.org/abs/2408.033\\n14.\\nT. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human\\ndemonstrations. Nature,2024. doi: 10.1038/s41586-023-06747-5.\\nJ. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and\\nI.Higgins. Solvingmathwordproblemswithprocess-andoutcome-basedfeedback. arXiv\\npreprintarXiv:2211.14275,2022.\\nP.Wang,L.Li,Z.Shao,R.Xu,D.Dai,Y.Li,D.Chen,Y.Wu,andZ.Sui. Math-shepherd: Alabel-\\nfreestep-by-stepverifierforllmsinmathematicalreasoning. arXivpreprintarXiv:2312.08935,\\n2023.\\nX. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou.\\nSelf-consistency improves chain of thought reasoning in language models. arXiv preprint\\narXiv:2203.11171,2022.\\nY.Wang,X.Ma,G.Zhang,Y.Ni,A.Chandra,S.Guo,W.Ren,A.Arulraj,X.He,Z.Jiang,T.Li,\\nM. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and\\nchallenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024.\\nURLhttps://doi.org/10.48550/arXiv.2406.01574.\\nC. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software\\nengineeringagents. arXivpreprint,2024.\\nH.Xin,Z.Z.Ren,J.Song,Z.Shao,W.Zhao,H.Wang,B.Liu,L.Zhang,X.Lu,Q.Du,W.Gao,\\nQ.Zhu,D.Yang,Z.Gou,Z.F.Wu,F.Luo,andC.Ruan. Deepseek-prover-v1.5: Harnessing\\nproofassistantfeedbackforreinforcementlearningandmonte-carlotreesearch,2024. URL\\nhttps://arxiv.org/abs/2408.08152.\\nJ.Zhou,T.Lu,S.Mishra,S.Brahma,S.Basu,Y.Luan,D.Zhou,andL.Hou. Instruction-following\\nevaluationforlargelanguagemodels. arXivpreprintarXiv:2311.07911,2023.\\n19Appendix\\nA. Contributions and Acknowledgments\\nCoreContributors HuiLi\\nDayaGuo JianzhongGuo\\nDejianYang JiashiLi\\nHaoweiZhang JingchangChen\\nJunxiaoSong JingyangYuan\\nRuoyuZhang JinhaoTu\\nRunxinXu JunjieQiu\\nQihaoZhu JunlongLi\\nShirongMa J.L.Cai\\nPeiyiWang JiaqiNi\\nXiaoBi JianLiang\\nXiaokangZhang JinChen\\nXingkaiYu KaiDong\\nYuWu KaiHu*\\nZ.F.Wu KaichaoYou\\nZhibinGou KaigeGao\\nZhihongShao KangGuan\\nZhuoshuLi KexinHuang\\nZiyiGao KuaiYu\\nLeanWang\\nLecongZhang\\nContributors\\nLiangZhao\\nAixinLiu\\nLitongWang\\nBingXue\\nLiyueZhang\\nBingxuanWang\\nLeiXu\\nBochaoWu\\nLeyiXia\\nBeiFeng\\nMingchuanZhang\\nChengdaLu\\nMinghuaZhang\\nChenggangZhao\\nMinghuiTang\\nChengqiDeng\\nMingxuZhou\\nChongRuan\\nMengLi\\nDamaiDai\\nMiaojunWang\\nDeliChen\\nMingmingLi\\nDongjieJi\\nNingTian\\nErhangLi\\nPanpanHuang\\nFangyunLin\\nPengZhang\\nFucongDai\\nQianchengWang\\nFuliLuo*\\nQinyuChen\\nGuangboHao\\nQiushiDu\\nGuantingChen\\nRuiqiGe*\\nGuoweiLi\\nRuisongZhang\\nH.Zhang\\nRuizhePan\\nHanweiXu\\nRunjiWang\\nHonghuiDing\\nR.J.Chen\\nHuazuoGao\\nR.L.Jin\\nHuiQu\\n20RuyiChen Y.X.Wei\\nShanghaoLu YangZhang\\nShangyanZhou YanhongXu\\nShanhuangChen YaoLi\\nShengfengYe YaoZhao\\nShiyuWang YaofengSun\\nShuipingYu YaohuiWang\\nShunfengZhou YiYu\\nShutingPan YichaoZhang\\nS.S.Li YifanShi\\nShuangZhou YiliangXiong\\nShaoqingWu YingHe\\nShengfengYe YishiPiao\\nTaoYun YisongWang\\nTianPei YixuanTan\\nTianyuSun YiyangMa*\\nT.Wang YiyuanLiu\\nWangdingZeng YongqiangGuo\\nWenLiu YuanOu\\nWenfengLiang YuduanWang\\nWenjunGao YueGong\\nWenqinYu* YuhengZou\\nWentaoZhang YujiaHe\\nW.L.Xiao YunfanXiong\\nWeiAn YuxiangLuo\\nXiaodongLiu YuxiangYou\\nXiaohanWang YuxuanLiu\\nXiaokangChen YuyangZhou\\nXiaotaoNie Y.X.Zhu\\nXinCheng YanpingHuang\\nXinLiu YaohuiLi\\nXinXie YiZheng\\nXingchaoLiu YuchenZhu\\nXinyuYang YunxianMa\\nXinyuanLi YingTang\\nXuechengSu YukunZha\\nXuhengLin YutingYan\\nX.Q.Li Z.Z.Ren\\nXiangyueJin ZehuiRen\\nXiaojinShen ZhangliSha\\nXiaoshaChen ZheFu\\nXiaowenSun ZheanXu\\nXiaoxiangWang ZhendaXie\\nXinnanSong ZhengyanZhang\\nXinyiZhou ZhewenHao\\nXianzuWang ZhichengMa\\nXinxiaShan ZhigangYan\\nY.K.Li ZhiyuWu\\nY.Q.Wang ZihuiGu\\n21ZijiaZhu ZhenHuang\\nZijunLiu* ZhipengXu\\nZilinLi ZhongyuZhang\\nZiweiXie ZhenZhang\\nZiyangSong\\nZizhengPan\\nWithineachrole,authorsarelistedalphabeticallybythefirstname. Namesmarkedwith*\\ndenoteindividualswhohavedepartedfromourteam.\\n22'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = text.replace('\\n', ' ')\n",
        "text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 199
        },
        "id": "cjhkdT_olozK",
        "outputId": "be7e0f90-6e46-4ab3-de2d-d127fdb36547"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'DeepSeek-R1: Incentivizing Reasoning Capability in LLMs via Reinforcement Learning DeepSeek-AI research@deepseek.com Abstract We introduce our first-generation reasoning models, DeepSeek-R1-Zero and DeepSeek-R1. DeepSeek-R1-Zero,amodeltrainedvialarge-scalereinforcementlearning(RL)withoutsuper- visedfine-tuning(SFT)asapreliminarystep,demonstratesremarkablereasoningcapabilities. Through RL, DeepSeek-R1-Zero naturally emerges with numerous powerful and intriguing reasoningbehaviors. However,itencounterschallengessuchaspoorreadability,andlanguage mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1,whichincorporatesmulti-stagetrainingandcold-startdatabeforeRL.DeepSeek- R1achievesperformancecomparabletoOpenAI-o1-1217onreasoningtasks. Tosupportthe researchcommunity,weopen-sourceDeepSeek-R1-Zero,DeepSeek-R1,andsixdensemodels (1.5B,7B,8B,14B,32B,70B)distilledfromDeepSeek-R1basedonQwenandLlama. 100 80 60 40 20 0 AIME 2024 Codeforces GPQA Diamond MATH-500 MMLU SWE-bench Verified (Pass@1) (Percentile) (Pass@1) (Pass@1) (Pass@1) (Resolved) )%( elitnecreP / ycaruccA DeepSeek-R1 OpenAI-o1-1217 DeepSeek-R1-32B OpenAI-o1-mini DeepSeek-V3 96.396.6 97.396.4 93.4 94.3 90.6 90.090.2 90.891.8 87.4 88.5 85.2 79.879.2 75.7 72.6 71.5 63.6 62.1 58.7 60.059.1 49.248.9 41.642.0 39.2 36.8 Figure1 | BenchmarkperformanceofDeepSeek-R1. 5202 naJ 22 ]LC.sc[ 1v84921.1052:viXraContents 1 Introduction 3 1.1 Contributions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 1.2 SummaryofEvaluationResults . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 4 2 Approach 5 2.1 Overview . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 5 2.2 DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel . . . . . . . . . . 5 2.2.1 ReinforcementLearningAlgorithm . . . . . . . . . . . . . . . . . . . . . . 5 2.2.2 RewardModeling . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.3 TrainingTemplate . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 6 2.2.4 Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero 6 2.3 DeepSeek-R1: ReinforcementLearningwithColdStart . . . . . . . . . . . . . . . 9 2.3.1 ColdStart . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 9 2.3.2 Reasoning-orientedReinforcementLearning . . . . . . . . . . . . . . . . . 10 2.3.3 RejectionSamplingandSupervisedFine-Tuning . . . . . . . . . . . . . . . 10 2.3.4 ReinforcementLearningforallScenarios . . . . . . . . . . . . . . . . . . . 11 2.4 Distillation: EmpowerSmallModelswithReasoningCapability . . . . . . . . . . 11 3 Experiment 11 3.1 DeepSeek-R1Evaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 13 3.2 DistilledModelEvaluation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 14 4 Discussion 14 4.1 Distillationv.s. ReinforcementLearning . . . . . . . . . . . . . . . . . . . . . . . . 14 4.2 UnsuccessfulAttempts . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 15 5 Conclusion,Limitations,andFutureWork 16 A ContributionsandAcknowledgments 20 21. Introduction In recent years, Large Language Models (LLMs) have been undergoing rapid iteration and evolution(Anthropic,2024;Google,2024;OpenAI,2024a),progressivelydiminishingthegap towardsArtificialGeneralIntelligence(AGI). Recently,post-traininghasemergedasanimportantcomponentofthefulltrainingpipeline. Ithasbeenshowntoenhanceaccuracyonreasoningtasks,alignwithsocialvalues,andadapt to user preferences, all while requiring relatively minimal computational resources against pre-training. Inthecontextofreasoningcapabilities,OpenAIâ€™so1(OpenAI,2024b)seriesmodels were the first to introduce inference-time scaling by increasing the length of the Chain-of- Thoughtreasoningprocess. Thisapproachhasachievedsignificantimprovementsinvarious reasoningtasks,suchasmathematics,coding,andscientificreasoning. However,thechallenge ofeffectivetest-timescalingremainsanopenquestionfortheresearchcommunity. Severalprior workshaveexploredvariousapproaches,includingprocess-basedrewardmodels(Lightman etal.,2023;Uesatoetal.,2022;Wangetal.,2023),reinforcementlearning(Kumaretal.,2024), andsearchalgorithmssuchasMonteCarloTreeSearchandBeamSearch(Fengetal.,2024;Trinh etal.,2024;Xinetal.,2024). However,noneofthesemethodshasachievedgeneralreasoning performancecomparabletoOpenAIâ€™so1seriesmodels. Inthispaper,wetakethefirststeptowardimprovinglanguagemodelreasoningcapabilities usingpurereinforcementlearning(RL).OurgoalistoexplorethepotentialofLLMstodevelop reasoningcapabilitieswithoutanysuperviseddata,focusingontheirself-evolutionthrough a pure RL process. Specifically, we use DeepSeek-V3-Base as the base model and employ GRPO (Shao et al., 2024) as the RL framework to improve model performance in reasoning. Duringtraining,DeepSeek-R1-Zeronaturallyemergedwithnumerouspowerfulandinteresting reasoningbehaviors. AfterthousandsofRLsteps,DeepSeek-R1-Zeroexhibitssuperperformance onreasoningbenchmarks. Forinstance,thepass@1scoreonAIME2024increasesfrom15.6%to 71.0%,andwithmajorityvoting,thescorefurtherimprovesto86.7%,matchingtheperformance ofOpenAI-o1-0912. However,DeepSeek-R1-Zeroencounterschallengessuchaspoorreadability,andlanguage mixing. To address these issues and further enhance reasoning performance, we introduce DeepSeek-R1,whichincorporatesasmallamountofcold-startdataandamulti-stagetraining pipeline. Specifically, we begin by collecting thousands of cold-start data to fine-tune the DeepSeek-V3-Basemodel. Followingthis,weperformreasoning-orientedRLlikeDeepSeek-R1- Zero. UponnearingconvergenceintheRLprocess,wecreatenewSFTdatathroughrejection samplingontheRLcheckpoint,combinedwithsuperviseddatafromDeepSeek-V3indomains suchaswriting,factualQA,andself-cognition,andthenretraintheDeepSeek-V3-Basemodel. Afterfine-tuningwiththenewdata,thecheckpointundergoesanadditionalRLprocess,taking intoaccountpromptsfromallscenarios. Afterthesesteps,weobtainedacheckpointreferredto asDeepSeek-R1,whichachievesperformanceonparwithOpenAI-o1-1217. WefurtherexploredistillationfromDeepSeek-R1tosmallerdensemodels. UsingQwen2.5- 32B(Qwen,2024b)asthebasemodel,directdistillationfromDeepSeek-R1outperformsapplying RLonit. Thisdemonstratesthatthereasoningpatternsdiscoveredbylargerbasemodelsarecru- cialforimprovingreasoningcapabilities. Weopen-sourcethedistilledQwenandLlama(Dubey etal.,2024)series. Notably,ourdistilled14Bmodeloutperformsstate-of-the-artopen-source QwQ-32B-Preview(Qwen,2024a)byalargemargin,andthedistilled32Band70Bmodelsseta newrecordonthereasoningbenchmarksamongdensemodels. 31.1. Contributions Post-Training: Large-ScaleReinforcementLearningontheBaseModel â€¢ WedirectlyapplyRLtothebasemodelwithoutrelyingonsupervisedfine-tuning(SFT)as apreliminarystep. Thisapproachallowsthemodeltoexplorechain-of-thought(CoT)for solvingcomplexproblems,resultinginthedevelopmentofDeepSeek-R1-Zero. DeepSeek- R1-Zero demonstrates capabilities such as self-verification, reflection, and generating longCoTs,markingasignificantmilestonefortheresearchcommunity. Notably,itisthe first open research to validate that reasoning capabilities of LLMs can be incentivized purelythroughRL,withouttheneedforSFT.Thisbreakthroughpavesthewayforfuture advancementsinthisarea. â€¢ We introduce our pipeline to develop DeepSeek-R1. The pipeline incorporates two RL stagesaimedatdiscoveringimprovedreasoningpatternsandaligningwithhumanpref- erences, as well as two SFT stages that serve as the seed for the modelâ€™s reasoning and non-reasoningcapabilities. Webelievethepipelinewillbenefittheindustrybycreating bettermodels. Distillation: SmallerModelsCanBePowerfulToo â€¢ Wedemonstratethatthereasoningpatternsoflargermodelscanbedistilledintosmaller models,resultinginbetterperformancecomparedtothereasoningpatternsdiscovered throughRLonsmallmodels. TheopensourceDeepSeek-R1,aswellasitsAPI,willbenefit theresearchcommunitytodistillbettersmallermodelsinthefuture. â€¢ UsingthereasoningdatageneratedbyDeepSeek-R1,wefine-tunedseveraldensemodels thatarewidelyusedintheresearchcommunity. Theevaluationresultsdemonstratethat thedistilledsmallerdensemodelsperformexceptionallywellonbenchmarks. DeepSeek- R1-Distill-Qwen-7Bachieves55.5%onAIME2024,surpassingQwQ-32B-Preview. Addi- tionally,DeepSeek-R1-Distill-Qwen-32Bscores72.6%onAIME2024,94.3%onMATH-500, and 57.2% on LiveCodeBench. These results significantly outperform previous open- sourcemodelsandarecomparabletoo1-mini. Weopen-sourcedistilled1.5B,7B,8B,14B, 32B,and70BcheckpointsbasedonQwen2.5andLlama3seriestothecommunity. 1.2. SummaryofEvaluationResults â€¢ Reasoningtasks: (1)DeepSeek-R1achievesascoreof79.8%Pass@1onAIME2024,slightly surpassing OpenAI-o1-1217. On MATH-500, it attains an impressive score of 97.3%, performingonparwithOpenAI-o1-1217andsignificantlyoutperformingothermodels. (2) Oncoding-relatedtasks,DeepSeek-R1demonstratesexpertlevelincodecompetitiontasks, asitachieves2,029EloratingonCodeforcesoutperforming96.3%humanparticipantsin thecompetition. Forengineering-relatedtasks,DeepSeek-R1performsslightlybetterthan DeepSeek-V3,whichcouldhelpdevelopersinrealworldtasks. â€¢ Knowledge: OnbenchmarkssuchasMMLU,MMLU-Pro,andGPQADiamond,DeepSeek- R1 achieves outstanding results, significantly outperforming DeepSeek-V3 with scores of 90.8% on MMLU, 84.0% on MMLU-Pro, and 71.5% on GPQA Diamond. While its performanceisslightlybelowthatofOpenAI-o1-1217onthesebenchmarks,DeepSeek-R1 surpassesotherclosed-sourcemodels,demonstratingitscompetitiveedgeineducational tasks. On the factual benchmark SimpleQA, DeepSeek-R1 outperforms DeepSeek-V3, demonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobserved whereOpenAI-o1surpasses4oonthisbenchmark. 4â€¢ Others: DeepSeek-R1 also excels in a wide range of tasks, including creative writing, generalquestionanswering,editing,summarization,andmore. Itachievesanimpressive length-controlled win-rate of 87.6% on AlpacaEval 2.0 and a win-rate of 92.3% on Are- naHard,showcasingitsstrongabilitytointelligentlyhandlenon-exam-orientedqueries. Additionally, DeepSeek-R1 demonstrates outstanding performance on tasks requiring long-contextunderstanding,substantiallyoutperformingDeepSeek-V3onlong-context benchmarks. 2. Approach 2.1. Overview Previous work has heavily relied on large amounts of supervised data to enhance model performance. In this study, we demonstrate that reasoning capabilities can be significantly improved through large-scale reinforcement learning (RL), even without using supervised fine-tuning (SFT) as a cold start. Furthermore, performance can be further enhanced with the inclusion of a small amount of cold-start data. In the following sections, we present: (1) DeepSeek-R1-Zero, which applies RL directly to the base model without any SFT data, and (2) DeepSeek-R1, which applies RL starting from a checkpoint fine-tuned with thousands of longChain-of-Thought(CoT)examples. 3)DistillthereasoningcapabilityfromDeepSeek-R1to smalldensemodels. 2.2. DeepSeek-R1-Zero: ReinforcementLearningontheBaseModel Reinforcement learning has demonstrated significant effectiveness in reasoning tasks, as ev- idenced by our previous works (Shao et al., 2024; Wang et al., 2023). However, these works heavilydependedonsuperviseddata,whicharetime-intensivetogather. Inthissection,we explorethepotentialofLLMstodevelopreasoningcapabilitieswithoutanysuperviseddata, focusingontheirself-evolutionthroughapurereinforcementlearningprocess. Westartwitha briefoverviewofourRLalgorithm,followedbythepresentationofsomeexcitingresults,and hopethisprovidesthecommunitywithvaluableinsights. 2.2.1. ReinforcementLearningAlgorithm GroupRelativePolicyOptimization InordertosavethetrainingcostsofRL,weadoptGroup RelativePolicyOptimization(GRPO)(Shaoetal.,2024),whichforegoesthecriticmodelthatis typicallythesamesizeasthepolicymodel,andestimatesthebaselinefromgroupscoresinstead. Specifically,foreachquestionğ‘,GRPOsamplesagroupofoutputs {ğ‘œ 1,ğ‘œ 2,Â·Â·Â· ,ğ‘œ ğº} fromtheold policyğœ‹ ğœƒ andthenoptimizesthepolicymodelğœ‹ ğœƒ bymaximizingthefollowingobjective: ğ‘œğ‘™ğ‘‘ J ğºğ‘…ğ‘ƒğ‘‚(ğœƒ) = E[ğ‘ âˆ¼ ğ‘ƒ(ğ‘„),{ğ‘œ ğ‘–}ğº ğ‘–=1 âˆ¼ ğœ‹ ğœƒ ğ‘œğ‘™ğ‘‘(ğ‘‚|ğ‘)] 1 âˆ‘ï¸ğº (cid:18) min(cid:18) ğœ‹ ğœƒ(ğ‘œ ğ‘–|ğ‘) ğ´ ğ‘–,clip(cid:18) ğœ‹ ğœƒ(ğ‘œ ğ‘–|ğ‘) ,1âˆ’ğœ€,1+ğœ€(cid:19) ğ´ ğ‘–(cid:19) âˆ’ğ›½D ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ||ğœ‹ ğ‘Ÿğ‘’ğ‘“(cid:1)(cid:19) , (1) ğº ğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘) ğœƒ ğ‘– ğœƒ ğ‘– ğ‘–=1 ğ‘œğ‘™ğ‘‘ ğ‘œğ‘™ğ‘‘ ğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘) D ğ¾ğ¿ (cid:0)ğœ‹ ğœƒ||ğœ‹ ğ‘Ÿğ‘’ğ‘“(cid:1) = ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’log ğ‘Ÿğ‘’ğ‘“ ğ‘– âˆ’1, (2) ğœ‹ (ğ‘œ |ğ‘) ğœ‹ (ğ‘œ |ğ‘) ğœƒ ğ‘– ğœƒ ğ‘– where ğœ€ and ğ›½ are hyper-parameters, and ğ´ ğ‘– is the advantage, computed using a group of rewards{ğ‘Ÿ 1,ğ‘Ÿ 2,...,ğ‘Ÿ ğº}correspondingtotheoutputswithineachgroup: ğ´ ğ‘– = ğ‘Ÿ ğ‘–âˆ’mğ‘’ğ‘ğ‘›({ğ‘Ÿ 1,ğ‘Ÿ 2,Â·Â·Â· ,ğ‘Ÿ ğº}) . (3) sğ‘¡ğ‘‘({ğ‘Ÿ 1,ğ‘Ÿ 2,Â·Â·Â· ,ğ‘Ÿ ğº}) 5AconversationbetweenUserandAssistant. Theuserasksaquestion,andtheAssistantsolvesit. Theassistantfirstthinksaboutthereasoningprocessinthemindandthenprovidestheuser withtheanswer. Thereasoningprocessandanswerareenclosedwithin<think></think>and <answer></answer>tags,respectively,i.e.,<think>reasoningprocesshere</think> <answer>answerhere</answer>. User: prompt. Assistant: Table1 | TemplateforDeepSeek-R1-Zero. promptwillbereplacedwiththespecificreasoning questionduringtraining. 2.2.2. RewardModeling Therewardisthesourceofthetrainingsignal,whichdecidestheoptimizationdirectionofRL. TotrainDeepSeek-R1-Zero,weadoptarule-basedrewardsystemthatmainlyconsistsoftwo typesofrewards: â€¢ Accuracyrewards: Theaccuracyrewardmodelevaluateswhethertheresponseiscorrect. Forexample,inthecaseofmathproblemswithdeterministicresults,themodelisrequired to provide the final answer in a specified format (e.g., within a box), enabling reliable rule-basedverificationofcorrectness. Similarly,forLeetCodeproblems,acompilercanbe usedtogeneratefeedbackbasedonpredefinedtestcases. â€¢ Formatrewards: Inadditiontotheaccuracyrewardmodel,weemployaformatreward modelthatenforcesthemodeltoputitsthinkingprocessbetweenâ€˜<think>â€™andâ€˜</think>â€™ tags. WedonotapplytheoutcomeorprocessneuralrewardmodelindevelopingDeepSeek-R1-Zero, becausewefindthattheneuralrewardmodelmaysufferfromrewardhackinginthelarge-scale reinforcement learning process, and retraining the reward model needs additional training resourcesanditcomplicatesthewholetrainingpipeline. 2.2.3. TrainingTemplate To train DeepSeek-R1-Zero, we begin by designing a straightforward template that guides the base model to adhere to our specified instructions. As depicted in Table 1, this template requiresDeepSeek-R1-Zerotofirstproduceareasoningprocess,followedbythefinalanswer. Weintentionallylimitourconstraintstothisstructuralformat,avoidinganycontent-specific biasesâ€”suchasmandatingreflectivereasoningorpromotingparticularproblem-solvingstrate- giesâ€”toensurethatwecanaccuratelyobservethemodelâ€™snaturalprogressionduringtheRL process. 2.2.4. Performance,Self-evolutionProcessandAhaMomentofDeepSeek-R1-Zero PerformanceofDeepSeek-R1-Zero Figure2depictstheperformancetrajectoryofDeepSeek- R1-Zero on the AIME 2024 benchmark throughout the RL training process. As illustrated, DeepSeek-R1-Zerodemonstratesasteadyandconsistentenhancementinperformanceasthe RL training advances. Notably, the average pass@1 score on AIME 2024 shows a significant increase, jumpingfromaninitial15.6%toanimpressive71.0%, reachingperformancelevels comparabletoOpenAI-o1-0912. ThissignificantimprovementhighlightstheefficacyofourRL algorithminoptimizingthemodelâ€™sperformanceovertime. Table2providesacomparativeanalysisbetweenDeepSeek-R1-ZeroandOpenAIâ€™so1-0912 modelsacrossavarietyofreasoning-relatedbenchmarks. ThefindingsrevealthatRLempowers 6GPQA LiveCode AIME2024 MATH-500 CodeForces Model Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 OpenAI-o1-0912 74.4 83.3 94.8 77.3 63.4 1843 DeepSeek-R1-Zero 71.0 86.7 95.9 73.3 50.0 1444 Table2 | ComparisonofDeepSeek-R1-ZeroandOpenAIo1modelsonreasoning-related benchmarks. Figure2 | AIMEaccuracyofDeepSeek-R1-Zeroduringtraining. Foreachquestion,wesample 16responsesandcalculatetheoverallaverageaccuracytoensureastableevaluation. DeepSeek-R1-Zerotoattainrobustreasoningcapabilitieswithouttheneedforanysupervised fine-tuning data. This is a noteworthy achievement, as it underscores the modelâ€™s ability to learnandgeneralizeeffectivelythroughRLalone. Additionally,theperformanceofDeepSeek- R1-Zero can be further augmented through the application of majority voting. For example, whenmajorityvotingisemployedontheAIMEbenchmark,DeepSeek-R1-Zeroâ€™sperformance escalates from 71.0% to 86.7%, thereby exceeding the performance of OpenAI-o1-0912. The abilityofDeepSeek-R1-Zerotoachievesuchcompetitiveperformance,bothwithandwithout majority voting, highlights its strong foundational capabilities and its potential for further advancementsinreasoningtasks. Self-evolutionProcessofDeepSeek-R1-Zero Theself-evolutionprocessofDeepSeek-R1-Zero isafascinatingdemonstrationofhowRLcandriveamodeltoimproveitsreasoningcapabilities autonomously. ByinitiatingRLdirectlyfromthebasemodel,wecancloselymonitorthemodelâ€™s progressionwithouttheinfluenceofthesupervisedfine-tuningstage. Thisapproachprovides aclearviewofhowthemodelevolvesovertime,particularlyintermsofitsabilitytohandle complexreasoningtasks. AsdepictedinFigure3,thethinkingtimeofDeepSeek-R1-Zeroshowsconsistentimprove- 7Figure3 | TheaverageresponselengthofDeepSeek-R1-ZeroonthetrainingsetduringtheRL process. DeepSeek-R1-Zeronaturallylearnstosolvereasoningtaskswithmorethinkingtime. mentthroughoutthetrainingprocess. Thisimprovementisnottheresultofexternaladjustments butratheranintrinsicdevelopmentwithinthemodel. DeepSeek-R1-Zeronaturallyacquiresthe abilitytosolveincreasinglycomplexreasoningtasksbyleveragingextendedtest-timecompu- tation. Thiscomputationrangesfromgeneratinghundredstothousandsofreasoningtokens, allowingthemodeltoexploreandrefineitsthoughtprocessesingreaterdepth. Oneofthemostremarkableaspectsofthisself-evolutionistheemergenceofsophisticated behaviorsasthetest-timecomputationincreases. Behaviorssuchasreflectionâ€”wherethemodel revisits and reevaluates its previous stepsâ€”and the exploration of alternative approaches to problem-solvingarisespontaneously. Thesebehaviorsarenotexplicitlyprogrammedbutinstead emergeasaresultofthemodelâ€™sinteractionwiththereinforcementlearningenvironment. This spontaneousdevelopmentsignificantlyenhancesDeepSeek-R1-Zeroâ€™sreasoningcapabilities, enablingittotacklemorechallengingtaskswithgreaterefficiencyandaccuracy. AhaMomentofDeepSeek-R1-Zero Aparticularlyintriguingphenomenonobservedduring the training of DeepSeek-R1-Zero is the occurrence of an â€œaha momentâ€. This moment, as illustrated in Table 3, occurs in an intermediate version of the model. During this phase, DeepSeek-R1-Zerolearnstoallocatemorethinkingtimetoaproblembyreevaluatingitsinitial approach. This behavior is not only a testament to the modelâ€™s growing reasoning abilities but also a captivating example of how reinforcement learning can lead to unexpected and sophisticatedoutcomes. This moment is not only an â€œaha momentâ€ for the model but also for the researchers observingitsbehavior. Itunderscoresthepowerandbeautyofreinforcementlearning: rather thanexplicitlyteachingthemodelonhowtosolveaproblem,wesimplyprovideitwiththe right incentives, and it autonomously develops advanced problem-solving strategies. The â€œaha momentâ€ serves as a powerful reminder of the potential of RL to unlock new levels of intelligenceinartificialsystems,pavingthewayformoreautonomousandadaptivemodelsin thefuture. 8âˆšï¸ âˆš Question: Ifğ‘ > 1,thenthesumoftherealsolutionsof ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥ isequalto Response: <think> âˆšï¸ âˆš Tosolvetheequation ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥,letâ€™sstartbysquaringbothÂ·Â·Â· (cid:16)âˆšï¸ âˆš (cid:17)2 âˆš ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2. Rearrangetoisolatetheinnersquarerootterm: (ğ‘âˆ’ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘2âˆ’2ğ‘ğ‘¥2+(ğ‘¥2)2 = ğ‘+ğ‘¥ =â‡’ ğ‘¥4âˆ’2ğ‘ğ‘¥2âˆ’ğ‘¥+(ğ‘2âˆ’ğ‘) = 0 ... Wait,wait. Wait. Thatâ€™sanahamomentIcanflaghere. Letâ€™sreevaluatethisstep-by-steptoidentifyifthecorrectsumcanbeÂ·Â·Â· Westartedwiththeequation: âˆšï¸ âˆš ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥ First,letâ€™ssquarebothsides: âˆš âˆš ğ‘âˆ’ ğ‘+ğ‘¥ = ğ‘¥2 =â‡’ ğ‘+ğ‘¥ = ğ‘âˆ’ğ‘¥2 Next,Icouldsquarebothsidesagain,treatingtheequation: Â·Â·Â· ... Table3 | Aninterestingâ€œahamomentâ€ofanintermediateversionofDeepSeek-R1-Zero. The modellearnstorethinkusingananthropomorphictone. Thisisalsoanahamomentforus, allowingustowitnessthepowerandbeautyofreinforcementlearning. Drawback of DeepSeek-R1-Zero Although DeepSeek-R1-Zero exhibits strong reasoning capabilitiesandautonomouslydevelopsunexpectedandpowerfulreasoningbehaviors,itfaces severalissues. Forinstance,DeepSeek-R1-Zerostruggleswithchallengeslikepoorreadability, and language mixing. To make reasoning processes more readable and share them with the open community, we explore DeepSeek-R1, a method that utilizes RL with human-friendly cold-startdata. 2.3. DeepSeek-R1: ReinforcementLearningwithColdStart Inspired by the promising results of DeepSeek-R1-Zero, two natural questions arise: 1) Can reasoningperformancebefurtherimprovedorconvergenceacceleratedbyincorporatingasmall amount of high-quality data as a cold start? 2) How can we train a user-friendly model that notonlyproducesclearandcoherentChainsofThought(CoT)butalsodemonstratesstrong generalcapabilities? Toaddressthesequestions,wedesignapipelinetotrainDeepSeek-R1. The pipelineconsistsoffourstages,outlinedasfollows. 2.3.1. ColdStart UnlikeDeepSeek-R1-Zero,topreventtheearlyunstablecoldstartphaseofRLtrainingfrom the base model, for DeepSeek-R1 we construct and collect a small amount of long CoT data to fine-tune the model as the initial RL actor. To collect such data, we have explored several approaches: using few-shot prompting with a long CoT as an example, directly prompting modelstogeneratedetailedanswerswithreflectionandverification,gatheringDeepSeek-R1- Zerooutputsinareadableformat,andrefiningtheresultsthroughpost-processingbyhuman annotators. Inthiswork,wecollectthousandsofcold-startdatatofine-tunetheDeepSeek-V3-Baseas the starting point for RL. Compared to DeepSeek-R1-Zero, the advantages of cold start data 9include: â€¢ Readability: AkeylimitationofDeepSeek-R1-Zeroisthatitscontentisoftennotsuitable for reading. Responses may mix multiple languages or lack markdown formatting to highlightanswersforusers. Incontrast,whencreatingcold-startdataforDeepSeek-R1, wedesignareadablepatternthatincludesasummaryattheendofeachresponseand filters out responses that are not reader-friendly. Here, we define the output format as |special_token|<reasoning_process>|special_token|<summary>,wherethereasoning processistheCoTforthequery,andthesummaryisusedtosummarizethereasoning results. â€¢ Potential: Bycarefullydesigningthepatternforcold-startdatawithhumanpriors, we observebetterperformanceagainstDeepSeek-R1-Zero. Webelievetheiterativetrainingis abetterwayforreasoningmodels. 2.3.2. Reasoning-orientedReinforcementLearning After fine-tuning DeepSeek-V3-Base on the cold start data, we apply the same large-scale reinforcementlearningtrainingprocessasemployedinDeepSeek-R1-Zero. Thisphasefocuses onenhancingthemodelâ€™sreasoningcapabilities,particularlyinreasoning-intensivetaskssuch ascoding,mathematics,science,andlogicreasoning,whichinvolvewell-definedproblemswith clearsolutions. Duringthetrainingprocess,weobservethatCoToftenexhibitslanguagemixing, particularlywhenRLpromptsinvolvemultiplelanguages. Tomitigatetheissueoflanguage mixing,weintroducealanguageconsistencyrewardduringRLtraining,whichiscalculated astheproportionoftargetlanguagewordsintheCoT.Althoughablationexperimentsshow that such alignment results in a slight degradation in the modelâ€™s performance, this reward alignswithhumanpreferences,makingitmorereadable. Finally,wecombinetheaccuracyof reasoningtasksandtherewardforlanguageconsistencybydirectlysummingthemtoformthe finalreward. WethenapplyRLtrainingonthefine-tunedmodeluntilitachievesconvergence onreasoningtasks. 2.3.3. RejectionSamplingandSupervisedFine-Tuning When reasoning-oriented RL converges, we utilize the resulting checkpoint to collect SFT (SupervisedFine-Tuning)dataforthesubsequentround. Unliketheinitialcold-startdata,which primarilyfocusesonreasoning,thisstageincorporatesdatafromotherdomainstoenhancethe modelâ€™scapabilitiesinwriting,role-playing,andothergeneral-purposetasks. Specifically,we generatethedataandfine-tunethemodelasdescribedbelow. Reasoningdata Wecuratereasoningpromptsandgeneratereasoningtrajectoriesbyperform- ingrejectionsamplingfromthecheckpointfromtheaboveRLtraining. Inthepreviousstage, weonlyincludeddatathatcouldbeevaluatedusingrule-basedrewards. However,inthisstage, weexpandthedatasetbyincorporatingadditionaldata,someofwhichuseagenerativereward model by feeding the ground-truth and model predictions into DeepSeek-V3 for judgment. Additionally, because the model output is sometimes chaotic and difficult to read, we have filtered out chain-of-thought with mixed languages, long parapraphs, and code blocks. For eachprompt,wesamplemultipleresponsesandretainonlythecorrectones. Intotal,wecollect about600kreasoningrelatedtrainingsamples. 10Non-Reasoning data For non-reasoning data, such as writing, factual QA, self-cognition, andtranslation,weadopttheDeepSeek-V3pipelineandreuseportionsoftheSFTdatasetof DeepSeek-V3. For certain non-reasoning tasks, we call DeepSeek-V3 to generate a potential chain-of-thoughtbeforeansweringthequestionbyprompting. However,forsimplerqueries, such as â€œhelloâ€ we do not provide a CoT in response. In the end, we collected a total of approximately200ktrainingsamplesthatareunrelatedtoreasoning. Wefine-tuneDeepSeek-V3-Basefortwoepochsusingtheabovecurateddatasetofabout 800ksamples. 2.3.4. ReinforcementLearningforallScenarios Tofurtheralignthemodelwithhumanpreferences,weimplementasecondaryreinforcement learningstageaimedatimprovingthemodelâ€™shelpfulnessandharmlessnesswhilesimultane- ouslyrefiningitsreasoningcapabilities. Specifically,wetrainthemodelusingacombination of reward signals and diverse prompt distributions. For reasoning data, we adhere to the methodology outlined in DeepSeek-R1-Zero, which utilizes rule-based rewards to guide the learningprocessinmath,code,andlogicalreasoningdomains. Forgeneraldata,weresortto reward models to capture human preferences in complex and nuanced scenarios. We build upontheDeepSeek-V3pipelineandadoptasimilardistributionofpreferencepairsandtrain- ing prompts. For helpfulness, we focus exclusively on the final summary, ensuring that the assessmentemphasizestheutilityandrelevanceoftheresponsetotheuserwhileminimizing interferencewiththeunderlyingreasoningprocess. Forharmlessness,weevaluatetheentire responseofthemodel,includingboththereasoningprocessandthesummary,toidentifyand mitigate any potential risks, biases, or harmful content that may arise during the generation process. Ultimately,theintegrationofrewardsignalsanddiversedatadistributionsenablesus totrainamodelthatexcelsinreasoningwhileprioritizinghelpfulnessandharmlessness. 2.4. Distillation: EmpowerSmallModelswithReasoningCapability ToequipmoreefficientsmallermodelswithreasoningcapabilitieslikeDeepSeek-R1,wedirectly fine-tuned open-source models like Qwen (Qwen, 2024b) and Llama (AI@Meta, 2024) using the800ksamplescuratedwithDeepSeek-R1,asdetailedinÂ§2.3.3. Ourfindingsindicatethat thisstraightforwarddistillationmethodsignificantlyenhancesthereasoningabilitiesofsmaller models. The base models we use here are Qwen2.5-Math-1.5B, Qwen2.5-Math-7B, Qwen2.5- 14B,Qwen2.5-32B,Llama-3.1-8B,andLlama-3.3-70B-Instruct. WeselectLlama-3.3becauseits reasoningcapabilityisslightlybetterthanthatofLlama-3.1. For distilled models, we apply only SFT and do not include an RL stage, even though incorporating RL could substantially boost model performance. Our primary goal here is to demonstrate the effectiveness of the distillation technique, leaving the exploration of the RL stagetothebroaderresearchcommunity. 3. Experiment Benchmarks WeevaluatemodelsonMMLU(Hendrycksetal.,2020),MMLU-Redux(Gema etal.,2024),MMLU-Pro(Wangetal.,2024),C-Eval(Huangetal.,2023),andCMMLU(Lietal., 2023),IFEval(Zhouetal.,2023),FRAMES(Krishnaetal.,2024),GPQADiamond (Reinetal., 2023),SimpleQA(OpenAI,2024c),C-SimpleQA(Heetal.,2024),SWE-BenchVerified(OpenAI, 112024d), Aider 1, LiveCodeBench (Jain et al., 2024) (2024-08 â€“ 2025-01), Codeforces 2, Chinese NationalHighSchoolMathematicsOlympiad(CNMO2024)3,andAmericanInvitationalMath- ematicsExamination2024(AIME2024)(MAA,2024). Inadditiontostandardbenchmarks,we alsoevaluateourmodelsonopen-endedgenerationtasksusingLLMsasjudges. Specifically,we adheretotheoriginalconfigurationsofAlpacaEval2.0(Duboisetal.,2024)andArena-Hard(Li etal.,2024),whichleverageGPT-4-Turbo-1106asjudgesforpairwisecomparisons. Here,we only feed the final summary to evaluation to avoid the length bias. For distilled models, we report representative results on AIME 2024, MATH-500, GPQA Diamond, Codeforces, and LiveCodeBench. Evaluation Prompts Following the setup in DeepSeek-V3, standard benchmarks such as MMLU,DROP,GPQADiamond,andSimpleQAareevaluatedusingpromptsfromthesimple- evalsframework. ForMMLU-Redux,weadopttheZero-Evalpromptformat(Lin,2024)ina zero-shotsetting. IntermsofMMLU-Pro,C-EvalandCLUE-WSC,sincetheoriginalprompts are few-shot, we slightly modify the prompt to the zero-shot setting. The CoT in few-shot may hurt the performance of DeepSeek-R1. Other datasets follow their original evaluation protocolswithdefaultpromptsprovidedbytheircreators. Forcodeandmathbenchmarks,the HumanEval-Muldatasetcoverseightmainstreamprogramminglanguages(Python,Java,C++, C#,JavaScript,TypeScript,PHP,andBash). ModelperformanceonLiveCodeBenchisevaluated usingCoTformat,withdatacollectedbetweenAugust2024andJanuary2025. TheCodeforces datasetisevaluatedusingproblemsfrom10Div.2contestsalongwithexpert-craftedtestcases, after which the expected ratings and percentages of competitors are calculated. SWE-Bench verified results are obtained via the agentless framework (Xia et al., 2024). AIDER-related benchmarksaremeasuredusinga\"diff\"format. DeepSeek-R1outputsarecappedatamaximum of32,768tokensforeachbenchmark. Baselines Weconductcomprehensiveevaluationsagainstseveralstrongbaselines,including DeepSeek-V3, Claude-Sonnet-3.5-1022, GPT-4o-0513, OpenAI-o1-mini, andOpenAI-o1-1217. SinceaccessingtheOpenAI-o1-1217APIischallenginginmainlandChina,wereportitsperfor- mancebasedonofficialreports. Fordistilledmodels,wealsocomparetheopen-sourcemodel QwQ-32B-Preview(Qwen,2024a). Evaluation Setup We set the maximum generation length to 32,768 tokens for the models. We found that using greedy decoding to evaluate long-output reasoning models results in higherrepetitionratesandsignificantvariabilityacrossdifferentcheckpoints. Therefore,we defaulttopass@ğ‘˜evaluation(Chenetal.,2021)andreportpass@1usinganon-zerotemperature. Specifically, we use a sampling temperature of 0.6 and a top-ğ‘ value of 0.95 to generate ğ‘˜ responses(typicallybetween4and64,dependingonthetestsetsize)foreachquestion. Pass@1 isthencalculatedas ğ‘˜ 1 âˆ‘ï¸ pass@1 = ğ‘ ğ‘–, ğ‘˜ ğ‘–=1 where ğ‘ ğ‘– denotes the correctness of the ğ‘–-th response. This method provides more reliable performanceestimates. ForAIME2024,wealsoreportconsensus(majorityvote)results(Wang etal.,2022)using64samples,denotedascons@64. 1https://aider.chat 2https://codeforces.com 3https://www.cms.org.cn/Home/comp/comp/cid/12.html 123.1. DeepSeek-R1Evaluation Claude-3.5- GPT-4o DeepSeek OpenAI OpenAI DeepSeek Benchmark(Metric) Sonnet-1022 0513 V3 o1-mini o1-1217 R1 Architecture - - MoE - - MoE #ActivatedParams - - 37B - - 37B #TotalParams - - 671B - - 671B MMLU(Pass@1) 88.3 87.2 88.5 85.2 91.8 90.8 MMLU-Redux(EM) 88.9 88.0 89.1 86.7 - 92.9 MMLU-Pro(EM) 78.0 72.6 75.9 80.3 - 84.0 DROP(3-shotF1) 88.3 83.7 91.6 83.9 90.2 92.2 IF-Eval(PromptStrict) 86.5 84.3 86.1 84.8 - 83.3 English GPQADiamond(Pass@1) 65.0 49.9 59.1 60.0 75.7 71.5 SimpleQA(Correct) 28.4 38.2 24.9 7.0 47.0 30.1 FRAMES(Acc.) 72.5 80.5 73.3 76.9 - 82.5 AlpacaEval2.0(LC-winrate) 52.0 51.1 70.0 57.8 - 87.6 ArenaHard(GPT-4-1106) 85.2 80.4 85.5 92.0 - 92.3 LiveCodeBench(Pass@1-COT) 38.9 32.9 36.2 53.8 63.4 65.9 Codeforces(Percentile) 20.3 23.6 58.7 93.4 96.6 96.3 Code Codeforces(Rating) 717 759 1134 1820 2061 2029 SWEVerified(Resolved) 50.8 38.8 42.0 41.6 48.9 49.2 Aider-Polyglot(Acc.) 45.3 16.0 49.6 32.9 61.7 53.3 AIME2024(Pass@1) 16.0 9.3 39.2 63.6 79.2 79.8 Math MATH-500(Pass@1) 78.3 74.6 90.2 90.0 96.4 97.3 CNMO2024(Pass@1) 13.1 10.8 43.2 67.6 - 78.8 CLUEWSC(EM) 85.4 87.9 90.9 89.9 - 92.8 Chinese C-Eval(EM) 76.7 76.0 86.5 68.9 - 91.8 C-SimpleQA(Correct) 55.4 58.7 68.0 40.3 - 63.7 Table4 | ComparisonbetweenDeepSeek-R1andotherrepresentativemodels. For education-oriented knowledge benchmarks such as MMLU, MMLU-Pro, and GPQA Diamond,DeepSeek-R1demonstratessuperiorperformancecomparedtoDeepSeek-V3. Thisim- provementisprimarilyattributedtoenhancedaccuracyinSTEM-relatedquestions,wheresignif- icantgainsareachievedthroughlarge-scalereinforcementlearning. Additionally,DeepSeek-R1 excelsonFRAMES,along-context-dependentQAtask,showcasingitsstrongdocumentanalysis capabilities. This highlights the potential of reasoning models in AI-driven search and data analysistasks. OnthefactualbenchmarkSimpleQA,DeepSeek-R1outperformsDeepSeek-V3, demonstratingitscapabilityinhandlingfact-basedqueries. Asimilartrendisobservedwhere OpenAI-o1surpassesGPT-4oonthisbenchmark. However,DeepSeek-R1performsworsethan DeepSeek-V3 on the Chinese SimpleQA benchmark, primarily due to its tendency to refuse answeringcertainqueriesaftersafetyRL.WithoutsafetyRL,DeepSeek-R1couldachievean accuracyofover70%. DeepSeek-R1alsodeliversimpressiveresultsonIF-Eval,abenchmarkdesignedtoassessa modelâ€™sabilitytofollowformatinstructions. Theseimprovementscanbelinkedtotheinclusion of instruction-following data during the final stages of supervised fine-tuning (SFT) and RL training. Furthermore,remarkableperformanceisobservedonAlpacaEval2.0andArenaHard, indicatingDeepSeek-R1â€™sstrengthsinwritingtasksandopen-domainquestionanswering. Its significantoutperformanceofDeepSeek-V3underscoresthegeneralizationbenefitsoflarge-scale RL,whichnotonlyboostsreasoningcapabilitiesbutalsoimprovesperformanceacrossdiverse domains. Moreover, the summary lengths generated by DeepSeek-R1 are concise, with an averageof689tokensonArenaHardand2,218charactersonAlpacaEval2.0. Thisindicatesthat 13DeepSeek-R1avoidsintroducinglengthbiasduringGPT-basedevaluations,furthersolidifying itsrobustnessacrossmultipletasks. On math tasks, DeepSeek-R1 demonstrates performance on par with OpenAI-o1-1217, surpassingothermodelsbyalargemargin. Asimilartrendisobservedoncodingalgorithm tasks,suchasLiveCodeBenchandCodeforces,wherereasoning-focusedmodelsdominatethese benchmarks. Onengineering-orientedcodingtasks,OpenAI-o1-1217outperformsDeepSeek-R1 onAiderbutachievescomparableperformanceonSWEVerified. Webelievetheengineering performance of DeepSeek-R1 will improve in the next version, as the amount of related RL trainingdatacurrentlyremainsverylimited. 3.2. DistilledModelEvaluation GPQA LiveCode AIME2024 MATH-500 CodeForces Model Diamond Bench pass@1 cons@64 pass@1 pass@1 pass@1 rating GPT-4o-0513 9.3 13.4 74.6 49.9 32.9 759 Claude-3.5-Sonnet-1022 16.0 26.7 78.3 65.0 38.9 717 OpenAI-o1-mini 63.6 80.0 90.0 60.0 53.8 1820 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 1316 DeepSeek-R1-Distill-Qwen-1.5B 28.9 52.7 83.9 33.8 16.9 954 DeepSeek-R1-Distill-Qwen-7B 55.5 83.3 92.8 49.1 37.6 1189 DeepSeek-R1-Distill-Qwen-14B 69.7 80.0 93.9 59.1 53.1 1481 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 1691 DeepSeek-R1-Distill-Llama-8B 50.4 80.0 89.1 49.0 39.6 1205 DeepSeek-R1-Distill-Llama-70B 70.0 86.7 94.5 65.2 57.5 1633 Table5 | ComparisonofDeepSeek-R1distilledmodelsandothercomparablemodelson reasoning-relatedbenchmarks. AsshowninTable5,simplydistillingDeepSeek-R1â€™soutputsenablestheefficientDeepSeek- R1-7B (i.e., DeepSeek-R1-Distill-Qwen-7B, abbreviated similarly below) to outperform non- reasoningmodelslikeGPT-4o-0513acrosstheboard. DeepSeek-R1-14BsurpassesQwQ-32B- Previewonallevaluationmetrics,whileDeepSeek-R1-32BandDeepSeek-R1-70Bsignificantly exceedo1-minionmostbenchmarks. Theseresultsdemonstratethestrongpotentialofdistilla- tion. Additionally,wefoundthatapplyingRLtothesedistilledmodelsyieldssignificantfurther gains. Webelievethiswarrantsfurtherexplorationandthereforepresentonlytheresultsofthe simpleSFT-distilledmodelshere. 4. Discussion 4.1. Distillationv.s. ReinforcementLearning InSection3.2,wecanseethatbydistillingDeepSeek-R1,thesmallmodelcanachieveimpressive results. However,thereisstillonequestionleft: canthemodelachievecomparableperformance throughthelarge-scaleRLtrainingdiscussedinthepaperwithoutdistillation? Toanswerthisquestion,weconductlarge-scaleRLtrainingonQwen-32B-Baseusingmath, code,andSTEMdata,trainingforover10Ksteps,resultinginDeepSeek-R1-Zero-Qwen-32B.The experimentalresults,showninTable6,demonstratethatthe32Bbasemodel,afterlarge-scale 14AIME2024 MATH-500 GPQADiamond LiveCodeBench Model pass@1 cons@64 pass@1 pass@1 pass@1 QwQ-32B-Preview 50.0 60.0 90.6 54.5 41.9 DeepSeek-R1-Zero-Qwen-32B 47.0 60.0 91.6 55.0 40.2 DeepSeek-R1-Distill-Qwen-32B 72.6 83.3 94.3 62.1 57.2 Table6 | ComparisonofdistilledandRLModelsonReasoning-RelatedBenchmarks. RL training, achieves performance on par with QwQ-32B-Preview. However, DeepSeek-R1- Distill-Qwen-32B, which is distilled from DeepSeek-R1, performs significantly better than DeepSeek-R1-Zero-Qwen-32Bacrossallbenchmarks. Therefore,wecandrawtwoconclusions: First,distillingmorepowerfulmodelsintosmaller onesyieldsexcellentresults,whereassmallermodelsrelyingonthelarge-scaleRLmentionedin thispaperrequireenormouscomputationalpowerandmaynotevenachievetheperformance ofdistillation. Second,whiledistillationstrategiesarebotheconomicalandeffective,advancing beyondtheboundariesofintelligencemaystillrequiremorepowerfulbasemodelsandlarger- scalereinforcementlearning. 4.2. UnsuccessfulAttempts IntheearlystagesofdevelopingDeepSeek-R1,wealsoencounteredfailuresandsetbacksalong theway. Weshareourfailureexperiencesheretoprovideinsights,butthisdoesnotimplythat theseapproachesareincapableofdevelopingeffectivereasoningmodels. ProcessRewardModel(PRM) PRMisareasonablemethodtoguidethemodeltowardbetter approachesforsolvingreasoningtasks(Lightmanetal.,2023;Uesatoetal.,2022;Wangetal., 2023). However,inpractice,PRMhasthreemainlimitationsthatmayhinderitsultimatesuc- cess. First,itischallengingtoexplicitlydefineafine-grainstepingeneralreasoning. Second, determiningwhetherthecurrentintermediatestepiscorrectisachallengingtask. Automated annotationusingmodelsmaynotyieldsatisfactoryresults,whilemanualannotationisnotcon- ducivetoscalingup. Third,onceamodel-basedPRMisintroduced,itinevitablyleadstoreward hacking(Gaoetal.,2022),andretrainingtherewardmodelneedsadditionaltrainingresources anditcomplicatesthewholetrainingpipeline. Inconclusion,whilePRMdemonstratesagood abilitytorerankthetop-Nresponsesgeneratedbythemodelorassistinguidedsearch(Snell etal.,2024),itsadvantagesarelimitedcomparedtotheadditionalcomputationaloverheadit introducesduringthelarge-scalereinforcementlearningprocessinourexperiments. MonteCarloTreeSearch(MCTS) InspiredbyAlphaGo(Silveretal.,2017b)andAlphaZero(Sil- ver et al., 2017a), we explored using Monte Carlo Tree Search (MCTS) to enhance test-time computescalability. Thisapproachinvolvesbreakinganswersintosmallerpartstoallowthe modeltoexplorethesolutionspacesystematically. Tofacilitatethis,wepromptthemodelto generatemultipletagsthatcorrespondtospecificreasoningstepsnecessaryforthesearch. For training,wefirstusecollectedpromptstofindanswersviaMCTSguidedbyapre-trainedvalue model. Subsequently,weusetheresultingquestion-answerpairstotrainboththeactormodel andthevaluemodel,iterativelyrefiningtheprocess. However,thisapproachencountersseveralchallengeswhenscalingupthetraining. First, unlike chess, wherethe search spaceis relatively well-defined, tokengeneration presents an 15exponentiallylargersearchspace. Toaddressthis,wesetamaximumextensionlimitforeach node, but this can lead to the model getting stuck in local optima. Second, the value model directly influences the quality of generation since it guides each step of the search process. Trainingafine-grainedvaluemodelisinherentlydifficult,whichmakesitchallengingforthe modeltoiterativelyimprove. WhileAlphaGoâ€™scoresuccessreliedontrainingavaluemodelto progressivelyenhanceitsperformance,thisprincipleprovesdifficulttoreplicateinoursetup duetothecomplexitiesoftokengeneration. Inconclusion,whileMCTScanimproveperformanceduringinferencewhenpairedwitha pre-trainedvaluemodel,iterativelyboostingmodelperformancethroughself-searchremainsa significantchallenge. 5. Conclusion, Limitations, and Future Work Inthiswork,weshareourjourneyinenhancingmodelreasoningabilitiesthroughreinforcement learning. DeepSeek-R1-Zero represents a pure RL approach without relying on cold-start data, achieving strong performance across various tasks. DeepSeek-R1 is more powerful, leveragingcold-startdataalongsideiterativeRLfine-tuning. Ultimately,DeepSeek-R1achieves performancecomparabletoOpenAI-o1-1217onarangeoftasks. We further explore distillation the reasoning capability to small dense models. We use DeepSeek-R1astheteachermodeltogenerate800Ktrainingsamples,andfine-tuneseveralsmall densemodels. Theresultsarepromising: DeepSeek-R1-Distill-Qwen-1.5BoutperformsGPT-4o andClaude-3.5-Sonnetonmathbenchmarkswith28.9%onAIMEand83.9%onMATH.Other dense models also achieve impressive results, significantly outperforming other instruction- tunedmodelsbasedonthesameunderlyingcheckpoints. Inthefuture,weplantoinvestinresearchacrossthefollowingdirectionsforDeepSeek-R1. â€¢ GeneralCapability: Currently,thecapabilitiesofDeepSeek-R1fallshortofDeepSeek-V3 in tasks such as function calling, multi-turn, complex role-playing, and JSON output. Movingforward,weplantoexplorehowlongCoTcanbeleveragedtoenhancetasksin thesefields. â€¢ LanguageMixing: DeepSeek-R1iscurrentlyoptimizedforChineseandEnglish,which may result in language mixing issues when handling queries in other languages. For instance,DeepSeek-R1mightuseEnglishforreasoningandresponses,evenifthequeryis inalanguageotherthanEnglishorChinese. Weaimtoaddressthislimitationinfuture updates. â€¢ PromptingEngineering: WhenevaluatingDeepSeek-R1,weobservethatitissensitive toprompts. Few-shotpromptingconsistentlydegradesitsperformance. Therefore,we recommend users directly describe the problem and specify the output format using a zero-shotsettingforoptimalresults. â€¢ Software Engineering Tasks: Due to the long evaluation times, which impact the effi- ciency of the RL process, large-scale RL has not been applied extensively in software engineeringtasks. Asaresult,DeepSeek-R1hasnotdemonstratedahugeimprovement over DeepSeek-V3 on software engineering benchmarks. Future versions will address thisbyimplementingrejectionsamplingonsoftwareengineeringdataorincorporating asynchronousevaluationsduringtheRLprocesstoimproveefficiency. 16References AI@Meta. Llama3.1modelcard,2024. URLhttps://github.com/meta-llama/llama-m odels/blob/main/models/llama3_1/MODEL_CARD.md. Anthropic. Claude3.5sonnet,2024. URLhttps://www.anthropic.com/news/claude-3 -5-sonnet. M.Chen,J.Tworek,H.Jun,Q.Yuan,H.P.deOliveiraPinto,J.Kaplan,H.Edwards,Y.Burda, N.Joseph,G.Brockman,A.Ray,R.Puri,G.Krueger,M.Petrov,H.Khlaaf,G.Sastry,P.Mishkin, B.Chan,S.Gray,N.Ryder,M.Pavlov,A.Power,L.Kaiser,M.Bavarian,C.Winter,P.Tillet, F.P.Such,D.Cummings,M.Plappert,F.Chantzis,E.Barnes,A.Herbert-Voss,W.H.Guss, A.Nichol,A.Paino,N.Tezak,J.Tang,I.Babuschkin,S.Balaji,S.Jain,W.Saunders,C.Hesse, A.N.Carr,J.Leike,J.Achiam,V.Misra,E.Morikawa,A.Radford,M.Knight,M.Brundage, M.Murati,K.Mayer,P.Welinder,B.McGrew,D.Amodei,S.McCandlish,I.Sutskever,and W.Zaremba. Evaluatinglargelanguagemodelstrainedoncode. CoRR,abs/2107.03374,2021. URLhttps://arxiv.org/abs/2107.03374. A. Dubey, A. Jauhri, A. Pandey, A. Kadian, A. Al-Dahle, A. Letman, A. Mathur, A. Schelten, A.Yang,A.Fan,etal. Thellama3herdofmodels. arXivpreprintarXiv:2407.21783,2024. Y.Dubois,B.Galambosi,P.Liang,andT.B.Hashimoto. Length-controlledalpacaeval: Asimple waytodebiasautomaticevaluators. arXivpreprintarXiv:2404.04475,2024. X. Feng, Z. Wan, M. Wen, S. M. McAleer, Y. Wen, W. Zhang, and J. Wang. Alphazero-like tree-search can guide large language model decoding and training, 2024. URL https: //arxiv.org/abs/2309.17179. L.Gao,J.Schulman,andJ.Hilton. Scalinglawsforrewardmodeloveroptimization,2022. URL https://arxiv.org/abs/2210.10760. A.P.Gema, J.O.J.Leang, G.Hong, A.Devoto, A.C.M.Mancino, R.Saxena, X.He, Y.Zhao, X. Du, M. R. G. Madani, C. Barale, R. McHardy, J. Harris, J. Kaddour, E. van Krieken, and P.Minervini. Arewedonewithmmlu? CoRR,abs/2406.04127,2024. URLhttps://doi.or g/10.48550/arXiv.2406.04127. Google. Ournext-generationmodel: Gemini1.5,2024. URLhttps://blog.google/techno logy/ai/google-gemini-next-generation-model-february-2024. Y. He, S. Li, J. Liu, Y. Tan, W. Wang, H. Huang, X. Bu, H. Guo, C. Hu, B. Zheng, et al. Chi- nese simpleqa: A chinese factuality evaluation for large language models. arXiv preprint arXiv:2411.07140,2024. D.Hendrycks,C.Burns,S.Basart,A.Zou,M.Mazeika,D.Song,andJ.Steinhardt. Measuring massivemultitasklanguageunderstanding. arXivpreprintarXiv:2009.03300,2020. Y.Huang,Y.Bai,Z.Zhu,J.Zhang,J.Zhang,T.Su,J.Liu,C.Lv,Y.Zhang,J.Lei,etal. C-Eval: A multi-levelmulti-disciplinechineseevaluationsuiteforfoundationmodels. arXivpreprint arXiv:2305.08322,2023. N.Jain,K.Han,A.Gu,W.Li,F.Yan,T.Zhang,S.Wang,A.Solar-Lezama,K.Sen,andI.Stoica. Livecodebench: Holisticandcontaminationfreeevaluationoflargelanguagemodelsforcode. CoRR,abs/2403.07974,2024. URLhttps://doi.org/10.48550/arXiv.2403.07974. 17S.Krishna,K.Krishna,A.Mohananey,S.Schwarcz,A.Stambler,S.Upadhyay,andM.Faruqui. Fact, fetch, and reason: A unified evaluation of retrieval-augmented generation. CoRR, abs/2409.12941,2024. doi: 10.48550/ARXIV.2409.12941. URLhttps://doi.org/10.485 50/arXiv.2409.12941. A.Kumar,V.Zhuang,R.Agarwal,Y.Su,J.D.Co-Reyes,A.Singh,K.Baumli,S.Iqbal,C.Bishop, R.Roelofs,etal. Traininglanguagemodelstoself-correctviareinforcementlearning. arXiv preprintarXiv:2409.12917,2024. H.Li,Y.Zhang,F.Koto,Y.Yang,H.Zhao,Y.Gong,N.Duan,andT.Baldwin. CMMLU:Measur- ingmassivemultitasklanguageunderstandinginChinese. arXivpreprintarXiv:2306.09212, 2023. T. Li, W.-L. Chiang, E. Frick, L. Dunlap, T. Wu, B. Zhu, J. E. Gonzalez, and I. Stoica. From crowdsourceddatatohigh-qualitybenchmarks: Arena-hardandbenchbuilderpipeline. arXiv preprintarXiv:2406.11939,2024. H. Lightman, V. Kosaraju, Y. Burda, H. Edwards, B. Baker, T. Lee, J. Leike, J. Schulman, I.Sutskever,andK.Cobbe. Letâ€™sverifystepbystep. arXivpreprintarXiv:2305.20050,2023. B.Y.Lin. ZeroEval: AUnifiedFrameworkforEvaluatingLanguageModels,July2024. URL https://github.com/WildEval/ZeroEval. MAA. American invitational mathematics examination - aime. In American Invitational Mathematics Examination - AIME 2024, February 2024. URL https://maa.org/math -competitions/american-invitational-mathematics-examination-aime. OpenAI. HelloGPT-4o,2024a. URLhttps://openai.com/index/hello-gpt-4o/. OpenAI. Learningtoreasonwithllms,2024b. URLhttps://openai.com/index/learnin g-to-reason-with-llms/. OpenAI. IntroducingSimpleQA,2024c. URLhttps://openai.com/index/introducing -simpleqa/. OpenAI. Introducing SWE-bench verified weâ€™re releasing a human-validated subset of swe- benchthatmore, 2024d. URLhttps://openai.com/index/introducing-swe-bench -verified/. Qwen. Qwq: Reflectdeeplyontheboundariesoftheunknown,2024a. URLhttps://qwenlm .github.io/blog/qwq-32b-preview/. Qwen. Qwen2.5: Apartyoffoundationmodels,2024b. URLhttps://qwenlm.github.io/b log/qwen2.5. D.Rein,B.L.Hou,A.C.Stickland,J.Petty,R.Y.Pang,J.Dirani,J.Michael,andS.R.Bowman. GPQA:Agraduate-levelgoogle-proofq&abenchmark. arXivpreprintarXiv:2311.12022,2023. Z.Shao,P.Wang,Q.Zhu,R.Xu,J.Song,M.Zhang,Y.Li,Y.Wu,andD.Guo. Deepseekmath: Pushing the limits of mathematical reasoning in open language models. arXiv preprint arXiv:2402.03300,2024. D. Silver, T. Hubert, J. Schrittwieser, I. Antonoglou, M. Lai, A. Guez, M. Lanctot, L. Sifre, D.Kumaran,T.Graepel,T.P.Lillicrap,K.Simonyan,andD.Hassabis. Masteringchessand shogibyself-playwithageneralreinforcementlearningalgorithm. CoRR,abs/1712.01815, 2017a. URLhttp://arxiv.org/abs/1712.01815. 18D.Silver,J.Schrittwieser,K.Simonyan,I.Antonoglou,A.Huang,A.Guez,T.Hubert,L.Baker, M.Lai,A.Bolton,Y.Chen,T.P.Lillicrap,F.Hui,L.Sifre,G.vandenDriessche,T.Graepel,and D.Hassabis. Masteringthegameofgowithouthumanknowledge. Nat.,550(7676):354â€“359, 2017b. doi: 10.1038/NATURE24270. URLhttps://doi.org/10.1038/nature24270. C. Snell, J. Lee, K. Xu, and A. Kumar. Scaling llm test-time compute optimally can be more effectivethanscalingmodelparameters,2024. URLhttps://arxiv.org/abs/2408.033 14. T. Trinh, Y. Wu, Q. Le, H. He, and T. Luong. Solving olympiad geometry without human demonstrations. Nature,2024. doi: 10.1038/s41586-023-06747-5. J. Uesato, N. Kushman, R. Kumar, F. Song, N. Siegel, L. Wang, A. Creswell, G. Irving, and I.Higgins. Solvingmathwordproblemswithprocess-andoutcome-basedfeedback. arXiv preprintarXiv:2211.14275,2022. P.Wang,L.Li,Z.Shao,R.Xu,D.Dai,Y.Li,D.Chen,Y.Wu,andZ.Sui. Math-shepherd: Alabel- freestep-by-stepverifierforllmsinmathematicalreasoning. arXivpreprintarXiv:2312.08935, 2023. X. Wang, J. Wei, D. Schuurmans, Q. Le, E. Chi, S. Narang, A. Chowdhery, and D. Zhou. Self-consistency improves chain of thought reasoning in language models. arXiv preprint arXiv:2203.11171,2022. Y.Wang,X.Ma,G.Zhang,Y.Ni,A.Chandra,S.Guo,W.Ren,A.Arulraj,X.He,Z.Jiang,T.Li, M. Ku, K. Wang, A. Zhuang, R. Fan, X. Yue, and W. Chen. Mmlu-pro: A more robust and challenging multi-task language understanding benchmark. CoRR, abs/2406.01574, 2024. URLhttps://doi.org/10.48550/arXiv.2406.01574. C. S. Xia, Y. Deng, S. Dunn, and L. Zhang. Agentless: Demystifying llm-based software engineeringagents. arXivpreprint,2024. H.Xin,Z.Z.Ren,J.Song,Z.Shao,W.Zhao,H.Wang,B.Liu,L.Zhang,X.Lu,Q.Du,W.Gao, Q.Zhu,D.Yang,Z.Gou,Z.F.Wu,F.Luo,andC.Ruan. Deepseek-prover-v1.5: Harnessing proofassistantfeedbackforreinforcementlearningandmonte-carlotreesearch,2024. URL https://arxiv.org/abs/2408.08152. J.Zhou,T.Lu,S.Mishra,S.Brahma,S.Basu,Y.Luan,D.Zhou,andL.Hou. Instruction-following evaluationforlargelanguagemodels. arXivpreprintarXiv:2311.07911,2023. 19Appendix A. Contributions and Acknowledgments CoreContributors HuiLi DayaGuo JianzhongGuo DejianYang JiashiLi HaoweiZhang JingchangChen JunxiaoSong JingyangYuan RuoyuZhang JinhaoTu RunxinXu JunjieQiu QihaoZhu JunlongLi ShirongMa J.L.Cai PeiyiWang JiaqiNi XiaoBi JianLiang XiaokangZhang JinChen XingkaiYu KaiDong YuWu KaiHu* Z.F.Wu KaichaoYou ZhibinGou KaigeGao ZhihongShao KangGuan ZhuoshuLi KexinHuang ZiyiGao KuaiYu LeanWang LecongZhang Contributors LiangZhao AixinLiu LitongWang BingXue LiyueZhang BingxuanWang LeiXu BochaoWu LeyiXia BeiFeng MingchuanZhang ChengdaLu MinghuaZhang ChenggangZhao MinghuiTang ChengqiDeng MingxuZhou ChongRuan MengLi DamaiDai MiaojunWang DeliChen MingmingLi DongjieJi NingTian ErhangLi PanpanHuang FangyunLin PengZhang FucongDai QianchengWang FuliLuo* QinyuChen GuangboHao QiushiDu GuantingChen RuiqiGe* GuoweiLi RuisongZhang H.Zhang RuizhePan HanweiXu RunjiWang HonghuiDing R.J.Chen HuazuoGao R.L.Jin HuiQu 20RuyiChen Y.X.Wei ShanghaoLu YangZhang ShangyanZhou YanhongXu ShanhuangChen YaoLi ShengfengYe YaoZhao ShiyuWang YaofengSun ShuipingYu YaohuiWang ShunfengZhou YiYu ShutingPan YichaoZhang S.S.Li YifanShi ShuangZhou YiliangXiong ShaoqingWu YingHe ShengfengYe YishiPiao TaoYun YisongWang TianPei YixuanTan TianyuSun YiyangMa* T.Wang YiyuanLiu WangdingZeng YongqiangGuo WenLiu YuanOu WenfengLiang YuduanWang WenjunGao YueGong WenqinYu* YuhengZou WentaoZhang YujiaHe W.L.Xiao YunfanXiong WeiAn YuxiangLuo XiaodongLiu YuxiangYou XiaohanWang YuxuanLiu XiaokangChen YuyangZhou XiaotaoNie Y.X.Zhu XinCheng YanpingHuang XinLiu YaohuiLi XinXie YiZheng XingchaoLiu YuchenZhu XinyuYang YunxianMa XinyuanLi YingTang XuechengSu YukunZha XuhengLin YutingYan X.Q.Li Z.Z.Ren XiangyueJin ZehuiRen XiaojinShen ZhangliSha XiaoshaChen ZheFu XiaowenSun ZheanXu XiaoxiangWang ZhendaXie XinnanSong ZhengyanZhang XinyiZhou ZhewenHao XianzuWang ZhichengMa XinxiaShan ZhigangYan Y.K.Li ZhiyuWu Y.Q.Wang ZihuiGu 21ZijiaZhu ZhenHuang ZijunLiu* ZhipengXu ZilinLi ZhongyuZhang ZiweiXie ZhenZhang ZiyangSong ZizhengPan Withineachrole,authorsarelistedalphabeticallybythefirstname. Namesmarkedwith* denoteindividualswhohavedepartedfromourteam. 22'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "#text = re.sub(r\"\\s+\", \" \", text)\n",
        "#text"
      ],
      "metadata": {
        "id": "My3u-fSclx7P"
      },
      "execution_count": 74,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = re.sub(r\"[^\\w\\s\\-\\.\\,]\", \"\", text)  # Remove tudo que nÃ£o Ã© letra, nÃºmero, espaÃ§o, hÃ­fen, ponto ou vÃ­rgula\n",
        "#text"
      ],
      "metadata": {
        "id": "iGfwEM0WmAeN"
      },
      "execution_count": 75,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = text.lower()\n",
        "#text"
      ],
      "metadata": {
        "id": "fJBmeHOBnFyL"
      },
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#text = re.sub(r\"-\\s+\", \"\", text)  # Remove hifens no fim de linha\n",
        "#text"
      ],
      "metadata": {
        "id": "vrMBR6ZbnNwC"
      },
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "nlp = spacy.blank(\"en\")"
      ],
      "metadata": {
        "id": "IxzjBDmS1NAx"
      },
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "@Language.component(\"model_name_recognizer\")\n",
        "def model_name_recognizer(doc):\n",
        "    # Lista de possÃ­veis nomes de modelos baseados no seu PDF\n",
        "    model_names = [\n",
        "        \"DeepSeek-R1\", \"DeepSeek-R1-Zero\", \"DeepSeek-V3\", \"Qwen2.5-32B\",\n",
        "        \"Llama-3.3-70B-Instruct\", \"QwQ-32B-Preview\", \"OpenAI-o1-1217\",\n",
        "        \"OpenAI-o1-mini\", \"Claude-3.5-Sonnet\", \"GPT-4o\"\n",
        "    ]\n",
        "\n",
        "    # Procurar por esses nomes no texto\n",
        "    spans = []\n",
        "    for model_name in model_names:\n",
        "        start = 0\n",
        "        while True:\n",
        "            start = doc.text.find(model_name, start)\n",
        "            if start == -1:\n",
        "                break\n",
        "            end = start + len(model_name)\n",
        "\n",
        "            # Tentar criar um span; verificar se nÃ£o Ã© None\n",
        "            char_span = doc.char_span(start, end)\n",
        "            if char_span is not None:\n",
        "                spans.append(Span(doc, char_span.start, char_span.end, label=\"MODEL\"))\n",
        "\n",
        "            start = end  # Continuar a busca apÃ³s o modelo encontrado\n",
        "\n",
        "    # Filtrar spans para evitar sobreposiÃ§Ã£o\n",
        "    filtered_spans = filter_spans(spans)\n",
        "\n",
        "    # Adicionar as entidades encontradas ao doc\n",
        "    doc.ents = list(doc.ents) + filtered_spans\n",
        "    return doc"
      ],
      "metadata": {
        "id": "bCtclCOf3USQ"
      },
      "execution_count": 79,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "1. @Language.component(\"model_name_recognizer\")\n",
        "\n",
        "Isso registra a funÃ§Ã£o como um componente personalizado do pipeline do spaCy.\n",
        "\n",
        "2. Span(doc, doc.char_span(start, end).start, doc.char_span(start, end).end, label=\"MODEL\")\n",
        "\n",
        "  - Aqui estamos criando spans para as entidades encontradas no texto.\n",
        "\n",
        "  - doc.char_span(start, end) Ã© um mÃ©todo do spaCy que mapeia Ã­ndices de caracteres para tokens no Doc.\n",
        "  - Span(doc, start, end, label=\"MODEL\") cria uma entidade nomeada personalizada.\n",
        "\n",
        "3. doc.ents = list(doc.ents) + spans\n",
        "\n",
        "Adicionamos as novas entidades detectadas ao documento."
      ],
      "metadata": {
        "id": "uBRArS84hruP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Adicionar o componente personalizado ao pipeline\n",
        "nlp.add_pipe(\"model_name_recognizer\", last=True)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 103
        },
        "id": "30g3Cn_W3Kii",
        "outputId": "c5e16549-03d4-41f9-eb69-ddbddec7d9b9"
      },
      "execution_count": 80,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<function __main__.model_name_recognizer(doc)>"
            ],
            "text/html": [
              "<div style=\"max-width:800px; border: 1px solid var(--colab-border-color);\"><style>\n",
              "      pre.function-repr-contents {\n",
              "        overflow-x: auto;\n",
              "        padding: 8px 12px;\n",
              "        max-height: 500px;\n",
              "      }\n",
              "\n",
              "      pre.function-repr-contents.function-repr-contents-collapsed {\n",
              "        cursor: pointer;\n",
              "        max-height: 100px;\n",
              "      }\n",
              "    </style>\n",
              "    <pre style=\"white-space: initial; background:\n",
              "         var(--colab-secondary-surface-color); padding: 8px 12px;\n",
              "         border-bottom: 1px solid var(--colab-border-color);\"><b>model_name_recognizer</b><br/>def model_name_recognizer(doc)</pre><pre class=\"function-repr-contents function-repr-contents-collapsed\" style=\"\"><a class=\"filepath\" style=\"display:none\" href=\"#\">/content/&lt;ipython-input-79-3ee5376fe9b1&gt;</a>&lt;no docstring&gt;</pre></div>"
            ]
          },
          "metadata": {},
          "execution_count": 80
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Processar o texto com o spaCy\n",
        "doc = nlp(text)"
      ],
      "metadata": {
        "id": "QGiNu8OV1fG4"
      },
      "execution_count": 81,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Exibir as entidades encontradas\n",
        "for ent in doc.ents:\n",
        "    print(ent.text, ent.label_)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u5Ro-PEqlEIH",
        "outputId": "1b5e4ee6-a795-4e17-a145-b211335477fe"
      },
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-1217 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-mini MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-1217 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-mini MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "Qwen2.5-32B MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "OpenAI-o1-mini MODEL\n",
            "GPT-4o MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-1217 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "OpenAI-o1-mini MODEL\n",
            "QwQ-32B-Preview MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "QwQ-32B-Preview MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "QwQ-32B-Preview MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1-Zero MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-R1 MODEL\n",
            "DeepSeek-V3 MODEL\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gradio as gr\n",
        "import re"
      ],
      "metadata": {
        "id": "rGB-omEQlPb7"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_names = [\n",
        "        \"DeepSeek-R1\", \"DeepSeek-R1-Zero\", \"DeepSeek-V3\", \"Qwen2.5-32B\",\n",
        "        \"Llama-3.3-70B-Instruct\", \"QwQ-32B-Preview\", \"OpenAI-o1-1217\",\n",
        "        \"OpenAI-o1-mini\", \"Claude-3.5-Sonnet\", \"GPT-4o\"\n",
        "    ]"
      ],
      "metadata": {
        "id": "l6tAJp-WrMyz"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# CompilaÃ§Ã£o de uma Ãºnica expressÃ£o regular para todas as entidades\n",
        "ENTITY_REGEX = re.compile(r\"\\b(\" + \"|\".join(re.escape(name) for name in model_names) + r\")\\b\", re.IGNORECASE)\n",
        "\n",
        "\n",
        "# FunÃ§Ã£o principal para processar o PDF e destacar entidades\n",
        "def process_pdf(pdf_file, highlight_entities):\n",
        "    # Extrair texto do PDF\n",
        "    text = extract_text_from_pdf(pdf_file)\n",
        "\n",
        "    if highlight_entities:\n",
        "        # Destacar entidades usando regex\n",
        "        highlighted_text = ENTITY_REGEX.sub(r\"**\\1**\", text)\n",
        "        return highlighted_text\n",
        "    else:\n",
        "        # Apenas exibir o texto original\n",
        "        return text\n"
      ],
      "metadata": {
        "id": "H_jpl3ido2n0"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "```python\n",
        "ENTITY_REGEX = re.compile(r\"\\b(\" + \"|\".join(re.escape(name) for name in MODEL_NAMES) + r\")\\b\", re.IGNORECASE)\n",
        "```\n",
        "\n",
        "Essa linha cria uma **expressÃ£o regular** (regex) que serÃ¡ usada para encontrar e destacar as entidades (nomes dos modelos, no caso) no texto. Vou explicar cada parte:\n",
        "\n",
        "---\n",
        "\n",
        "### **1. `MODEL_NAMES`**\n",
        "Essa Ã© a lista de entidades que queremos buscar no texto:\n",
        "\n",
        "```python\n",
        "MODEL_NAMES = [\n",
        "    \"DeepSeek-R1\", \"DeepSeek-R1-Zero\", \"DeepSeek-V3\", \"Qwen2.5-32B\",\n",
        "    \"Llama-3.3-70B-Instruct\", \"QwQ-32B-Preview\", \"OpenAI-o1-1217\",\n",
        "    \"OpenAI-o1-mini\", \"Claude-3.5-Sonnet\", \"GPT-4o\"\n",
        "]\n",
        "```\n",
        "\n",
        "Cada um desses nomes serÃ¡ tratado como uma entidade que queremos localizar no texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **2. `re.escape(name)`**\n",
        "A funÃ§Ã£o `re.escape` Ã© usada para garantir que qualquer caractere especial (como `-`, `.`, ou `+`) nos nomes da lista seja tratado literalmente na expressÃ£o regular.\n",
        "\n",
        "Por exemplo:\n",
        "- O nome `\"DeepSeek-R1\"` contÃ©m um hÃ­fen (`-`), que Ã© um caractere especial em regex. Se nÃ£o usarmos `re.escape`, o hÃ­fen poderia ser interpretado como algo diferente, causando erros ou resultados inesperados.\n",
        "- Com `re.escape(\"DeepSeek-R1\")`, o hÃ­fen Ã© \"escapado\", ficando assim: `\"DeepSeek\\-R1\"`.\n",
        "\n",
        "Isso garante que os nomes sejam interpretados corretamente.\n",
        "\n",
        "---\n",
        "\n",
        "### **3. `\"|\".join(...)`**\n",
        "A funÃ§Ã£o `\"|\".join(...)` cria uma Ãºnica string onde os nomes da lista sÃ£o separados pelo caractere `|`, que em regex significa **\"ou\"**.\n",
        "\n",
        "Por exemplo:\n",
        "```python\n",
        "\"|\".join([\"DeepSeek-R1\", \"DeepSeek-V3\", \"GPT-4o\"])\n",
        "```\n",
        "Produz:\n",
        "```\n",
        "\"DeepSeek\\-R1|DeepSeek\\-V3|GPT\\-4o\"\n",
        "```\n",
        "\n",
        "Isso significa que a regex irÃ¡ buscar por **qualquer um dos nomes** da lista no texto.\n",
        "\n",
        "---\n",
        "\n",
        "### **4. `r\"\\b(...)\\b\"`**\n",
        "O `r\"\\b\"` Ã© um delimitador em regex que indica **limites de palavra**. Ele garante que a busca seja feita apenas por palavras completas, e nÃ£o por partes de palavras.\n",
        "\n",
        "Por exemplo:\n",
        "- Se vocÃª buscar `\"DeepSeek\"` sem usar `\\b`, ele pode coincidir com `\"DeepSeek-R1\"` ou atÃ© mesmo com `\"DeepSeeker\"`.\n",
        "- Com `\\bDeepSeek\\b`, ele sÃ³ encontrarÃ¡ a palavra exatamente como `\"DeepSeek\"`.\n",
        "\n",
        "O trecho `r\"\\b(...)\\b\"` envolve o grupo de nomes entre limites de palavra, garantindo que os nomes sejam encontrados apenas como palavras completas.\n",
        "\n",
        "---\n",
        "\n",
        "### **5. `re.compile(...)`**\n",
        "A funÃ§Ã£o `re.compile` compila a expressÃ£o regular em um **objeto regex**, que Ã© mais eficiente para buscas repetidas, especialmente em textos grandes. Em vez de recompilar a regex toda vez que vocÃª faz uma busca, ela Ã© prÃ©-compilada e pronta para uso.\n",
        "\n",
        "---\n",
        "\n",
        "### **6. `re.IGNORECASE`**\n",
        "Esse Ã© um **modificador** que indica que a busca serÃ¡ **case-insensitive**, ou seja, nÃ£o farÃ¡ distinÃ§Ã£o entre maiÃºsculas e minÃºsculas.\n",
        "\n",
        "Por exemplo:\n",
        "- `\"DeepSeek-R1\"` serÃ¡ encontrado mesmo que no texto esteja escrito como `\"deepseek-r1\"` ou `\"DEEPSEEK-R1\"`.\n",
        "\n",
        "---\n",
        "\n",
        "### **Como a Regex Final Fica?**\n",
        "Depois de processar a lista `MODEL_NAMES`, a regex gerada serÃ¡ algo assim:\n",
        "\n",
        "```regex\n",
        "\\b(DeepSeek\\-R1|DeepSeek\\-R1\\-Zero|DeepSeek\\-V3|Qwen2\\.5\\-32B|Llama\\-3\\.3\\-70B\\-Instruct|QwQ\\-32B\\-Preview|OpenAI\\-o1\\-1217|OpenAI\\-o1\\-mini|Claude\\-3\\.5\\-Sonnet|GPT\\-4o)\\b\n",
        "```\n",
        "\n",
        "Essa regex:\n",
        "- Busca qualquer um dos nomes da lista.\n",
        "- Garante que os nomes sejam tratados como palavras completas (graÃ§as a `\\b`).\n",
        "- Ignora diferenÃ§as entre maiÃºsculas e minÃºsculas (graÃ§as a `re.IGNORECASE`).\n",
        "\n",
        "---\n",
        "\n",
        "### **Exemplo de Uso**\n",
        "Se aplicarmos essa regex a um texto, ela encontra as entidades especificadas. Por exemplo:\n",
        "\n",
        "#### Texto:\n",
        "```text\n",
        "O modelo DeepSeek-R1 foi comparado ao DeepSeek-V3 e ao GPT-4o.\n",
        "```\n",
        "\n",
        "#### CÃ³digo:\n",
        "```python\n",
        "ENTITY_REGEX = re.compile(r\"\\b(\" + \"|\".join(re.escape(name) for name in MODEL_NAMES) + r\")\\b\", re.IGNORECASE)\n",
        "matches = ENTITY_REGEX.findall(\"O modelo DeepSeek-R1 foi comparado ao DeepSeek-V3 e ao GPT-4o.\")\n",
        "print(matches)\n",
        "```\n",
        "\n",
        "#### SaÃ­da:\n",
        "```python\n",
        "['DeepSeek-R1', 'DeepSeek-V3', 'GPT-4o']\n",
        "```\n"
      ],
      "metadata": {
        "id": "EbF8Iyx0tQKn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Interface do Gradio\n",
        "with gr.Blocks() as demo:\n",
        "    gr.Markdown(\"## Upload de PDF e Destaque de Entidades\")\n",
        "\n",
        "    with gr.Row():\n",
        "        pdf_input = gr.File(label=\"FaÃ§a upload do PDF\", file_types=[\".pdf\"])\n",
        "        highlight_checkbox = gr.Checkbox(label=\"Destacar entidades (nomes dos modelos)\", value=False)\n",
        "\n",
        "    output_text = gr.Markdown(label=\"Texto ExtraÃ­do\")\n",
        "\n",
        "    submit_button = gr.Button(\"Processar\")\n",
        "    submit_button.click(process_pdf, inputs=[pdf_input, highlight_checkbox], outputs=output_text)\n",
        "\n",
        "# Rodar a aplicaÃ§Ã£o\n",
        "demo.launch()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 649
        },
        "id": "IaW4S5dppiEY",
        "outputId": "a1671573-595c-43e9-b737-a2d34c1c02c2"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running Gradio in a Colab notebook requires sharing enabled. Automatically setting `share=True` (you can turn this off by setting `share=False` in `launch()` explicitly).\n",
            "\n",
            "Colab notebook detected. To show errors in colab notebook, set debug=True in launch()\n",
            "* Running on public URL: https://69d25595a202dd3dd4.gradio.live\n",
            "\n",
            "This share link expires in 72 hours. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "<div><iframe src=\"https://69d25595a202dd3dd4.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": []
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cUf5lTalpk01"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}